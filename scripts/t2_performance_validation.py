"""
T2 - é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾© - æ•ˆèƒ½é©—è­‰å’Œå„ªåŒ–åŸ·è¡Œè…³æœ¬
Task ID: T2

å°ˆæ¥­æ•ˆèƒ½é©—è­‰åŸ·è¡Œå™¨ï¼š
- æ•´åˆæ‰€æœ‰æ•ˆèƒ½å„ªåŒ–çµ„ä»¶
- è‡ªå‹•åŒ–T2éœ€æ±‚é©—è­‰
- ç”Ÿæˆå°ˆæ¥­æ•ˆèƒ½åˆ†æå ±å‘Š
- æä¾›ç³»çµ±èª¿æ ¡å»ºè­°
- é©—è­‰ä½µç™¼éŒ¯èª¤ç‡ â‰¤ 1% ç›®æ¨™

ä½œè€…: Ethan - æ•ˆèƒ½å„ªåŒ–å°ˆå®¶
"""

import asyncio
import logging
import json
import os
import sys
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
import argparse

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = Path(__file__).parent
project_root = current_dir.parent.parent
sys.path.append(str(project_root))

from services.connection_pool.connection_pool_manager import ConnectionPoolManager, PoolConfiguration
from services.connection_pool.adaptive_algorithm import AdaptiveScalingAlgorithm, CompetitionAwareScheduler
from services.connection_pool.performance_monitor import AdvancedPerformanceMonitor
from services.connection_pool.advanced_benchmark_engine import AdvancedPerformanceBenchmarkEngine
from tests.concurrency.test_connection_pool import ConnectionPoolTestSuite

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('performance_validation')


class T2PerformanceValidator:
    """
    T2ä»»å‹™æ•ˆèƒ½é©—è­‰å™¨
    
    å°ˆæ¥­åŠŸèƒ½ï¼š
    1. è‡ªå‹•åŒ–T2éœ€æ±‚é©—è­‰
    2. ä½µç™¼éŒ¯èª¤ç‡æ¸¬è©¦
    3. éŸ¿æ‡‰æ™‚é–“åŸºæº–é©—è­‰
    4. ç³»çµ±è² è¼‰èƒ½åŠ›è©•ä¼°
    5. å°ˆæ¥­æ•ˆèƒ½åˆ†æå ±å‘Š
    """
    
    def __init__(self, results_dir: str = "t2_validation_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # T2 ç›®æ¨™è¦æ±‚
        self.t2_requirements = {
            'max_error_rate_percent': 1.0,       # ä½µç™¼éŒ¯èª¤ç‡ â‰¤ 1%
            'max_p95_response_time_ms': 50.0,    # P95éŸ¿æ‡‰æ™‚é–“ â‰¤ 50ms
            'min_concurrent_workers': 10,        # æ”¯æ´10+å·¥ä½œè€…ä½µç™¼
            'target_concurrent_workers': 20,     # ç›®æ¨™20+å·¥ä½œè€…ä½µç™¼
            'min_throughput_ops_per_sec': 100,   # æœ€ä½ååé‡è¦æ±‚
            'min_success_rate_percent': 99.0     # æœ€ä½æˆåŠŸç‡è¦æ±‚
        }
        
        # é©—è­‰çµæœ
        self.validation_results = {
            'start_time': None,
            'end_time': None,
            'system_info': {},
            'test_results': [],
            'compliance_analysis': {},
            'performance_summary': {},
            'optimization_recommendations': [],
            'final_verdict': 'PENDING'
        }
        
        logger.info("T2æ•ˆèƒ½é©—è­‰å™¨å·²åˆå§‹åŒ–")
        logger.info(f"T2ç›®æ¨™è¦æ±‚: {json.dumps(self.t2_requirements, indent=2, ensure_ascii=False)}")
    
    async def run_full_validation(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„T2æ•ˆèƒ½é©—è­‰
        
        é©—è­‰æµç¨‹ï¼š
        1. ç³»çµ±åŸºç¤é©—è­‰
        2. 10+å·¥ä½œè€…ä½µç™¼æ¸¬è©¦  
        3. 20+å·¥ä½œè€…æ¥µé™æ¸¬è©¦
        4. æ··åˆè² è¼‰ç©©å®šæ€§æ¸¬è©¦
        5. æ•ˆèƒ½å›æ­¸åˆ†æ
        6. å„ªåŒ–å»ºè­°ç”Ÿæˆ
        """
        self.validation_results['start_time'] = datetime.now().isoformat()
        self.validation_results['system_info'] = self._collect_system_info()
        
        logger.info("ğŸš€ é–‹å§‹T2æ•ˆèƒ½é©—è­‰æµç¨‹")
        
        try:
            # éšæ®µ1: åŸºç¤æ•ˆèƒ½é©—è­‰
            logger.info("ğŸ“‹ éšæ®µ1: åŸºç¤æ•ˆèƒ½é©—è­‰")
            basic_results = await self._run_basic_performance_test()
            self.validation_results['test_results'].append(basic_results)
            
            # éšæ®µ2: T2æ¨™æº–ä½µç™¼æ¸¬è©¦ (10å·¥ä½œè€…)
            logger.info("âš¡ éšæ®µ2: T2æ¨™æº–ä½µç™¼æ¸¬è©¦ (10å·¥ä½œè€…)")
            standard_results = await self._run_t2_standard_concurrency_test()
            self.validation_results['test_results'].append(standard_results)
            
            # éšæ®µ3: T2æ¥µé™ä½µç™¼æ¸¬è©¦ (20å·¥ä½œè€…)
            logger.info("ğŸ”¥ éšæ®µ3: T2æ¥µé™ä½µç™¼æ¸¬è©¦ (20å·¥ä½œè€…)")
            extreme_results = await self._run_t2_extreme_concurrency_test()
            self.validation_results['test_results'].append(extreme_results)
            
            # éšæ®µ4: æ··åˆè² è¼‰ç©©å®šæ€§æ¸¬è©¦
            logger.info("ğŸ”„ éšæ®µ4: æ··åˆè² è¼‰ç©©å®šæ€§æ¸¬è©¦")
            stability_results = await self._run_stability_test()
            self.validation_results['test_results'].append(stability_results)
            
            # éšæ®µ5: å£“åŠ›æ¥µé™æ¸¬è©¦
            logger.info("ğŸ’¥ éšæ®µ5: å£“åŠ›æ¥µé™æ¸¬è©¦")  
            stress_results = await self._run_stress_limit_test()
            self.validation_results['test_results'].append(stress_results)
            
            # åˆ†æåˆè¦æ€§
            logger.info("ğŸ“Š åˆ†æT2åˆè¦æ€§")
            self.validation_results['compliance_analysis'] = self._analyze_t2_compliance()
            
            # ç”Ÿæˆæ•ˆèƒ½ç¸½çµ
            logger.info("ğŸ“ˆ ç”Ÿæˆæ•ˆèƒ½ç¸½çµ")
            self.validation_results['performance_summary'] = self._generate_performance_summary()
            
            # ç”Ÿæˆå„ªåŒ–å»ºè­°
            logger.info("ğŸ’¡ ç”Ÿæˆå„ªåŒ–å»ºè­°")
            self.validation_results['optimization_recommendations'] = await self._generate_optimization_recommendations()
            
            # æœ€çµ‚è£æ±º
            self.validation_results['final_verdict'] = self._make_final_verdict()
            
        except Exception as e:
            logger.error(f"é©—è­‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            self.validation_results['error'] = str(e)
            self.validation_results['final_verdict'] = 'ERROR'
        
        finally:
            self.validation_results['end_time'] = datetime.now().isoformat()
        
        # ä¿å­˜é©—è­‰å ±å‘Š
        self._save_validation_report()
        
        # è¼¸å‡ºæœ€çµ‚çµæœ
        self._print_final_results()
        
        logger.info("âœ… T2æ•ˆèƒ½é©—è­‰å®Œæˆ")
        return self.validation_results
    
    async def _run_basic_performance_test(self) -> Dict[str, Any]:
        """åŸ·è¡ŒåŸºç¤æ•ˆèƒ½æ¸¬è©¦"""
        logger.info("åŸ·è¡ŒåŸºç¤æ•ˆèƒ½æ¸¬è©¦: 5å·¥ä½œè€…, 50æ“ä½œ/å·¥ä½œè€…")
        
        pool_config = PoolConfiguration(
            min_connections=2,
            max_connections=15,
            connection_timeout=30.0,
            acquire_timeout=10.0,
            enable_monitoring=True
        )
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(pool_config)
            
            # é ç†±é€£ç·šæ± 
            await self._warmup_connection_pool(test_suite)
            
            # åŸ·è¡Œè®€å–æ¸¬è©¦
            read_result = await test_suite.run_concurrent_read_test(
                num_workers=5,
                operations_per_worker=50
            )
            
            # åŸ·è¡Œå¯«å…¥æ¸¬è©¦
            write_result = await test_suite.run_concurrent_write_test(
                num_workers=3,
                operations_per_worker=30
            )
            
            return {
                'test_phase': 'basic_performance',
                'read_test': self._serialize_test_result(read_result),
                'write_test': self._serialize_test_result(write_result),
                'timestamp': datetime.now().isoformat()
            }
    
    async def _run_t2_standard_concurrency_test(self) -> Dict[str, Any]:
        """åŸ·è¡ŒT2æ¨™æº–ä½µç™¼æ¸¬è©¦ - 10+å·¥ä½œè€…"""
        logger.info("åŸ·è¡ŒT2æ¨™æº–ä½µç™¼æ¸¬è©¦: 10å·¥ä½œè€…, 100æ“ä½œ/å·¥ä½œè€…")
        
        # å„ªåŒ–çš„é€£ç·šæ± é…ç½®
        pool_config = PoolConfiguration(
            min_connections=5,
            max_connections=20,
            connection_timeout=30.0,
            acquire_timeout=5.0,
            enable_monitoring=True
        )
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(pool_config)
            
            # é ç†±ç³»çµ±
            await self._warmup_connection_pool(test_suite)
            
            # æ··åˆå·¥ä½œè² è¼‰æ¸¬è©¦ - é€™æ˜¯T2çš„æ ¸å¿ƒé©—è­‰
            mixed_result = await test_suite.run_mixed_workload_test(
                num_workers=10,
                read_percentage=70.0,
                test_duration=120.0  # 2åˆ†é˜ç©©å®šæ¸¬è©¦
            )
            
            # ä½µç™¼è®€å–å£“åŠ›æ¸¬è©¦
            read_result = await test_suite.run_concurrent_read_test(
                num_workers=12,
                operations_per_worker=80
            )
            
            return {
                'test_phase': 't2_standard_concurrency',
                'target_workers': 10,
                'mixed_workload_test': self._serialize_test_result(mixed_result),
                'concurrent_read_test': self._serialize_test_result(read_result),
                't2_compliance_check': self._check_t2_compliance(mixed_result),
                'timestamp': datetime.now().isoformat()
            }
    
    async def _run_t2_extreme_concurrency_test(self) -> Dict[str, Any]:
        """åŸ·è¡ŒT2æ¥µé™ä½µç™¼æ¸¬è©¦ - 20+å·¥ä½œè€…"""
        logger.info("åŸ·è¡ŒT2æ¥µé™ä½µç™¼æ¸¬è©¦: 20å·¥ä½œè€…, æ¥µé™è² è¼‰")
        
        # æœ€å¤§åŒ–é€£ç·šæ± é…ç½®
        pool_config = PoolConfiguration(
            min_connections=8,
            max_connections=25,
            connection_timeout=30.0,
            acquire_timeout=8.0,
            enable_monitoring=True
        )
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(pool_config)
            
            # é ç†±ç³»çµ±åˆ°æœ€ä½³ç‹€æ…‹
            await self._warmup_connection_pool(test_suite)
            
            # 20å·¥ä½œè€…æ··åˆæ¸¬è©¦
            extreme_mixed_result = await test_suite.run_mixed_workload_test(
                num_workers=20,
                read_percentage=65.0,
                test_duration=180.0  # 3åˆ†é˜æ¥µé™æ¸¬è©¦
            )
            
            # å£“åŠ›æ¸¬è©¦ï¼šæ¼¸é€²å¼å¢åŠ åˆ°25å·¥ä½œè€…
            stress_result = await test_suite.run_stress_test(
                max_workers=25,
                ramp_up_duration=30.0,
                test_duration=150.0
            )
            
            return {
                'test_phase': 't2_extreme_concurrency',
                'target_workers': 20,
                'extreme_mixed_test': self._serialize_test_result(extreme_mixed_result),
                'stress_ramp_test': self._serialize_test_result(stress_result),
                't2_compliance_check': self._check_t2_compliance(extreme_mixed_result),
                'timestamp': datetime.now().isoformat()
            }
    
    async def _run_stability_test(self) -> Dict[str, Any]:
        """åŸ·è¡Œç©©å®šæ€§æ¸¬è©¦"""
        logger.info("åŸ·è¡Œç©©å®šæ€§æ¸¬è©¦: æŒçºŒè² è¼‰, é•·æ™‚é–“é‹è¡Œ")
        
        pool_config = PoolConfiguration(
            min_connections=6,
            max_connections=22,
            connection_timeout=30.0,
            acquire_timeout=6.0,
            enable_monitoring=True
        )
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(pool_config)
            
            # é•·æ™‚é–“ç©©å®šæ€§æ¸¬è©¦ - 15å·¥ä½œè€…æŒçºŒ5åˆ†é˜
            stability_result = await test_suite.run_mixed_workload_test(
                num_workers=15,
                read_percentage=75.0,
                test_duration=300.0  # 5åˆ†é˜æŒçºŒæ¸¬è©¦
            )
            
            return {
                'test_phase': 'stability_test',
                'duration_minutes': 5,
                'stability_result': self._serialize_test_result(stability_result),
                't2_compliance_check': self._check_t2_compliance(stability_result),
                'timestamp': datetime.now().isoformat()
            }
    
    async def _run_stress_limit_test(self) -> Dict[str, Any]:
        """åŸ·è¡Œå£“åŠ›æ¥µé™æ¸¬è©¦"""
        logger.info("åŸ·è¡Œå£“åŠ›æ¥µé™æ¸¬è©¦: å°‹æ‰¾ç³»çµ±æ¥µé™")
        
        pool_config = PoolConfiguration(
            min_connections=10,
            max_connections=30,
            connection_timeout=30.0,
            acquire_timeout=10.0,
            enable_monitoring=True
        )
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(pool_config)
            
            # æ¥µé™æ¸¬è©¦ï¼š30å·¥ä½œè€…
            limit_result = await test_suite.run_stress_test(
                max_workers=30,
                ramp_up_duration=45.0,
                test_duration=120.0
            )
            
            return {
                'test_phase': 'stress_limit_test',
                'max_workers_tested': 30,
                'limit_stress_result': self._serialize_test_result(limit_result),
                'system_limit_analysis': self._analyze_system_limits(limit_result),
                'timestamp': datetime.now().isoformat()
            }
    
    async def _warmup_connection_pool(self, test_suite, warmup_operations: int = 20):
        """é ç†±é€£ç·šæ± """
        logger.debug("é ç†±é€£ç·šæ± ...")
        
        try:
            # åŸ·è¡Œå°‘é‡æ“ä½œé ç†±
            warmup_result = await test_suite.run_concurrent_read_test(
                num_workers=3,
                operations_per_worker=warmup_operations
            )
            logger.debug(f"é ç†±å®Œæˆ: {warmup_result.successful_operations} æ“ä½œæˆåŠŸ")
        except Exception as e:
            logger.warning(f"é ç†±éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    def _serialize_test_result(self, test_result) -> Dict[str, Any]:
        """åºåˆ—åŒ–æ¸¬è©¦çµæœ"""
        if not test_result:
            return {}
        
        return {
            'test_name': getattr(test_result, 'test_name', ''),
            'duration_seconds': getattr(test_result, 'duration_seconds', 0),
            'concurrent_workers': getattr(test_result, 'concurrent_workers', 0),
            'total_operations': getattr(test_result, 'total_operations', 0),
            'successful_operations': getattr(test_result, 'successful_operations', 0),
            'failed_operations': getattr(test_result, 'failed_operations', 0),
            'operations_per_second': getattr(test_result, 'operations_per_second', 0),
            'average_response_time_ms': getattr(test_result, 'average_response_time_ms', 0),
            'p50_response_time_ms': getattr(test_result, 'p50_response_time_ms', 0),
            'p95_response_time_ms': getattr(test_result, 'p95_response_time_ms', 0),
            'p99_response_time_ms': getattr(test_result, 'p99_response_time_ms', 0),
            'error_rate_percentage': getattr(test_result, 'error_rate_percentage', 0),
            'max_connections_used': getattr(test_result, 'max_connections_used', 0),
            'timestamp': getattr(test_result, 'timestamp', datetime.now()).isoformat()
        }
    
    def _check_t2_compliance(self, test_result) -> Dict[str, Any]:
        """æª¢æŸ¥T2åˆè¦æ€§"""
        if not test_result:
            return {'compliant': False, 'reason': 'No test result'}
        
        error_rate = getattr(test_result, 'error_rate_percentage', 100)
        p95_response_time = getattr(test_result, 'p95_response_time_ms', 1000)
        workers = getattr(test_result, 'concurrent_workers', 0)
        success_rate = (getattr(test_result, 'successful_operations', 0) / 
                       max(getattr(test_result, 'total_operations', 1), 1)) * 100
        
        compliance_checks = {
            'error_rate_ok': error_rate <= self.t2_requirements['max_error_rate_percent'],
            'response_time_ok': p95_response_time <= self.t2_requirements['max_p95_response_time_ms'],
            'workers_ok': workers >= self.t2_requirements['min_concurrent_workers'],
            'success_rate_ok': success_rate >= self.t2_requirements['min_success_rate_percent']
        }
        
        all_compliant = all(compliance_checks.values())
        
        return {
            'compliant': all_compliant,
            'checks': compliance_checks,
            'metrics': {
                'error_rate_percent': error_rate,
                'p95_response_time_ms': p95_response_time,
                'concurrent_workers': workers,
                'success_rate_percent': success_rate
            },
            'requirements': self.t2_requirements
        }
    
    def _analyze_t2_compliance(self) -> Dict[str, Any]:
        """åˆ†ææ•´é«”T2åˆè¦æ€§"""
        compliant_tests = 0
        total_tests = 0
        detailed_analysis = {}
        
        for test_result in self.validation_results['test_results']:
            # åˆ†ææ¯å€‹æ¸¬è©¦éšæ®µçš„åˆè¦æ€§
            phase = test_result.get('test_phase', 'unknown')
            
            if 't2_compliance_check' in test_result:
                compliance = test_result['t2_compliance_check']
                detailed_analysis[phase] = compliance
                
                if compliance.get('compliant', False):
                    compliant_tests += 1
                total_tests += 1
        
        overall_compliance_rate = (compliant_tests / total_tests * 100) if total_tests > 0 else 0
        
        return {
            'overall_compliant': overall_compliance_rate >= 80,  # 80%ä»¥ä¸Šæ¸¬è©¦é€šé
            'compliance_rate_percent': overall_compliance_rate,
            'compliant_tests': compliant_tests,
            'total_tests': total_tests,
            'detailed_analysis': detailed_analysis,
            't2_requirements': self.t2_requirements
        }
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•ˆèƒ½ç¸½çµ"""
        all_throughputs = []
        all_response_times = []
        all_error_rates = []
        max_workers_tested = 0
        
        for test_result in self.validation_results['test_results']:
            # å¾å„ç¨®æ¸¬è©¦çµæœä¸­æå–æŒ‡æ¨™
            for test_key in test_result:
                if test_key.endswith('_test') or test_key.endswith('_result'):
                    test_data = test_result[test_key]
                    if isinstance(test_data, dict):
                        all_throughputs.append(test_data.get('operations_per_second', 0))
                        all_response_times.append(test_data.get('p95_response_time_ms', 0))
                        all_error_rates.append(test_data.get('error_rate_percentage', 0))
                        max_workers_tested = max(max_workers_tested, 
                                               test_data.get('concurrent_workers', 0))
        
        # éæ¿¾ç„¡æ•ˆæ•¸æ“š
        all_throughputs = [t for t in all_throughputs if t > 0]
        all_response_times = [r for r in all_response_times if r > 0]
        all_error_rates = [e for e in all_error_rates if e >= 0]
        
        import statistics
        
        return {
            'performance_metrics': {
                'max_throughput_ops_per_sec': max(all_throughputs) if all_throughputs else 0,
                'avg_throughput_ops_per_sec': statistics.mean(all_throughputs) if all_throughputs else 0,
                'min_response_time_p95_ms': min(all_response_times) if all_response_times else 0,
                'avg_response_time_p95_ms': statistics.mean(all_response_times) if all_response_times else 0,
                'max_error_rate_percent': max(all_error_rates) if all_error_rates else 0,
                'avg_error_rate_percent': statistics.mean(all_error_rates) if all_error_rates else 0
            },
            'test_coverage': {
                'max_workers_tested': max_workers_tested,
                'total_test_phases': len(self.validation_results['test_results']),
                'meets_10_worker_requirement': max_workers_tested >= 10,
                'meets_20_worker_target': max_workers_tested >= 20
            }
        }
    
    async def _generate_optimization_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼æ¸¬è©¦çµæœåˆ†ææ€§èƒ½ç“¶é ¸
        perf_summary = self.validation_results['performance_summary']
        
        if perf_summary:
            metrics = perf_summary.get('performance_metrics', {})
            
            # éŒ¯èª¤ç‡å»ºè­°
            max_error_rate = metrics.get('max_error_rate_percent', 0)
            if max_error_rate > 1.0:
                recommendations.append({
                    'category': 'reliability',
                    'priority': 'critical',
                    'title': 'ä½µç™¼éŒ¯èª¤ç‡éé«˜',
                    'description': f'æª¢æ¸¬åˆ°æœ€å¤§éŒ¯èª¤ç‡ç‚º {max_error_rate:.2f}%ï¼Œè¶…é1%çš„T2è¦æ±‚',
                    'actions': [
                        'å¯¦æ–½é€£ç·šé‡è©¦æ©Ÿåˆ¶',
                        'å¢å¼·éŒ¯èª¤æ¢å¾©é‚è¼¯',
                        'æ·»åŠ é€£ç·šå¥åº·æª¢æŸ¥',
                        'å„ªåŒ–é€£ç·šæ± é…ç½®'
                    ]
                })
            
            # éŸ¿æ‡‰æ™‚é–“å»ºè­°
            avg_response_time = metrics.get('avg_response_time_p95_ms', 0)
            if avg_response_time > 50:
                recommendations.append({
                    'category': 'performance',
                    'priority': 'high',
                    'title': 'éŸ¿æ‡‰æ™‚é–“å„ªåŒ–',
                    'description': f'å¹³å‡P95éŸ¿æ‡‰æ™‚é–“ç‚º {avg_response_time:.2f}msï¼Œè¶…é50msåŸºæº–',
                    'actions': [
                        'å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢æ•ˆç‡',
                        'å¢åŠ é€£ç·šæ± é åˆ†é…',
                        'å¯¦æ–½æŸ¥è©¢çµæœç·©å­˜',
                        'èª¿æ•´é€£ç·šç²å–è¶…æ™‚åƒæ•¸'
                    ]
                })
            
            # ååé‡å»ºè­°
            max_throughput = metrics.get('max_throughput_ops_per_sec', 0)
            if max_throughput < 200:
                recommendations.append({
                    'category': 'scalability',
                    'priority': 'medium',
                    'title': 'ååé‡æå‡',
                    'description': f'æœ€å¤§ååé‡ç‚º {max_throughput:.2f} ops/sï¼Œæœ‰æå‡ç©ºé–“',
                    'actions': [
                        'å¢åŠ é€£ç·šæ± æœ€å¤§å¤§å°',
                        'å¯¦æ–½ç•°æ­¥æ“ä½œæ¨¡å¼',
                        'å„ªåŒ–æ¥­å‹™é‚è¼¯è™•ç†',
                        'è€ƒæ…®åˆ†ç‰‡æˆ–åˆ†æ•£å¼è™•ç†'
                    ]
                })
        
        # å¦‚æœæ²’æœ‰å•é¡Œï¼Œæä¾›ä¸€èˆ¬æ€§å»ºè­°
        if not recommendations:
            recommendations.append({
                'category': 'maintenance',
                'priority': 'low',
                'title': 'æŒçºŒå„ªåŒ–',
                'description': 'ç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°é€²è¡ŒæŒçºŒç›£æ§å’Œç¶­è­·',
                'actions': [
                    'å®šæœŸåŸ·è¡Œæ•ˆèƒ½åŸºæº–æ¸¬è©¦',
                    'ç›£æ§ç”Ÿç”¢ç’°å¢ƒæŒ‡æ¨™',
                    'ä¿æŒä¾è³´é …ç›®æ›´æ–°',
                    'å®šæœŸæª¢æŸ¥é€£ç·šæ± é…ç½®'
                ]
            })
        
        return recommendations
    
    def _analyze_system_limits(self, stress_result) -> Dict[str, Any]:
        """åˆ†æç³»çµ±æ¥µé™"""
        if not stress_result:
            return {'analysis': 'No stress test result available'}
        
        error_rate = getattr(stress_result, 'error_rate_percentage', 0)
        workers = getattr(stress_result, 'concurrent_workers', 0)
        throughput = getattr(stress_result, 'operations_per_second', 0)
        
        return {
            'max_stable_workers': workers if error_rate <= 5 else max(1, workers - 5),
            'breaking_point_error_rate': error_rate,
            'max_observed_throughput': throughput,
            'system_stability': 'stable' if error_rate <= 2 else 'unstable' if error_rate <= 10 else 'critical'
        }
    
    def _make_final_verdict(self) -> str:
        """åšå‡ºæœ€çµ‚è£æ±º"""
        compliance = self.validation_results.get('compliance_analysis', {})
        
        if compliance.get('overall_compliant', False):
            return 'PASS'
        elif compliance.get('compliance_rate_percent', 0) >= 50:
            return 'PARTIAL_PASS'
        else:
            return 'FAIL'
    
    def _collect_system_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»çµ±è³‡è¨Š"""
        import psutil
        import platform
        
        return {
            'platform': platform.platform(),
            'cpu_count': psutil.cpu_count(),
            'total_memory_gb': psutil.virtual_memory().total / (1024**3),
            'python_version': sys.version,
            'timestamp': datetime.now().isoformat()
        }
    
    def _save_validation_report(self):
        """ä¿å­˜é©—è­‰å ±å‘Š"""
        timestamp = int(time.time())
        report_file = self.results_dir / f"t2_validation_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"é©—è­‰å ±å‘Šå·²ä¿å­˜: {report_file}")
    
    def _print_final_results(self):
        """è¼¸å‡ºæœ€çµ‚çµæœ"""
        verdict = self.validation_results.get('final_verdict', 'UNKNOWN')
        compliance = self.validation_results.get('compliance_analysis', {})
        summary = self.validation_results.get('performance_summary', {})
        
        print("\n" + "="*80)
        print("ğŸ¯ T2 é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾© - æ•ˆèƒ½é©—è­‰çµæœ")
        print("="*80)
        
        print(f"\nğŸ“Š æœ€çµ‚è£æ±º: {verdict}")
        
        if compliance:
            rate = compliance.get('compliance_rate_percent', 0)
            print(f"ğŸ“ˆ T2åˆè¦ç‡: {rate:.1f}% ({compliance.get('compliant_tests', 0)}/{compliance.get('total_tests', 0)} æ¸¬è©¦é€šé)")
        
        if summary and 'performance_metrics' in summary:
            metrics = summary['performance_metrics']
            print("\nğŸš€ æ•ˆèƒ½æŒ‡æ¨™ç¸½çµ:")
            print(f"   â€¢ æœ€å¤§ååé‡: {metrics.get('max_throughput_ops_per_sec', 0):.2f} ops/s")
            print(f"   â€¢ å¹³å‡P95éŸ¿æ‡‰æ™‚é–“: {metrics.get('avg_response_time_p95_ms', 0):.2f} ms")
            print(f"   â€¢ æœ€å¤§éŒ¯èª¤ç‡: {metrics.get('max_error_rate_percent', 0):.2f}%")
        
        recommendations = self.validation_results.get('optimization_recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ å„ªåŒ–å»ºè­° ({len(recommendations)} é …):")
            for i, rec in enumerate(recommendations[:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                print(f"   {i}. [{rec.get('priority', 'unknown')}] {rec.get('title', 'Unknown')}")
        
        print("\n" + "="*80)


async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    parser = argparse.ArgumentParser(description="T2 é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾© - æ•ˆèƒ½é©—è­‰")
    parser.add_argument('--results-dir', default='t2_validation_results', 
                       help='çµæœè¼¸å‡ºç›®éŒ„')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è©³ç´°è¼¸å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # å‰µå»ºé©—è­‰å™¨ä¸¦åŸ·è¡Œ
    validator = T2PerformanceValidator(results_dir=args.results_dir)
    
    try:
        results = await validator.run_full_validation()
        
        # è¼¸å‡ºçµæœæ‘˜è¦
        verdict = results.get('final_verdict', 'UNKNOWN')
        if verdict == 'PASS':
            print("\nâœ… T2æ•ˆèƒ½é©—è­‰é€šéï¼ç³»çµ±æ»¿è¶³ä½µç™¼æ•ˆèƒ½è¦æ±‚ã€‚")
            return 0
        elif verdict == 'PARTIAL_PASS':
            print("\nâš ï¸ T2æ•ˆèƒ½é©—è­‰éƒ¨åˆ†é€šéï¼Œå»ºè­°æŸ¥çœ‹å„ªåŒ–å»ºè­°ã€‚")
            return 1
        else:
            print("\nâŒ T2æ•ˆèƒ½é©—è­‰æœªé€šéï¼Œéœ€è¦ç³»çµ±å„ªåŒ–ã€‚")
            return 2
    
    except Exception as e:
        logger.error(f"é©—è­‰åŸ·è¡Œå¤±æ•—: {e}")
        print(f"\nğŸ’¥ é©—è­‰åŸ·è¡Œå¤±æ•—: {e}")
        return 3


if __name__ == "__main__":
    exit(asyncio.run(main()))