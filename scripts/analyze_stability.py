#!/usr/bin/env python3
"""
ç©©å®šæ€§åˆ†æå™¨
Task ID: T5 - Discord testing: dpytest and random interactions

åˆ†æå¤šæ¬¡åŸ·è¡Œçš„æ¸¬è©¦çµæœï¼Œæª¢æ¸¬flakyæ¸¬è©¦å’Œç©©å®šæ€§å•é¡Œã€‚
"""

import argparse
import json
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
import logging
from datetime import datetime
from collections import defaultdict, Counter

logger = logging.getLogger(__name__)


class StabilityAnalyzer:
    """ç©©å®šæ€§åˆ†æå™¨"""
    
    def __init__(self, input_dir: Path, output_path: Path):
        self.input_dir = input_dir
        self.output_path = output_path
        
    def analyze(self) -> Dict[str, Any]:
        """åŸ·è¡Œç©©å®šæ€§åˆ†æ"""
        
        # æ”¶é›†æ‰€æœ‰ç©©å®šæ€§æ¸¬è©¦çµæœæª”æ¡ˆ
        stability_files = self._find_stability_files()
        
        if not stability_files:
            logger.warning("No stability test files found")
            return {"status": "no_data", "analysis": {}}
        
        # è§£ææ¸¬è©¦çµæœ
        all_results = self._parse_stability_results(stability_files)
        
        # åŸ·è¡Œåˆ†æ
        analysis = self._perform_analysis(all_results)
        
        # ç”Ÿæˆå ±å‘Š
        report = self._generate_report(analysis, stability_files)
        
        # ä¿å­˜å ±å‘Š
        self._save_report(report)
        
        return report
    
    def _find_stability_files(self) -> List[Path]:
        """æŸ¥æ‰¾ç©©å®šæ€§æ¸¬è©¦çµæœæª”æ¡ˆ"""
        files = []
        
        # æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„æª”æ¡ˆ
        patterns = [
            "stability-run-*.xml",
            "stability_*.xml", 
            "*stability*.xml"
        ]
        
        for pattern in patterns:
            files.extend(self.input_dir.glob(pattern))
        
        # æ’åºä»¥ç¢ºä¿ä¸€è‡´çš„è™•ç†é †åº
        return sorted(files)
    
    def _parse_stability_results(self, files: List[Path]) -> List[Dict[str, Any]]:
        """è§£æç©©å®šæ€§æ¸¬è©¦çµæœ"""
        all_results = []
        
        for file_path in files:
            try:
                results = self._parse_single_result_file(file_path)
                results["source_file"] = str(file_path)
                all_results.append(results)
                
            except Exception as e:
                logger.error(f"Failed to parse {file_path}: {e}")
                continue
        
        return all_results
    
    def _parse_single_result_file(self, file_path: Path) -> Dict[str, Any]:
        """è§£æå–®å€‹çµæœæª”æ¡ˆ"""
        
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            results = {
                "run_info": {
                    "timestamp": root.get("timestamp", "unknown"),
                    "tests": int(root.get("tests", 0)),
                    "failures": int(root.get("failures", 0)),
                    "errors": int(root.get("errors", 0)),
                    "time": float(root.get("time", 0))
                },
                "testcases": []
            }
            
            # è§£ææ¸¬è©¦æ¡ˆä¾‹
            for testcase in root.findall('.//testcase'):
                case_info = {
                    "name": testcase.get("name", "unknown"),
                    "classname": testcase.get("classname", "unknown"),
                    "time": float(testcase.get("time", 0)),
                    "status": "passed"  # é è¨­ç‚ºé€šé
                }
                
                # æª¢æŸ¥å¤±æ•—æˆ–éŒ¯èª¤
                if testcase.find("failure") is not None:
                    failure = testcase.find("failure")
                    case_info["status"] = "failed"
                    case_info["failure_type"] = failure.get("type", "unknown")
                    case_info["failure_message"] = failure.get("message", "")
                    
                elif testcase.find("error") is not None:
                    error = testcase.find("error")
                    case_info["status"] = "error"
                    case_info["error_type"] = error.get("type", "unknown")
                    case_info["error_message"] = error.get("message", "")
                
                elif testcase.find("skipped") is not None:
                    case_info["status"] = "skipped"
                    case_info["skip_reason"] = testcase.find("skipped").get("message", "")
                
                results["testcases"].append(case_info)
            
            return results
            
        except ET.ParseError as e:
            logger.error(f"XML parsing error in {file_path}: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error parsing {file_path}: {e}")
            raise
    
    def _perform_analysis(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸ·è¡Œè©³ç´°åˆ†æ"""
        
        if not all_results:
            return {"status": "no_data"}
        
        # æ”¶é›†æ‰€æœ‰æ¸¬è©¦çš„åŸ·è¡Œè¨˜éŒ„
        test_records = defaultdict(list)  # test_key -> [run_records]
        
        for run_idx, result in enumerate(all_results):
            for testcase in result["testcases"]:
                test_key = f"{testcase['classname']}::{testcase['name']}"
                test_records[test_key].append({
                    "run": run_idx + 1,
                    "status": testcase["status"],
                    "time": testcase["time"],
                    "details": testcase.get("failure_message") or testcase.get("error_message", "")
                })
        
        # åˆ†ææ¯å€‹æ¸¬è©¦çš„ç©©å®šæ€§
        analysis = {
            "total_runs": len(all_results),
            "total_unique_tests": len(test_records),
            "test_analysis": {},
            "flaky_tests": [],
            "consistent_failures": [],
            "performance_issues": [],
            "summary": {}
        }
        
        # åˆ†ææ¯å€‹æ¸¬è©¦
        for test_key, records in test_records.items():
            test_analysis = self._analyze_single_test(test_key, records)
            analysis["test_analysis"][test_key] = test_analysis
            
            # åˆ†é¡å•é¡Œæ¸¬è©¦
            if test_analysis["is_flaky"]:
                analysis["flaky_tests"].append({
                    "test": test_key,
                    "success_rate": test_analysis["success_rate"],
                    "failure_pattern": test_analysis["failure_pattern"]
                })
            
            if test_analysis["always_fails"]:
                analysis["consistent_failures"].append({
                    "test": test_key,
                    "failure_reason": test_analysis["common_failure_reason"]
                })
            
            if test_analysis["has_performance_issues"]:
                analysis["performance_issues"].append({
                    "test": test_key,
                    "avg_time": test_analysis["avg_execution_time"],
                    "max_time": test_analysis["max_execution_time"]
                })
        
        # ç”Ÿæˆæ‘˜è¦
        analysis["summary"] = self._generate_summary(analysis)
        
        return analysis
    
    def _analyze_single_test(self, test_key: str, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå–®å€‹æ¸¬è©¦çš„ç©©å®šæ€§"""
        
        total_runs = len(records)
        statuses = [r["status"] for r in records]
        times = [r["time"] for r in records]
        
        # è¨ˆç®—æˆåŠŸç‡
        success_count = statuses.count("passed")
        success_rate = success_count / total_runs if total_runs > 0 else 0
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºflakyæ¸¬è©¦ï¼ˆæœ‰æ™‚æˆåŠŸï¼Œæœ‰æ™‚å¤±æ•—ï¼‰
        unique_statuses = set(statuses)
        is_flaky = len(unique_statuses) > 1 and "passed" in unique_statuses
        
        # åˆ¤æ–·æ˜¯å¦ç¸½æ˜¯å¤±æ•—
        always_fails = success_count == 0 and total_runs > 0
        
        # åˆ†æå¤±æ•—æ¨¡å¼
        failures = [r for r in records if r["status"] != "passed"]
        failure_reasons = [f["details"] for f in failures if f["details"]]
        common_failure_reason = None
        
        if failure_reasons:
            # æ‰¾å‡ºæœ€å¸¸è¦‹çš„å¤±æ•—åŸå› 
            reason_counts = Counter(failure_reasons)
            if reason_counts:
                common_failure_reason = reason_counts.most_common(1)[0][0]
        
        # åˆ†ææ•ˆèƒ½
        avg_time = sum(times) / len(times) if times else 0
        max_time = max(times) if times else 0
        min_time = min(times) if times else 0
        
        # åˆ¤æ–·æ•ˆèƒ½å•é¡Œï¼ˆå¹³å‡æ™‚é–“è¶…é30ç§’æˆ–æœ€å¤§æ™‚é–“è¶…é60ç§’ï¼‰
        has_performance_issues = avg_time > 30 or max_time > 60
        
        return {
            "total_runs": total_runs,
            "success_count": success_count,
            "success_rate": success_rate,
            "is_flaky": is_flaky,
            "always_fails": always_fails,
            "failure_pattern": dict(Counter(statuses)),
            "common_failure_reason": common_failure_reason,
            "avg_execution_time": avg_time,
            "max_execution_time": max_time,
            "min_execution_time": min_time,
            "has_performance_issues": has_performance_issues,
            "execution_records": records
        }
    
    def _generate_summary(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        
        total_tests = analysis["total_unique_tests"]
        flaky_count = len(analysis["flaky_tests"])
        consistent_failure_count = len(analysis["consistent_failures"])
        performance_issue_count = len(analysis["performance_issues"])
        
        # è¨ˆç®—æ•´é«”ç©©å®šæ€§åˆ†æ•¸ (0-100)
        if total_tests > 0:
            stability_score = max(0, 100 - (
                (flaky_count * 30) +  # flakyæ¸¬è©¦æ‰£30åˆ†
                (consistent_failure_count * 50) +  # æŒçºŒå¤±æ•—æ‰£50åˆ†
                (performance_issue_count * 10)  # æ•ˆèƒ½å•é¡Œæ‰£10åˆ†
            ) / total_tests)
        else:
            stability_score = 100
        
        return {
            "total_tests": total_tests,
            "total_runs": analysis["total_runs"],
            "flaky_tests_count": flaky_count,
            "consistent_failures_count": consistent_failure_count,
            "performance_issues_count": performance_issue_count,
            "stability_score": round(stability_score, 2),
            "stability_grade": self._get_stability_grade(stability_score),
            "recommendations": self._generate_recommendations(analysis)
        }
    
    def _get_stability_grade(self, score: float) -> str:
        """æ ¹æ“šç©©å®šæ€§åˆ†æ•¸çµ¦äºˆç­‰ç´š"""
        if score >= 95:
            return "A+"
        elif score >= 90:
            return "A"
        elif score >= 85:
            return "B+"
        elif score >= 80:
            return "B"
        elif score >= 75:
            return "C+"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"
    
    def _generate_recommendations(self, analysis: Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        if analysis["flaky_tests"]:
            recommendations.append(f"ä¿®å¾© {len(analysis['flaky_tests'])} å€‹ä¸ç©©å®šæ¸¬è©¦ï¼Œé€™äº›æ¸¬è©¦æœƒé–“æ­‡æ€§å¤±æ•—")
        
        if analysis["consistent_failures"]:
            recommendations.append(f"è§£æ±º {len(analysis['consistent_failures'])} å€‹æŒçºŒå¤±æ•—çš„æ¸¬è©¦")
        
        if analysis["performance_issues"]:
            recommendations.append(f"å„ªåŒ– {len(analysis['performance_issues'])} å€‹æ•ˆèƒ½å•é¡Œæ¸¬è©¦")
        
        # æ ¹æ“šç©©å®šæ€§åˆ†æ•¸çµ¦å‡ºå»ºè­°
        score = analysis["summary"]["stability_score"]
        if score < 80:
            recommendations.append("æ•´é«”æ¸¬è©¦ç©©å®šæ€§éœ€è¦é¡¯è‘—æ”¹å–„")
        elif score < 90:
            recommendations.append("æ¸¬è©¦ç©©å®šæ€§æœ‰æ”¹å–„ç©ºé–“")
        
        if not recommendations:
            recommendations.append("æ¸¬è©¦ç©©å®šæ€§è‰¯å¥½ï¼Œä¿æŒç¾ç‹€")
        
        return recommendations
    
    def _generate_report(self, analysis: Dict[str, Any], source_files: List[Path]) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´å ±å‘Š"""
        
        return {
            "generated_at": datetime.now().isoformat(),
            "analysis_metadata": {
                "source_files": [str(f) for f in source_files],
                "analyzer_version": "1.0.0"
            },
            "stability_analysis": analysis
        }
    
    def _save_report(self, report: Dict[str, Any]):
        """ä¿å­˜åˆ†æå ±å‘Š"""
        try:
            self.output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Stability analysis report saved to: {self.output_path}")
            
        except Exception as e:
            logger.error(f"Failed to save report: {e}")
            raise


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="Analyze test stability from multiple test runs")
    parser.add_argument("--input-dir", required=True,
                       help="Directory containing stability test result files")
    parser.add_argument("--output", required=True,
                       help="Output path for stability analysis JSON report")
    parser.add_argument("--verbose", action="store_true",
                       help="Enable verbose logging")
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
    
    # åŸ·è¡Œåˆ†æ
    try:
        analyzer = StabilityAnalyzer(
            input_dir=Path(args.input_dir),
            output_path=Path(args.output)
        )
        
        report = analyzer.analyze()
        
        if report.get("status") == "no_data":
            print("âš ï¸ No stability test data found")
            return 0
        
        analysis = report["stability_analysis"]
        summary = analysis["summary"]
        
        print(f"ğŸ“Š Stability Analysis Results:")
        print(f"   Total tests analyzed: {summary['total_tests']}")
        print(f"   Total test runs: {summary['total_runs']}")
        print(f"   Stability score: {summary['stability_score']}/100 (Grade: {summary['stability_grade']})")
        
        if analysis["flaky_tests"]:
            print(f"âš ï¸  Flaky tests detected: {len(analysis['flaky_tests'])}")
            
        if analysis["consistent_failures"]:
            print(f"âŒ Consistent failures: {len(analysis['consistent_failures'])}")
            
        if analysis["performance_issues"]:
            print(f"ğŸŒ Performance issues: {len(analysis['performance_issues'])}")
        
        print(f"ğŸ“„ Full report saved to: {args.output}")
        
    except Exception as e:
        logger.error(f"Failed to analyze stability: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())