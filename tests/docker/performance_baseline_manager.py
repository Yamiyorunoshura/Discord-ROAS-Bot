"""
Docker æ¸¬è©¦æ¡†æ¶æ•ˆèƒ½åŸºæº–ç®¡ç†å’Œå›æ­¸æª¢æ¸¬ç³»çµ±
Task ID: T1 - æ•ˆèƒ½å„ªåŒ–å°ˆé–€åŒ–

Ethan æ•ˆèƒ½å°ˆå®¶çš„åŸºæº–ç®¡ç†æ ¸å¿ƒå¯¦ä½œï¼š
- æ•ˆèƒ½åŸºæº–å»ºç«‹å’Œç¶­è­·
- è‡ªå‹•åŒ–å›æ­¸æª¢æ¸¬
- æ­·å²è¶¨å‹¢åˆ†æ
- æ•ˆèƒ½é€€åŒ–å‘Šè­¦
- åŸºæº–ç‰ˆæœ¬ç®¡ç†
"""

import time
import json
import logging
import statistics
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
from collections import defaultdict, deque
import hashlib
import uuid

logger = logging.getLogger(__name__)


class PerformanceMetricType(Enum):
    """æ•ˆèƒ½æŒ‡æ¨™é¡å‹"""
    EXECUTION_TIME = "execution_time"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"
    SUCCESS_RATE = "success_rate"
    THROUGHPUT = "throughput"
    LATENCY = "latency"
    RESOURCE_EFFICIENCY = "resource_efficiency"


class RegressionSeverity(Enum):
    """å›æ­¸åš´é‡ç¨‹åº¦"""
    NONE = "none"
    MINOR = "minor"         # 5-15% é€€åŒ–
    MODERATE = "moderate"   # 15-30% é€€åŒ–
    MAJOR = "major"         # 30-50% é€€åŒ–
    CRITICAL = "critical"   # >50% é€€åŒ–


@dataclass
class PerformanceMetric:
    """æ•ˆèƒ½æŒ‡æ¨™"""
    metric_type: PerformanceMetricType
    value: float
    unit: str
    timestamp: float
    test_context: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'metric_type': self.metric_type.value,
            'value': self.value,
            'unit': self.unit,
            'timestamp': self.timestamp,
            'test_context': self.test_context
        }


@dataclass
class PerformanceBaseline:
    """æ•ˆèƒ½åŸºæº–"""
    baseline_id: str
    version: str
    created_at: float
    test_suite_config: Dict[str, Any]
    metrics: Dict[PerformanceMetricType, Dict[str, float]]  # çµ±è¨ˆæ•¸æ“š (mean, std, min, max)
    sample_size: int
    confidence_level: float = 0.95
    notes: str = ""
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'baseline_id': self.baseline_id,
            'version': self.version,
            'created_at': self.created_at,
            'test_suite_config': self.test_suite_config,
            'metrics': {k.value: v for k, v in self.metrics.items()},
            'sample_size': self.sample_size,
            'confidence_level': self.confidence_level,
            'notes': self.notes
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PerformanceBaseline':
        metrics = {PerformanceMetricType(k): v for k, v in data['metrics'].items()}
        return cls(
            baseline_id=data['baseline_id'],
            version=data['version'],
            created_at=data['created_at'],
            test_suite_config=data['test_suite_config'],
            metrics=metrics,
            sample_size=data['sample_size'],
            confidence_level=data.get('confidence_level', 0.95),
            notes=data.get('notes', '')
        )


@dataclass
class RegressionDetectionResult:
    """å›æ­¸æª¢æ¸¬çµæœ"""
    metric_type: PerformanceMetricType
    baseline_value: float
    current_value: float
    regression_percent: float
    severity: RegressionSeverity
    is_regression: bool
    confidence: float
    statistical_significance: bool
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'metric_type': self.metric_type.value,
            'baseline_value': self.baseline_value,
            'current_value': self.current_value,
            'regression_percent': self.regression_percent,
            'severity': self.severity.value,
            'is_regression': self.is_regression,
            'confidence': self.confidence,
            'statistical_significance': self.statistical_significance
        }


class PerformanceBaselineManager:
    """æ•ˆèƒ½åŸºæº–ç®¡ç†å™¨
    
    ç®¡ç†æ•ˆèƒ½åŸºæº–çš„å»ºç«‹ã€æ›´æ–°ã€ç‰ˆæœ¬æ§åˆ¶ï¼š
    - è‡ªå‹•åŸºæº–å»ºç«‹å’Œç¶­è­·
    - ç‰ˆæœ¬åŒ–åŸºæº–ç®¡ç†
    - åŸºæº–æ¯”è¼ƒå’Œé¸æ“‡
    - åŸºæº–æ•¸æ“šæŒä¹…åŒ–
    """
    
    def __init__(self, storage_path: str = "/Users/tszkinlai/Coding/roas-bot/test_reports"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        self.baselines_file = self.storage_path / "performance_baselines.json"
        
        # è¼‰å…¥ç¾æœ‰åŸºæº–
        self.baselines: Dict[str, PerformanceBaseline] = {}
        self.load_baselines()
        
        # åŸºæº–å»ºç«‹é…ç½®
        self.min_sample_size = 10
        self.max_sample_size = 100
        self.confidence_level = 0.95
        
    def create_baseline_from_metrics(
        self, 
        version: str,
        raw_metrics: List[PerformanceMetric],
        test_suite_config: Dict[str, Any],
        notes: str = ""
    ) -> PerformanceBaseline:
        """å¾åŸå§‹æŒ‡æ¨™å‰µå»ºåŸºæº–"""
        if len(raw_metrics) < self.min_sample_size:
            raise ValueError(f"æ¨£æœ¬æ•¸é‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {self.min_sample_size} å€‹æ¨£æœ¬")
        
        # æŒ‰æŒ‡æ¨™é¡å‹åˆ†çµ„
        metrics_by_type = defaultdict(list)
        for metric in raw_metrics:
            metrics_by_type[metric.metric_type].append(metric.value)
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        baseline_metrics = {}
        for metric_type, values in metrics_by_type.items():
            if values:
                baseline_metrics[metric_type] = {
                    'mean': statistics.mean(values),
                    'std': statistics.stdev(values) if len(values) > 1 else 0.0,
                    'min': min(values),
                    'max': max(values),
                    'median': statistics.median(values),
                    'p95': self._calculate_percentile(values, 0.95),
                    'p99': self._calculate_percentile(values, 0.99)
                }
        
        # å‰µå»ºåŸºæº–
        baseline_id = self._generate_baseline_id(version, test_suite_config)
        baseline = PerformanceBaseline(
            baseline_id=baseline_id,
            version=version,
            created_at=time.time(),
            test_suite_config=test_suite_config,
            metrics=baseline_metrics,
            sample_size=len(raw_metrics),
            confidence_level=self.confidence_level,
            notes=notes
        )
        
        # å„²å­˜åŸºæº–
        self.baselines[baseline_id] = baseline
        self.save_baselines()
        
        logger.info(f"å·²å‰µå»ºæ•ˆèƒ½åŸºæº–: {baseline_id} (ç‰ˆæœ¬: {version}, æ¨£æœ¬: {len(raw_metrics)})")
        return baseline
    
    def create_baseline_from_test_results(
        self,
        version: str,
        test_results: Dict[str, Any],
        notes: str = ""
    ) -> PerformanceBaseline:
        """å¾æ¸¬è©¦çµæœå‰µå»ºåŸºæº–"""
        raw_metrics = self._extract_metrics_from_test_results(test_results)
        test_suite_config = self._extract_test_config(test_results)
        
        return self.create_baseline_from_metrics(
            version=version,
            raw_metrics=raw_metrics,
            test_suite_config=test_suite_config,
            notes=notes
        )
    
    def get_baseline(self, baseline_id: Optional[str] = None, version: Optional[str] = None) -> Optional[PerformanceBaseline]:
        """ç²å–åŸºæº–"""
        if baseline_id:
            return self.baselines.get(baseline_id)
        
        if version:
            # æŸ¥æ‰¾æŒ‡å®šç‰ˆæœ¬çš„åŸºæº–
            for baseline in self.baselines.values():
                if baseline.version == version:
                    return baseline
        
        # è¿”å›æœ€æ–°çš„åŸºæº–
        if self.baselines:
            latest_baseline = max(self.baselines.values(), key=lambda b: b.created_at)
            return latest_baseline
        
        return None
    
    def list_baselines(self) -> List[PerformanceBaseline]:
        """åˆ—å‡ºæ‰€æœ‰åŸºæº–"""
        return sorted(self.baselines.values(), key=lambda b: b.created_at, reverse=True)
    
    def update_baseline(self, baseline_id: str, additional_metrics: List[PerformanceMetric]) -> PerformanceBaseline:
        """æ›´æ–°åŸºæº–ï¼ˆå¢åŠ æ›´å¤šæ¨£æœ¬ï¼‰"""
        if baseline_id not in self.baselines:
            raise ValueError(f"åŸºæº–ä¸å­˜åœ¨: {baseline_id}")
        
        baseline = self.baselines[baseline_id]
        
        # é‡æ–°è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›å¯¦ä½œå¯èƒ½éœ€è¦æ›´è¤‡é›œçš„å¢é‡æ›´æ–°é‚è¼¯
        logger.info(f"åŸºæº–æ›´æ–°åŠŸèƒ½æš«ä¸æ”¯æ´å¢é‡æ›´æ–°ï¼Œå»ºè­°é‡æ–°å‰µå»ºåŸºæº–")
        return baseline
    
    def delete_baseline(self, baseline_id: str) -> bool:
        """åˆªé™¤åŸºæº–"""
        if baseline_id in self.baselines:
            del self.baselines[baseline_id]
            self.save_baselines()
            logger.info(f"å·²åˆªé™¤åŸºæº–: {baseline_id}")
            return True
        return False
    
    def save_baselines(self) -> None:
        """å„²å­˜åŸºæº–åˆ°æ–‡ä»¶"""
        baselines_data = {
            baseline_id: baseline.to_dict()
            for baseline_id, baseline in self.baselines.items()
        }
        
        with open(self.baselines_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'version': '1.0',
                    'created_at': time.time(),
                    'total_baselines': len(baselines_data)
                },
                'baselines': baselines_data
            }, f, indent=2, ensure_ascii=False)
    
    def load_baselines(self) -> None:
        """å¾æ–‡ä»¶è¼‰å…¥åŸºæº–"""
        if not self.baselines_file.exists():
            return
        
        try:
            with open(self.baselines_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            baselines_data = data.get('baselines', {})
            self.baselines = {
                baseline_id: PerformanceBaseline.from_dict(baseline_data)
                for baseline_id, baseline_data in baselines_data.items()
            }
            
            logger.info(f"å·²è¼‰å…¥ {len(self.baselines)} å€‹æ•ˆèƒ½åŸºæº–")
            
        except Exception as e:
            logger.error(f"è¼‰å…¥åŸºæº–å¤±æ•—: {e}")
            self.baselines = {}
    
    def _extract_metrics_from_test_results(self, test_results: Dict[str, Any]) -> List[PerformanceMetric]:
        """å¾æ¸¬è©¦çµæœæå–æŒ‡æ¨™"""
        metrics = []
        current_time = time.time()
        
        # å¾ä¸åŒçš„çµæœçµæ§‹ä¸­æå–æŒ‡æ¨™
        if 'execution_summary' in test_results:
            summary = test_results['execution_summary']
            
            # åŸ·è¡Œæ™‚é–“æŒ‡æ¨™
            if 'total_execution_time_seconds' in summary:
                metrics.append(PerformanceMetric(
                    metric_type=PerformanceMetricType.EXECUTION_TIME,
                    value=summary['total_execution_time_seconds'],
                    unit='seconds',
                    timestamp=current_time
                ))
            
            # æˆåŠŸç‡æŒ‡æ¨™
            if 'success_rate_percent' in summary:
                metrics.append(PerformanceMetric(
                    metric_type=PerformanceMetricType.SUCCESS_RATE,
                    value=summary['success_rate_percent'],
                    unit='percent',
                    timestamp=current_time
                ))
            
            # ååé‡æŒ‡æ¨™
            if 'total_tests' in summary and 'total_execution_time_seconds' in summary:
                throughput = summary['total_tests'] / summary['total_execution_time_seconds']
                metrics.append(PerformanceMetric(
                    metric_type=PerformanceMetricType.THROUGHPUT,
                    value=throughput,
                    unit='tests/second',
                    timestamp=current_time
                ))
        
        # å¾æ•ˆèƒ½åˆ†æä¸­æå–è³‡æºæŒ‡æ¨™
        if 'performance_analysis' in test_results:
            analysis = test_results['performance_analysis']
            
            if 'resource_efficiency_analysis' in analysis:
                resource = analysis['resource_efficiency_analysis']
                
                # è¨˜æ†¶é«”æŒ‡æ¨™
                if 'average_memory_usage_mb' in resource:
                    metrics.append(PerformanceMetric(
                        metric_type=PerformanceMetricType.MEMORY_USAGE,
                        value=resource['average_memory_usage_mb'],
                        unit='MB',
                        timestamp=current_time
                    ))
                
                # CPU æŒ‡æ¨™
                if 'average_cpu_usage_percent' in resource:
                    metrics.append(PerformanceMetric(
                        metric_type=PerformanceMetricType.CPU_USAGE,
                        value=resource['average_cpu_usage_percent'],
                        unit='percent',
                        timestamp=current_time
                    ))
        
        return metrics
    
    def _extract_test_config(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ¸¬è©¦é…ç½®"""
        config = {}
        
        if 'execution_metadata' in test_results:
            metadata = test_results['execution_metadata']
            config['strategy'] = metadata.get('strategy')
            config['total_batches'] = metadata.get('total_batches')
            config['scalability_profile'] = metadata.get('scalability_profile')
        
        if 'execution_summary' in test_results:
            summary = test_results['execution_summary']
            config['total_tests'] = summary.get('total_tests')
        
        return config
    
    def _generate_baseline_id(self, version: str, config: Dict[str, Any]) -> str:
        """ç”ŸæˆåŸºæº–ID"""
        config_hash = hashlib.md5(json.dumps(config, sort_keys=True).encode()).hexdigest()[:8]
        return f"baseline_{version}_{config_hash}"
    
    def _calculate_percentile(self, values: List[float], percentile: float) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        sorted_values = sorted(values)
        index = int(percentile * (len(sorted_values) - 1))
        return sorted_values[index]


class RegressionDetector:
    """æ•ˆèƒ½å›æ­¸æª¢æ¸¬å™¨
    
    æª¢æ¸¬æ•ˆèƒ½å›æ­¸å’Œæ”¹é€²ï¼š
    - çµ±è¨ˆé¡¯è‘—æ€§æª¢æ¸¬
    - è¶¨å‹¢åˆ†æ
    - å¤šæŒ‡æ¨™ç¶œåˆè©•ä¼°
    - æ™ºèƒ½å‘Šè­¦æ©Ÿåˆ¶
    """
    
    def __init__(self, baseline_manager: PerformanceBaselineManager):
        self.baseline_manager = baseline_manager
        
        # å›æ­¸é–¾å€¼é…ç½®
        self.regression_thresholds = {
            RegressionSeverity.MINOR: 0.05,      # 5%
            RegressionSeverity.MODERATE: 0.15,   # 15%
            RegressionSeverity.MAJOR: 0.30,      # 30%
            RegressionSeverity.CRITICAL: 0.50    # 50%
        }
        
        # æª¢æ¸¬æ­·å²
        self.detection_history: List[Dict[str, Any]] = []
    
    def detect_regression(
        self, 
        current_metrics: List[PerformanceMetric],
        baseline_id: Optional[str] = None,
        significance_level: float = 0.05
    ) -> Dict[str, Any]:
        """æª¢æ¸¬æ•ˆèƒ½å›æ­¸"""
        
        # ç²å–åŸºæº–
        baseline = self.baseline_manager.get_baseline(baseline_id)
        if not baseline:
            logger.warning("æœªæ‰¾åˆ°æ•ˆèƒ½åŸºæº–ï¼Œç„¡æ³•é€²è¡Œå›æ­¸æª¢æ¸¬")
            return {'error': 'æœªæ‰¾åˆ°æ•ˆèƒ½åŸºæº–'}
        
        logger.info(f"ä½¿ç”¨åŸºæº– {baseline.baseline_id} é€²è¡Œå›æ­¸æª¢æ¸¬")
        
        # æŒ‰æŒ‡æ¨™é¡å‹åˆ†çµ„ç•¶å‰æŒ‡æ¨™
        current_metrics_by_type = defaultdict(list)
        for metric in current_metrics:
            current_metrics_by_type[metric.metric_type].append(metric.value)
        
        detection_results = []
        overall_regression_detected = False
        max_severity = RegressionSeverity.NONE
        
        # æª¢æ¸¬æ¯å€‹æŒ‡æ¨™é¡å‹
        for metric_type, baseline_stats in baseline.metrics.items():
            if metric_type not in current_metrics_by_type:
                continue
            
            current_values = current_metrics_by_type[metric_type]
            current_mean = statistics.mean(current_values)
            baseline_mean = baseline_stats['mean']
            
            # è¨ˆç®—å›æ­¸ç™¾åˆ†æ¯”
            if baseline_mean != 0:
                regression_percent = (current_mean - baseline_mean) / baseline_mean
            else:
                regression_percent = 0.0
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºå›æ­¸ï¼ˆæ ¹æ“šæŒ‡æ¨™é¡å‹æ±ºå®šæ–¹å‘ï¼‰
            is_regression = self._is_regression_by_metric_type(metric_type, regression_percent)
            
            # ç¢ºå®šåš´é‡ç¨‹åº¦
            severity = self._determine_regression_severity(abs(regression_percent))
            
            # çµ±è¨ˆé¡¯è‘—æ€§æª¢æ¸¬ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            statistical_significance = self._test_statistical_significance(
                current_values, baseline_stats, significance_level
            )
            
            # ä¿¡å¿ƒåº¦è¨ˆç®—
            confidence = self._calculate_confidence(
                current_values, baseline_stats, statistical_significance
            )
            
            result = RegressionDetectionResult(
                metric_type=metric_type,
                baseline_value=baseline_mean,
                current_value=current_mean,
                regression_percent=regression_percent * 100,  # è½‰ç‚ºç™¾åˆ†æ¯”
                severity=severity,
                is_regression=is_regression,
                confidence=confidence,
                statistical_significance=statistical_significance
            )
            
            detection_results.append(result)
            
            if is_regression:
                overall_regression_detected = True
                if severity.value > max_severity.value:
                    max_severity = severity
        
        # ç”Ÿæˆæª¢æ¸¬å ±å‘Š
        detection_report = {
            'detection_metadata': {
                'timestamp': time.time(),
                'baseline_id': baseline.baseline_id,
                'baseline_version': baseline.version,
                'current_sample_size': len(current_metrics),
                'baseline_sample_size': baseline.sample_size
            },
            'overall_assessment': {
                'regression_detected': overall_regression_detected,
                'max_severity': max_severity.value,
                'total_metrics_analyzed': len(detection_results),
                'regressed_metrics_count': sum(1 for r in detection_results if r.is_regression)
            },
            'detailed_results': [result.to_dict() for result in detection_results],
            'recommendations': self._generate_regression_recommendations(detection_results)
        }
        
        # è¨˜éŒ„æª¢æ¸¬æ­·å²
        self.detection_history.append(detection_report)
        
        # è§¸ç™¼å‘Šè­¦
        if overall_regression_detected:
            self._trigger_regression_alert(detection_report)
        
        return detection_report
    
    def analyze_performance_trends(self, days: int = 30) -> Dict[str, Any]:
        """åˆ†ææ•ˆèƒ½è¶¨å‹¢"""
        cutoff_time = time.time() - (days * 24 * 3600)
        recent_detections = [
            d for d in self.detection_history 
            if d['detection_metadata']['timestamp'] > cutoff_time
        ]
        
        if not recent_detections:
            return {'error': f'éå» {days} å¤©å…§ç„¡æª¢æ¸¬æ•¸æ“š'}
        
        # åˆ†æè¶¨å‹¢
        trend_analysis = {
            'analysis_period_days': days,
            'total_detections': len(recent_detections),
            'regression_frequency': sum(1 for d in recent_detections if d['overall_assessment']['regression_detected']),
            'trend_by_metric': self._analyze_metric_trends(recent_detections),
            'severity_distribution': self._analyze_severity_distribution(recent_detections),
            'performance_stability_score': self._calculate_stability_score(recent_detections)
        }
        
        return trend_analysis
    
    def _is_regression_by_metric_type(self, metric_type: PerformanceMetricType, change_percent: float) -> bool:
        """æ ¹æ“šæŒ‡æ¨™é¡å‹åˆ¤æ–·æ˜¯å¦ç‚ºå›æ­¸"""
        # å°æ–¼é€™äº›æŒ‡æ¨™ï¼Œå¢åŠ ä»£è¡¨é€€åŒ–
        degrading_metrics = {
            PerformanceMetricType.EXECUTION_TIME,
            PerformanceMetricType.MEMORY_USAGE,
            PerformanceMetricType.CPU_USAGE,
            PerformanceMetricType.LATENCY
        }
        
        # å°æ–¼é€™äº›æŒ‡æ¨™ï¼Œæ¸›å°‘ä»£è¡¨é€€åŒ–
        improving_metrics = {
            PerformanceMetricType.SUCCESS_RATE,
            PerformanceMetricType.THROUGHPUT,
            PerformanceMetricType.RESOURCE_EFFICIENCY
        }
        
        threshold = self.regression_thresholds[RegressionSeverity.MINOR]
        
        if metric_type in degrading_metrics:
            return change_percent > threshold
        elif metric_type in improving_metrics:
            return change_percent < -threshold
        else:
            # é è¨­è¡Œç‚ºï¼šä»»ä½•é¡¯è‘—è®ŠåŒ–éƒ½è¦–ç‚ºéœ€è¦é—œæ³¨
            return abs(change_percent) > threshold
    
    def _determine_regression_severity(self, abs_change_percent: float) -> RegressionSeverity:
        """ç¢ºå®šå›æ­¸åš´é‡ç¨‹åº¦"""
        if abs_change_percent >= self.regression_thresholds[RegressionSeverity.CRITICAL]:
            return RegressionSeverity.CRITICAL
        elif abs_change_percent >= self.regression_thresholds[RegressionSeverity.MAJOR]:
            return RegressionSeverity.MAJOR
        elif abs_change_percent >= self.regression_thresholds[RegressionSeverity.MODERATE]:
            return RegressionSeverity.MODERATE
        elif abs_change_percent >= self.regression_thresholds[RegressionSeverity.MINOR]:
            return RegressionSeverity.MINOR
        else:
            return RegressionSeverity.NONE
    
    def _test_statistical_significance(
        self, 
        current_values: List[float],
        baseline_stats: Dict[str, float],
        significance_level: float
    ) -> bool:
        """æ¸¬è©¦çµ±è¨ˆé¡¯è‘—æ€§ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        if len(current_values) < 3:
            return False
        
        current_mean = statistics.mean(current_values)
        current_std = statistics.stdev(current_values) if len(current_values) > 1 else 0
        baseline_mean = baseline_stats['mean']
        baseline_std = baseline_stats['std']
        
        # ç°¡åŒ–çš„ t-test æª¢é©—
        if current_std == 0 and baseline_std == 0:
            return abs(current_mean - baseline_mean) > 0.001
        
        # åˆä½µæ¨™æº–èª¤å·®
        pooled_std = ((current_std ** 2) + (baseline_std ** 2)) ** 0.5
        if pooled_std == 0:
            return False
        
        # è¨ˆç®— t çµ±è¨ˆé‡
        t_stat = abs(current_mean - baseline_mean) / pooled_std
        
        # ç°¡åŒ–åˆ¤æ–·ï¼št > 2 èªç‚ºé¡¯è‘—
        return t_stat > 2.0
    
    def _calculate_confidence(
        self, 
        current_values: List[float],
        baseline_stats: Dict[str, float],
        statistical_significance: bool
    ) -> float:
        """è¨ˆç®—ä¿¡å¿ƒåº¦"""
        base_confidence = 0.5
        
        # æ¨£æœ¬å¤§å°å½±éŸ¿
        sample_size_factor = min(len(current_values) / 10, 1.0) * 0.2
        
        # çµ±è¨ˆé¡¯è‘—æ€§å½±éŸ¿
        significance_factor = 0.3 if statistical_significance else 0.0
        
        # è®ŠåŒ–å¹…åº¦å½±éŸ¿ï¼ˆè®ŠåŒ–è¶Šå¤§ï¼Œä¿¡å¿ƒåº¦è¶Šé«˜ï¼‰
        current_mean = statistics.mean(current_values)
        baseline_mean = baseline_stats['mean']
        if baseline_mean != 0:
            change_magnitude = abs((current_mean - baseline_mean) / baseline_mean)
            magnitude_factor = min(change_magnitude * 2, 0.3)
        else:
            magnitude_factor = 0.0
        
        confidence = base_confidence + sample_size_factor + significance_factor + magnitude_factor
        return min(confidence, 1.0)
    
    def _generate_regression_recommendations(self, results: List[RegressionDetectionResult]) -> List[str]:
        """ç”Ÿæˆå›æ­¸å»ºè­°"""
        recommendations = []
        
        regressed_results = [r for r in results if r.is_regression]
        if not regressed_results:
            recommendations.append("æœªæª¢æ¸¬åˆ°æ•ˆèƒ½å›æ­¸ï¼Œç¶­æŒç•¶å‰å„ªåŒ–ç­–ç•¥")
            return recommendations
        
        # æŒ‰åš´é‡ç¨‹åº¦åˆ†æ
        critical_regressions = [r for r in regressed_results if r.severity == RegressionSeverity.CRITICAL]
        major_regressions = [r for r in regressed_results if r.severity == RegressionSeverity.MAJOR]
        
        if critical_regressions:
            recommendations.append(f"ğŸš¨ åš´é‡æ•ˆèƒ½å›æ­¸: {len(critical_regressions)} å€‹æŒ‡æ¨™ï¼Œéœ€ç«‹å³èª¿æŸ¥å’Œä¿®å¾©")
        
        if major_regressions:
            recommendations.append(f"âš ï¸ é‡å¤§æ•ˆèƒ½å›æ­¸: {len(major_regressions)} å€‹æŒ‡æ¨™ï¼Œå»ºè­°å„ªå…ˆè™•ç†")
        
        # å…·é«”æŒ‡æ¨™å»ºè­°
        for result in regressed_results:
            if result.severity in [RegressionSeverity.CRITICAL, RegressionSeverity.MAJOR]:
                if result.metric_type == PerformanceMetricType.EXECUTION_TIME:
                    recommendations.append("åŸ·è¡Œæ™‚é–“é€€åŒ–ï¼Œæª¢æŸ¥ä¸¦è¡Œç­–ç•¥å’Œè³‡æºåˆ†é…")
                elif result.metric_type == PerformanceMetricType.MEMORY_USAGE:
                    recommendations.append("è¨˜æ†¶é«”ä½¿ç”¨å¢åŠ ï¼Œæª¢æŸ¥è¨˜æ†¶é«”æ´©æ¼å’Œæ¸…ç†æ©Ÿåˆ¶")
                elif result.metric_type == PerformanceMetricType.SUCCESS_RATE:
                    recommendations.append("æ¸¬è©¦æˆåŠŸç‡ä¸‹é™ï¼Œæª¢æŸ¥æ¸¬è©¦ç©©å®šæ€§å’ŒéŒ¯èª¤è™•ç†")
        
        return recommendations
    
    def _trigger_regression_alert(self, detection_report: Dict[str, Any]) -> None:
        """è§¸ç™¼å›æ­¸å‘Šè­¦"""
        severity = detection_report['overall_assessment']['max_severity']
        regressed_count = detection_report['overall_assessment']['regressed_metrics_count']
        
        logger.warning(f"ğŸš¨ æ•ˆèƒ½å›æ­¸å‘Šè­¦: åš´é‡ç¨‹åº¦={severity}, å½±éŸ¿æŒ‡æ¨™={regressed_count}")
        
        # é€™è£¡å¯ä»¥æ•´åˆå‘Šè­¦ç³»çµ±ï¼Œå¦‚ç™¼é€éƒµä»¶ã€Slack é€šçŸ¥ç­‰
    
    def _analyze_metric_trends(self, recent_detections: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææŒ‡æ¨™è¶¨å‹¢"""
        metric_trends = {}
        
        # æ”¶é›†æ¯å€‹æŒ‡æ¨™çš„æ­·å²æ•¸æ“š
        for detection in recent_detections:
            for result in detection['detailed_results']:
                metric_type = result['metric_type']
                if metric_type not in metric_trends:
                    metric_trends[metric_type] = {
                        'values': [],
                        'regression_count': 0,
                        'timestamps': []
                    }
                
                metric_trends[metric_type]['values'].append(result['current_value'])
                metric_trends[metric_type]['timestamps'].append(detection['detection_metadata']['timestamp'])
                
                if result['is_regression']:
                    metric_trends[metric_type]['regression_count'] += 1
        
        # è¨ˆç®—è¶¨å‹¢
        for metric_type, data in metric_trends.items():
            values = data['values']
            if len(values) >= 2:
                # ç°¡å–®ç·šæ€§è¶¨å‹¢
                slope = (values[-1] - values[0]) / max(len(values) - 1, 1)
                metric_trends[metric_type]['trend_slope'] = slope
                metric_trends[metric_type]['trend_direction'] = 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable'
            else:
                metric_trends[metric_type]['trend_slope'] = 0
                metric_trends[metric_type]['trend_direction'] = 'insufficient_data'
        
        return metric_trends
    
    def _analyze_severity_distribution(self, recent_detections: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æåš´é‡ç¨‹åº¦åˆ†ä½ˆ"""
        severity_counts = defaultdict(int)
        
        for detection in recent_detections:
            severity = detection['overall_assessment']['max_severity']
            severity_counts[severity] += 1
        
        return dict(severity_counts)
    
    def _calculate_stability_score(self, recent_detections: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—æ•ˆèƒ½ç©©å®šæ€§è©•åˆ†ï¼ˆ0-100ï¼‰"""
        if not recent_detections:
            return 100.0
        
        total_detections = len(recent_detections)
        regression_detections = sum(1 for d in recent_detections if d['overall_assessment']['regression_detected'])
        
        # åŸºç¤ç©©å®šæ€§è©•åˆ†
        stability_score = ((total_detections - regression_detections) / total_detections) * 100
        
        # æ ¹æ“šå›æ­¸åš´é‡ç¨‹åº¦èª¿æ•´
        for detection in recent_detections:
            if detection['overall_assessment']['regression_detected']:
                severity = detection['overall_assessment']['max_severity']
                if severity == 'critical':
                    stability_score -= 10
                elif severity == 'major':
                    stability_score -= 5
                elif severity == 'moderate':
                    stability_score -= 2
        
        return max(0.0, stability_score)


# ä¾¿åˆ©å‡½æ•¸å’Œå·¥å» 
def create_baseline_management_system(storage_path: Optional[str] = None) -> Tuple[PerformanceBaselineManager, RegressionDetector]:
    """å‰µå»ºåŸºæº–ç®¡ç†ç³»çµ±"""
    baseline_manager = PerformanceBaselineManager(storage_path or "/Users/tszkinlai/Coding/roas-bot/test_reports")
    regression_detector = RegressionDetector(baseline_manager)
    
    return baseline_manager, regression_detector


def establish_performance_baseline_from_current_results(
    test_results: Dict[str, Any],
    version: str = "v2.4.2",
    notes: str = ""
) -> PerformanceBaseline:
    """å¾ç•¶å‰æ¸¬è©¦çµæœå»ºç«‹æ•ˆèƒ½åŸºæº–"""
    baseline_manager, _ = create_baseline_management_system()
    
    return baseline_manager.create_baseline_from_test_results(
        version=version,
        test_results=test_results,
        notes=notes or f"åŸºæº–å»ºç«‹æ–¼ {datetime.now().isoformat()}"
    )