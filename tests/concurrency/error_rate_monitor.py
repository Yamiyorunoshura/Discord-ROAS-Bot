#!/usr/bin/env python3
"""
T2 - ä½µç™¼æ¸¬è©¦éŒ¯èª¤ç‡ç›£æ§å’Œå ±å‘Šç³»çµ±
æä¾›å¯¦æ™‚çš„éŒ¯èª¤ç‡ç›£æ§ã€è‡ªå‹•åŒ–æ¸¬è©¦å ±å‘Šç”Ÿæˆå’Œæ€§èƒ½åˆ†æåŠŸèƒ½
"""

import asyncio
import json
import logging
import time
import statistics
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path


@dataclass
class ErrorRateMetric:
    """éŒ¯èª¤ç‡æŒ‡æ¨™"""
    timestamp: datetime
    total_operations: int
    successful_operations: int
    failed_operations: int
    error_rate_percentage: float
    error_types: Dict[str, int]
    response_time_ms: float
    concurrent_workers: int
    test_phase: str


@dataclass
class PerformanceThresholds:
    """æ€§èƒ½é–¾å€¼é…ç½®"""
    max_error_rate: float = 1.0  # 1%
    max_p95_response_time_ms: float = 50.0  # 50ms
    min_success_rate: float = 99.0  # 99%
    max_concurrent_failures: int = 5  # æœ€å¤§é€£çºŒå¤±æ•—æ•¸
    alert_error_rate: float = 0.5  # 0.5% é–‹å§‹è­¦å‘Š
    
    def to_dict(self) -> Dict[str, float]:
        return asdict(self)


class ErrorRateMonitor:
    """éŒ¯èª¤ç‡ç›£æ§ç³»çµ±"""
    
    def __init__(self, thresholds: Optional[PerformanceThresholds] = None):
        self.thresholds = thresholds or PerformanceThresholds()
        self.metrics: List[ErrorRateMetric] = []
        self.current_phase = "initialization"
        self.consecutive_failures = 0
        self.alerts: List[str] = []
        self.logger = logging.getLogger('error_rate_monitor')
        
    def record_operation_batch(
        self,
        successful_ops: int,
        failed_ops: int,
        response_times: List[float],
        concurrent_workers: int,
        error_details: Optional[Dict[str, int]] = None,
        phase: str = "unknown"
    ):
        """è¨˜éŒ„ä¸€æ‰¹æ“ä½œçš„çµæœ"""
        total_ops = successful_ops + failed_ops
        error_rate = (failed_ops / total_ops * 100) if total_ops > 0 else 0
        avg_response_time = statistics.mean(response_times) if response_times else 0
        
        metric = ErrorRateMetric(
            timestamp=datetime.now(),
            total_operations=total_ops,
            successful_operations=successful_ops,
            failed_operations=failed_ops,
            error_rate_percentage=error_rate,
            error_types=error_details or {},
            response_time_ms=avg_response_time,
            concurrent_workers=concurrent_workers,
            test_phase=phase
        )
        
        self.metrics.append(metric)
        self.current_phase = phase
        
        # æª¢æŸ¥é–¾å€¼é•å
        self._check_thresholds(metric)
        
        self.logger.debug(
            f"è¨˜éŒ„ {phase} éšæ®µï¼š{successful_ops}/{total_ops} æˆåŠŸï¼Œ"
            f"éŒ¯èª¤ç‡ {error_rate:.2f}%ï¼ŒéŸ¿æ‡‰æ™‚é–“ {avg_response_time:.2f}ms"
        )
    
    def _check_thresholds(self, metric: ErrorRateMetric):
        """æª¢æŸ¥æ€§èƒ½é–¾å€¼"""
        alerts = []
        
        # æª¢æŸ¥éŒ¯èª¤ç‡
        if metric.error_rate_percentage > self.thresholds.max_error_rate:
            alerts.append(
                f"éŒ¯èª¤ç‡ {metric.error_rate_percentage:.2f}% è¶…éé–¾å€¼ {self.thresholds.max_error_rate}%"
            )
            self.consecutive_failures += 1
        else:
            self.consecutive_failures = 0
            
        # æª¢æŸ¥éŸ¿æ‡‰æ™‚é–“ï¼ˆéœ€è¦ P95ï¼Œé€™è£¡ç”¨å¹³å‡æ™‚é–“ä¼°ç®—ï¼‰
        if metric.response_time_ms > self.thresholds.max_p95_response_time_ms:
            alerts.append(
                f"éŸ¿æ‡‰æ™‚é–“ {metric.response_time_ms:.2f}ms å¯èƒ½è¶…é P95 é–¾å€¼ {self.thresholds.max_p95_response_time_ms}ms"
            )
            
        # æª¢æŸ¥æˆåŠŸç‡
        success_rate = (metric.successful_operations / metric.total_operations * 100) if metric.total_operations > 0 else 0
        if success_rate < self.thresholds.min_success_rate:
            alerts.append(
                f"æˆåŠŸç‡ {success_rate:.2f}% ä½æ–¼é–¾å€¼ {self.thresholds.min_success_rate}%"
            )
            
        # æª¢æŸ¥é€£çºŒå¤±æ•—
        if self.consecutive_failures >= self.thresholds.max_concurrent_failures:
            alerts.append(
                f"é€£çºŒ {self.consecutive_failures} æ‰¹æ¬¡æœªé”æ¨™æº–ï¼Œå¯èƒ½éœ€è¦åœæ­¢æ¸¬è©¦"
            )
        
        # è¨˜éŒ„è­¦å‘Š
        if alerts:
            timestamp = metric.timestamp.strftime("%Y-%m-%d %H:%M:%S")
            for alert in alerts:
                alert_msg = f"[{timestamp}] {metric.test_phase}: {alert}"
                self.alerts.append(alert_msg)
                self.logger.warning(alert_msg)
    
    def get_current_stats(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰çµ±è¨ˆ"""
        if not self.metrics:
            return {"error": "ç„¡ç›£æ§è³‡æ–™"}
            
        recent_metrics = self.metrics[-10:]  # æœ€è¿‘ 10 æ‰¹
        
        total_ops = sum(m.total_operations for m in recent_metrics)
        total_successful = sum(m.successful_operations for m in recent_metrics)
        total_failed = sum(m.failed_operations for m in recent_metrics)
        
        return {
            "current_phase": self.current_phase,
            "metrics_count": len(self.metrics),
            "recent_stats": {
                "total_operations": total_ops,
                "successful_operations": total_successful,
                "failed_operations": total_failed,
                "error_rate_percentage": (total_failed / total_ops * 100) if total_ops > 0 else 0,
                "average_response_time_ms": statistics.mean([m.response_time_ms for m in recent_metrics]),
                "average_concurrent_workers": statistics.mean([m.concurrent_workers for m in recent_metrics])
            },
            "consecutive_failures": self.consecutive_failures,
            "active_alerts": len([a for a in self.alerts if "è¶…éé–¾å€¼" in a or "ä½æ–¼é–¾å€¼" in a]),
            "threshold_compliance": self._check_overall_compliance()
        }
    
    def _check_overall_compliance(self) -> Dict[str, bool]:
        """æª¢æŸ¥æ•´é«”åˆè¦æ€§"""
        if not self.metrics:
            return {"overall": False}
            
        recent_metrics = self.metrics[-5:]  # æœ€è¿‘5æ‰¹
        
        # è¨ˆç®—æ•´é«”æŒ‡æ¨™
        total_ops = sum(m.total_operations for m in recent_metrics)
        total_successful = sum(m.successful_operations for m in recent_metrics)
        total_failed = sum(m.failed_operations for m in recent_metrics)
        
        overall_error_rate = (total_failed / total_ops * 100) if total_ops > 0 else 0
        overall_success_rate = (total_successful / total_ops * 100) if total_ops > 0 else 0
        avg_response_time = statistics.mean([m.response_time_ms for m in recent_metrics])
        
        return {
            "error_rate_ok": overall_error_rate <= self.thresholds.max_error_rate,
            "success_rate_ok": overall_success_rate >= self.thresholds.min_success_rate,
            "response_time_ok": avg_response_time <= self.thresholds.max_p95_response_time_ms,
            "no_consecutive_failures": self.consecutive_failures < self.thresholds.max_concurrent_failures,
            "overall": (
                overall_error_rate <= self.thresholds.max_error_rate and
                overall_success_rate >= self.thresholds.min_success_rate and
                avg_response_time <= self.thresholds.max_p95_response_time_ms and
                self.consecutive_failures < self.thresholds.max_concurrent_failures
            )
        }


class ConcurrencyTestReporter:
    """ä½µç™¼æ¸¬è©¦å ±å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "test_reports/concurrency"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger('test_reporter')
        
    def generate_comprehensive_report(
        self,
        monitor: ErrorRateMonitor,
        test_config: Dict[str, Any],
        test_duration: float
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆæ¸¬è©¦å ±å‘Š"""
        
        if not monitor.metrics:
            return {"error": "ç„¡æ¸¬è©¦è³‡æ–™å¯ç”Ÿæˆå ±å‘Š"}
            
        # åŸºæœ¬çµ±è¨ˆ
        total_ops = sum(m.total_operations for m in monitor.metrics)
        total_successful = sum(m.successful_operations for m in monitor.metrics)
        total_failed = sum(m.failed_operations for m in monitor.metrics)
        
        # æ™‚é–“åºåˆ—åˆ†æ
        time_series = self._analyze_time_series(monitor.metrics)
        
        # æ€§èƒ½åˆ†æ
        performance_analysis = self._analyze_performance(monitor.metrics)
        
        # éŒ¯èª¤åˆ†æ
        error_analysis = self._analyze_errors(monitor.metrics)
        
        # T2 åˆè¦æ€§æª¢æŸ¥
        t2_compliance = self._check_t2_compliance(monitor)
        
        report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "test_duration_seconds": test_duration,
                "test_configuration": test_config,
                "metrics_count": len(monitor.metrics),
                "thresholds": monitor.thresholds.to_dict()
            },
            "executive_summary": {
                "total_operations": total_ops,
                "successful_operations": total_successful,
                "failed_operations": total_failed,
                "overall_success_rate_percent": (total_successful / total_ops * 100) if total_ops > 0 else 0,
                "overall_error_rate_percent": (total_failed / total_ops * 100) if total_ops > 0 else 0,
                "average_throughput_ops_per_sec": total_ops / test_duration if test_duration > 0 else 0,
                "test_phases": list(set(m.test_phase for m in monitor.metrics))
            },
            "time_series_analysis": time_series,
            "performance_analysis": performance_analysis,
            "error_analysis": error_analysis,
            "t2_compliance": t2_compliance,
            "alerts_summary": {
                "total_alerts": len(monitor.alerts),
                "critical_alerts": len([a for a in monitor.alerts if "è¶…éé–¾å€¼" in a]),
                "recent_alerts": monitor.alerts[-10:] if len(monitor.alerts) > 10 else monitor.alerts
            },
            "recommendations": self._generate_recommendations(monitor, performance_analysis, error_analysis)
        }
        
        # ä¿å­˜å ±å‘Š
        report_file = self._save_report(report)
        self.logger.info(f"ç¶œåˆå ±å‘Šå·²ç”Ÿæˆï¼š{report_file}")
        
        return report
    
    def _analyze_time_series(self, metrics: List[ErrorRateMetric]) -> Dict[str, Any]:
        """æ™‚é–“åºåˆ—åˆ†æ"""
        if len(metrics) < 2:
            return {"insufficient_data": True}
            
        # æŒ‰éšæ®µåˆ†çµ„
        phases = {}
        for metric in metrics:
            if metric.test_phase not in phases:
                phases[metric.test_phase] = []
            phases[metric.test_phase].append(metric)
        
        phase_analysis = {}
        for phase, phase_metrics in phases.items():
            error_rates = [m.error_rate_percentage for m in phase_metrics]
            response_times = [m.response_time_ms for m in phase_metrics]
            
            phase_analysis[phase] = {
                "metrics_count": len(phase_metrics),
                "duration_minutes": (phase_metrics[-1].timestamp - phase_metrics[0].timestamp).total_seconds() / 60,
                "error_rate_trend": {
                    "start": error_rates[0] if error_rates else 0,
                    "end": error_rates[-1] if error_rates else 0,
                    "max": max(error_rates) if error_rates else 0,
                    "average": statistics.mean(error_rates) if error_rates else 0
                },
                "response_time_trend": {
                    "start_ms": response_times[0] if response_times else 0,
                    "end_ms": response_times[-1] if response_times else 0,
                    "max_ms": max(response_times) if response_times else 0,
                    "average_ms": statistics.mean(response_times) if response_times else 0
                }
            }
        
        return {
            "total_duration_minutes": (metrics[-1].timestamp - metrics[0].timestamp).total_seconds() / 60,
            "phases_analyzed": len(phases),
            "phase_details": phase_analysis
        }
    
    def _analyze_performance(self, metrics: List[ErrorRateMetric]) -> Dict[str, Any]:
        """æ€§èƒ½åˆ†æ"""
        response_times = [m.response_time_ms for m in metrics]
        concurrent_workers = [m.concurrent_workers for m in metrics]
        throughputs = [m.total_operations / 1 for m in metrics]  # ä¼°ç®—æ¯ç§’ååé‡
        
        return {
            "response_time_analysis": {
                "min_ms": min(response_times) if response_times else 0,
                "max_ms": max(response_times) if response_times else 0,
                "average_ms": statistics.mean(response_times) if response_times else 0,
                "median_ms": statistics.median(response_times) if response_times else 0,
                "std_dev_ms": statistics.stdev(response_times) if len(response_times) > 1 else 0
            },
            "concurrency_analysis": {
                "max_concurrent_workers": max(concurrent_workers) if concurrent_workers else 0,
                "average_concurrent_workers": statistics.mean(concurrent_workers) if concurrent_workers else 0
            },
            "throughput_analysis": {
                "max_ops_per_batch": max(throughputs) if throughputs else 0,
                "average_ops_per_batch": statistics.mean(throughputs) if throughputs else 0,
                "total_ops_all_batches": sum(throughputs) if throughputs else 0
            }
        }
    
    def _analyze_errors(self, metrics: List[ErrorRateMetric]) -> Dict[str, Any]:
        """éŒ¯èª¤åˆ†æ"""
        # åˆä½µæ‰€æœ‰éŒ¯èª¤é¡å‹çµ±è¨ˆ
        all_error_types = {}
        total_errors = 0
        
        for metric in metrics:
            total_errors += metric.failed_operations
            for error_type, count in metric.error_types.items():
                if error_type not in all_error_types:
                    all_error_types[error_type] = 0
                all_error_types[error_type] += count
        
        # è¨ˆç®—éŒ¯èª¤æ¨¡å¼
        error_patterns = []
        high_error_phases = [
            m.test_phase for m in metrics 
            if m.error_rate_percentage > 1.0
        ]
        
        if high_error_phases:
            error_patterns.append(f"é«˜éŒ¯èª¤ç‡éšæ®µï¼š{', '.join(set(high_error_phases))}")
        
        # æ‰¾å‡ºæœ€å¸¸è¦‹çš„éŒ¯èª¤é¡å‹
        most_common_error = max(all_error_types.items(), key=lambda x: x[1]) if all_error_types else None
        
        return {
            "total_errors": total_errors,
            "error_types_distribution": all_error_types,
            "most_common_error": {
                "type": most_common_error[0] if most_common_error else None,
                "count": most_common_error[1] if most_common_error else 0,
                "percentage": (most_common_error[1] / total_errors * 100) if most_common_error and total_errors > 0 else 0
            },
            "error_patterns": error_patterns,
            "high_error_rate_phases": list(set(high_error_phases))
        }
    
    def _check_t2_compliance(self, monitor: ErrorRateMonitor) -> Dict[str, Any]:
        """æª¢æŸ¥ T2 æ¨™æº–åˆè¦æ€§"""
        compliance = monitor._check_overall_compliance()
        
        # è©³ç´°çš„åˆè¦æ€§æª¢æŸ¥
        recent_metrics = monitor.metrics[-10:] if len(monitor.metrics) >= 10 else monitor.metrics
        
        total_ops = sum(m.total_operations for m in recent_metrics)
        total_successful = sum(m.successful_operations for m in recent_metrics)
        total_failed = sum(m.failed_operations for m in recent_metrics)
        
        overall_error_rate = (total_failed / total_ops * 100) if total_ops > 0 else 0
        overall_success_rate = (total_successful / total_ops * 100) if total_ops > 0 else 0
        avg_response_time = statistics.mean([m.response_time_ms for m in recent_metrics]) if recent_metrics else 0
        
        return {
            "t2_standards": {
                "max_error_rate_percent": monitor.thresholds.max_error_rate,
                "min_success_rate_percent": monitor.thresholds.min_success_rate,
                "max_p95_response_time_ms": monitor.thresholds.max_p95_response_time_ms
            },
            "measured_performance": {
                "actual_error_rate_percent": overall_error_rate,
                "actual_success_rate_percent": overall_success_rate,
                "average_response_time_ms": avg_response_time
            },
            "compliance_status": compliance,
            "overall_t2_compliant": compliance.get("overall", False),
            "compliance_score": sum(1 for v in compliance.values() if v) / len(compliance) * 100
        }
    
    def _generate_recommendations(
        self,
        monitor: ErrorRateMonitor,
        performance_analysis: Dict[str, Any],
        error_analysis: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼éŒ¯èª¤ç‡çš„å»ºè­°
        if error_analysis["total_errors"] > 0:
            error_rate = (error_analysis["total_errors"] / sum(m.total_operations for m in monitor.metrics)) * 100
            if error_rate > 1.0:
                recommendations.append(f"éŒ¯èª¤ç‡ {error_rate:.2f}% éé«˜ï¼Œå»ºè­°æª¢æŸ¥é€£ç·šæ± é…ç½®å’Œè³‡æ–™åº«æ•ˆèƒ½")
        
        # åŸºæ–¼éŸ¿æ‡‰æ™‚é–“çš„å»ºè­°
        avg_response_time = performance_analysis["response_time_analysis"]["average_ms"]
        if avg_response_time > 20:
            recommendations.append(f"å¹³å‡éŸ¿æ‡‰æ™‚é–“ {avg_response_time:.2f}ms åé«˜ï¼Œå»ºè­°å„ªåŒ–æŸ¥è©¢æˆ–å¢åŠ é€£ç·šæ± å¤§å°")
        
        # åŸºæ–¼ä½µç™¼æ€§çš„å»ºè­°
        max_workers = performance_analysis["concurrency_analysis"]["max_concurrent_workers"]
        if max_workers > 15:
            recommendations.append(f"æœ€å¤§ä½µç™¼å·¥ä½œè€… {max_workers} è¼ƒé«˜ï¼Œç¢ºä¿é€£ç·šæ± é…ç½®è¶³ä»¥æ”¯æŒæ­¤è² è¼‰")
        
        # åŸºæ–¼éŒ¯èª¤æ¨¡å¼çš„å»ºè­°
        if error_analysis["high_error_rate_phases"]:
            recommendations.append(f"åœ¨ {', '.join(error_analysis['high_error_rate_phases'])} éšæ®µéŒ¯èª¤ç‡è¼ƒé«˜ï¼Œéœ€è¦ç‰¹åˆ¥é—œæ³¨")
        
        # é€£çºŒå¤±æ•—çš„å»ºè­°
        if monitor.consecutive_failures > 0:
            recommendations.append(f"æª¢æ¸¬åˆ° {monitor.consecutive_failures} æ¬¡é€£çºŒå¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥ç³»çµ±ç©©å®šæ€§")
        
        # é»˜èªå»ºè­°
        if not recommendations:
            recommendations.append("ç³»çµ±æ•ˆèƒ½è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ä¿æŒç•¶å‰é…ç½®ä¸¦å®šæœŸç›£æ§")
        
        return recommendations
    
    def _save_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"concurrency_test_report_{timestamp}.json"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        return str(filepath)


# ä½¿ç”¨ç¯„ä¾‹å’Œæ¸¬è©¦å·¥å…·
async def demo_monitoring_system():
    """æ¼”ç¤ºç›£æ§ç³»çµ±"""
    print("ğŸ” ä½µç™¼æ¸¬è©¦ç›£æ§ç³»çµ±æ¼”ç¤º")
    
    # åˆå§‹åŒ–ç›£æ§å™¨
    monitor = ErrorRateMonitor()
    reporter = ConcurrencyTestReporter()
    
    # æ¨¡æ“¬ä¸€ç³»åˆ—æ¸¬è©¦æ‰¹æ¬¡
    test_phases = [
        ("warm_up", 5, 0, 10),      # é ç†±ï¼š5æˆåŠŸï¼Œ0å¤±æ•—ï¼Œ10å€‹å·¥ä½œè€…
        ("load_test", 50, 2, 15),   # è² è¼‰æ¸¬è©¦ï¼š50æˆåŠŸï¼Œ2å¤±æ•—ï¼Œ15å€‹å·¥ä½œè€…
        ("stress_test", 100, 8, 20), # å£“åŠ›æ¸¬è©¦ï¼š100æˆåŠŸï¼Œ8å¤±æ•—ï¼Œ20å€‹å·¥ä½œè€…
        ("peak_load", 80, 15, 25),  # å³°å€¼è² è¼‰ï¼š80æˆåŠŸï¼Œ15å¤±æ•—ï¼Œ25å€‹å·¥ä½œè€…
        ("cool_down", 20, 1, 10)    # å†·å»ï¼š20æˆåŠŸï¼Œ1å¤±æ•—ï¼Œ10å€‹å·¥ä½œè€…
    ]
    
    start_time = time.time()
    
    for phase, successful, failed, workers in test_phases:
        # æ¨¡æ“¬éŸ¿æ‡‰æ™‚é–“
        response_times = [10 + i * 2 + (failed * 5) for i in range(successful + failed)]
        
        # æ¨¡æ“¬éŒ¯èª¤é¡å‹
        error_types = {
            "connection_timeout": failed // 2,
            "database_lock": failed - (failed // 2)
        } if failed > 0 else {}
        
        monitor.record_operation_batch(
            successful_ops=successful,
            failed_ops=failed,
            response_times=response_times,
            concurrent_workers=workers,
            error_details=error_types,
            phase=phase
        )
        
        print(f"ğŸ“Š {phase}: {successful}/{successful + failed} æˆåŠŸ")
        
        # çŸ­æš«å»¶é²æ¨¡æ“¬æ¸¬è©¦æ™‚é–“
        await asyncio.sleep(0.1)
    
    test_duration = time.time() - start_time
    
    # ç”Ÿæˆå ±å‘Š
    test_config = {
        "test_type": "concurrency_demo",
        "phases": len(test_phases),
        "max_workers": 25
    }
    
    report = reporter.generate_comprehensive_report(monitor, test_config, test_duration)
    
    print("ğŸ“‹ æ¸¬è©¦å ±å‘Šæ‘˜è¦ï¼š")
    print(f"  ç¸½æ“ä½œæ•¸ï¼š{report['executive_summary']['total_operations']}")
    print(f"  æˆåŠŸç‡ï¼š{report['executive_summary']['overall_success_rate_percent']:.2f}%")
    print(f"  T2åˆè¦ï¼š{'âœ…' if report['t2_compliance']['overall_t2_compliant'] else 'âŒ'}")
    print(f"  å»ºè­°æ•¸ï¼š{len(report['recommendations'])}")
    
    return report


if __name__ == "__main__":
    # è¨­å®šæ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åŸ·è¡Œæ¼”ç¤º
    asyncio.run(demo_monitoring_system())