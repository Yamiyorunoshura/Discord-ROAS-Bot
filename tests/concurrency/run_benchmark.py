#!/usr/bin/env python3
"""
T2 - é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾©
é€£ç·šæ± æ•ˆèƒ½é©—è­‰è…³æœ¬

åŸ·è¡Œå®Œæ•´çš„ä½µç™¼æ¸¬è©¦å¥—ä»¶ï¼Œé©—è­‰é€£ç·šæ± ç®¡ç†å™¨æ˜¯å¦é”åˆ°T2ä»»å‹™çš„æ•ˆèƒ½ç›®æ¨™
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime

# æ·»åŠ ç³»çµ±è·¯å¾‘æ”¯æŒ
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from tests.concurrency.test_connection_pool import ConnectionPoolTestSuite
from services.connection_pool.models import PoolConfiguration

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('connection_pool_benchmark')


async def run_full_benchmark():
    """åŸ·è¡Œå®Œæ•´çš„é€£ç·šæ± åŸºæº–æ¸¬è©¦"""
    logger.info("é–‹å§‹åŸ·è¡ŒT2é€£ç·šæ± æ•ˆèƒ½åŸºæº–æ¸¬è©¦")
    
    # æ¸¬è©¦é…ç½®
    test_configs = [
        {
            "name": "æ¨™æº–é…ç½®",
            "description": "é è¨­é€£ç·šæ± é…ç½®ï¼Œé©ç”¨æ–¼ä¸€èˆ¬å ´æ™¯",
            "config": PoolConfiguration(
                min_connections=2,
                max_connections=20,
                connection_timeout=30.0,
                acquire_timeout=10.0,
                enable_monitoring=True
            )
        },
        {
            "name": "é«˜ä½µç™¼é…ç½®", 
            "description": "é‡å°é«˜ä½µç™¼å ´æ™¯å„ªåŒ–çš„é…ç½®",
            "config": PoolConfiguration(
                min_connections=5,
                max_connections=25,
                connection_timeout=20.0,
                acquire_timeout=5.0,
                enable_monitoring=True
            )
        },
        {
            "name": "ä¿å®ˆé…ç½®",
            "description": "è³‡æºå—é™ç’°å¢ƒä¸‹çš„ä¿å®ˆé…ç½®",
            "config": PoolConfiguration(
                min_connections=1,
                max_connections=10,
                connection_timeout=60.0,
                acquire_timeout=15.0,
                enable_monitoring=True
            )
        }
    ]
    
    all_results = []
    
    for test_config in test_configs:
        logger.info(f"\n{'='*60}")
        logger.info(f"æ¸¬è©¦é…ç½®ï¼š{test_config['name']}")
        logger.info(f"æè¿°ï¼š{test_config['description']}")
        logger.info(f"{'='*60}")
        
        async with ConnectionPoolTestSuite() as test_suite:
            await test_suite.setup_test_environment(test_config['config'])
            
            # åŸ·è¡Œå„ç¨®æ¸¬è©¦å ´æ™¯
            test_scenarios = [
                {
                    "name": "10å·¥ä½œè€…ä½µç™¼è®€å–",
                    "test": lambda: test_suite.run_concurrent_read_test(
                        num_workers=10, operations_per_worker=100
                    )
                },
                {
                    "name": "10å·¥ä½œè€…ä½µç™¼å¯«å…¥", 
                    "test": lambda: test_suite.run_concurrent_write_test(
                        num_workers=10, operations_per_worker=50
                    )
                },
                {
                    "name": "15å·¥ä½œè€…æ··åˆè² è¼‰",
                    "test": lambda: test_suite.run_mixed_workload_test(
                        num_workers=15, read_percentage=70.0, test_duration=30.0
                    )
                },
                {
                    "name": "20å·¥ä½œè€…å£“åŠ›æ¸¬è©¦",
                    "test": lambda: test_suite.run_stress_test(
                        max_workers=20, ramp_up_duration=10.0, test_duration=45.0
                    )
                }
            ]
            
            config_results = {
                "config_name": test_config['name'],
                "config_description": test_config['description'],
                "config_details": {
                    "min_connections": test_config['config'].min_connections,
                    "max_connections": test_config['config'].max_connections,
                    "connection_timeout": test_config['config'].connection_timeout,
                    "acquire_timeout": test_config['config'].acquire_timeout
                },
                "test_results": []
            }
            
            for scenario in test_scenarios:
                logger.info(f"\nåŸ·è¡Œæ¸¬è©¦å ´æ™¯ï¼š{scenario['name']}")
                try:
                    result = await scenario['test']()
                    config_results["test_results"].append({
                        "scenario_name": scenario['name'],
                        "result": {
                            "test_name": result.test_name,
                            "duration_seconds": result.duration_seconds,
                            "total_operations": result.total_operations,
                            "successful_operations": result.successful_operations,
                            "failed_operations": result.failed_operations,
                            "operations_per_second": result.operations_per_second,
                            "error_rate_percentage": result.error_rate_percentage,
                            "response_times": {
                                "average_ms": result.average_response_time_ms,
                                "p50_ms": result.p50_response_time_ms,
                                "p95_ms": result.p95_response_time_ms,
                                "p99_ms": result.p99_response_time_ms
                            },
                            "concurrent_workers": result.concurrent_workers,
                            "max_connections_used": result.max_connections_used
                        }
                    })
                    
                    logger.info(f"âœ“ {scenario['name']} å®Œæˆ")
                    logger.info(f"  æˆåŠŸç‡ï¼š{(result.successful_operations/result.total_operations*100):.2f}%")
                    logger.info(f"  ååé‡ï¼š{result.operations_per_second:.2f} ops/s")
                    logger.info(f"  P95éŸ¿æ‡‰æ™‚é–“ï¼š{result.p95_response_time_ms:.2f} ms")
                    
                except Exception as e:
                    logger.error(f"âœ— {scenario['name']} å¤±æ•—ï¼š{e}")
                    config_results["test_results"].append({
                        "scenario_name": scenario['name'],
                        "error": str(e)
                    })
            
            # ç”Ÿæˆæ•ˆèƒ½å ±å‘Š
            performance_report = test_suite.generate_performance_report()
            config_results["performance_assessment"] = performance_report.get("performance_assessment", {})
            
            all_results.append(config_results)
    
    return all_results


def analyze_benchmark_results(results):
    """åˆ†æåŸºæº–æ¸¬è©¦çµæœ"""
    logger.info(f"\n{'='*80}")
    logger.info("T2ä»»å‹™æ•ˆèƒ½åˆ†æå ±å‘Š")
    logger.info(f"{'='*80}")
    
    # T2ä»»å‹™çš„æ•ˆèƒ½è¦æ±‚
    t2_requirements = {
        "max_acceptable_error_rate": 1.0,  # â‰¤ 1%
        "min_required_throughput": 100,    # â‰¥ 100 ops/s
        "max_acceptable_response": 50,     # â‰¤ 50ms (P95)
        "max_concurrent_workers": 20       # æ”¯æ´20+ä½µç™¼
    }
    
    logger.info("T2ä»»å‹™æ•ˆèƒ½è¦æ±‚ï¼š")
    logger.info(f"  - éŒ¯èª¤ç‡ï¼šâ‰¤ {t2_requirements['max_acceptable_error_rate']}%")
    logger.info(f"  - ååé‡ï¼šâ‰¥ {t2_requirements['min_required_throughput']} ops/s") 
    logger.info(f"  - P95éŸ¿æ‡‰æ™‚é–“ï¼šâ‰¤ {t2_requirements['max_acceptable_response']} ms")
    logger.info(f"  - ä½µç™¼æ”¯æ´ï¼šâ‰¥ {t2_requirements['max_concurrent_workers']} å·¥ä½œè€…")
    
    overall_pass = True
    best_config = None
    best_score = 0
    
    for config_result in results:
        config_name = config_result["config_name"]
        logger.info(f"\n--- {config_name} åˆ†æçµæœ ---")
        
        config_pass = True
        total_score = 0
        scenario_count = 0
        
        for test_result in config_result["test_results"]:
            if "error" in test_result:
                logger.info(f"  âœ— {test_result['scenario_name']}: æ¸¬è©¦å¤±æ•—")
                config_pass = False
                continue
            
            result = test_result["result"]
            scenario_name = test_result["scenario_name"]
            scenario_count += 1
            
            # è¨ˆç®—å ´æ™¯åˆ†æ•¸
            scenario_score = 100
            issues = []
            
            # æª¢æŸ¥éŒ¯èª¤ç‡
            if result["error_rate_percentage"] > t2_requirements["max_acceptable_error_rate"]:
                scenario_score -= 30
                issues.append(f"éŒ¯èª¤ç‡éé«˜ ({result['error_rate_percentage']:.2f}%)")
            
            # æª¢æŸ¥ååé‡
            if result["operations_per_second"] < t2_requirements["min_required_throughput"]:
                scenario_score -= 25
                issues.append(f"ååé‡ä¸è¶³ ({result['operations_per_second']:.2f} ops/s)")
            
            # æª¢æŸ¥éŸ¿æ‡‰æ™‚é–“
            if result["response_times"]["p95_ms"] > t2_requirements["max_acceptable_response"]:
                scenario_score -= 20
                issues.append(f"éŸ¿æ‡‰æ™‚é–“éé•· (P95: {result['response_times']['p95_ms']:.2f} ms)")
            
            # æª¢æŸ¥ä½µç™¼æ”¯æ´
            if result["concurrent_workers"] < t2_requirements["max_concurrent_workers"]:
                scenario_score -= 15
                issues.append(f"ä½µç™¼æ”¯æ´ä¸è¶³ ({result['concurrent_workers']} å·¥ä½œè€…)")
            
            # ç¢ºä¿åˆ†æ•¸ä¸ç‚ºè² 
            scenario_score = max(0, scenario_score)
            total_score += scenario_score
            
            # è¼¸å‡ºå ´æ™¯çµæœ
            status = "âœ“" if scenario_score >= 80 else "âš " if scenario_score >= 60 else "âœ—"
            logger.info(f"  {status} {scenario_name}: {scenario_score}/100 åˆ†")
            logger.info(f"    - æˆåŠŸç‡: {(result['successful_operations']/result['total_operations']*100):.2f}%")
            logger.info(f"    - ååé‡: {result['operations_per_second']:.2f} ops/s")
            logger.info(f"    - P95éŸ¿æ‡‰: {result['response_times']['p95_ms']:.2f} ms")
            logger.info(f"    - ä½µç™¼æ•¸: {result['concurrent_workers']} å·¥ä½œè€…")
            
            if issues:
                logger.info(f"    å•é¡Œ: {', '.join(issues)}")
        
        # è¨ˆç®—é…ç½®ç¸½åˆ†
        avg_score = total_score / scenario_count if scenario_count > 0 else 0
        
        # åˆ¤æ–·æ˜¯å¦é€šéT2è¦æ±‚
        config_grade = "å„ªç§€" if avg_score >= 90 else "è‰¯å¥½" if avg_score >= 80 else "åŠæ ¼" if avg_score >= 60 else "ä¸åŠæ ¼"
        
        logger.info(f"\n  é…ç½®ç¸½è©•ï¼š{avg_score:.1f}/100 åˆ† ({config_grade})")
        
        if avg_score < 60:
            config_pass = False
            overall_pass = False
        
        if avg_score > best_score:
            best_score = avg_score
            best_config = config_name
    
    # ç¸½çµ
    logger.info(f"\n{'='*80}")
    logger.info("ç¸½çµèˆ‡å»ºè­°")
    logger.info(f"{'='*80}")
    
    if overall_pass:
        logger.info("ğŸ‰ T2ä»»å‹™æ•ˆèƒ½è¦æ±‚ï¼šé€šé")
        logger.info(f"ğŸ“Š æœ€ä½³é…ç½®ï¼š{best_config} (å¾—åˆ†: {best_score:.1f}/100)")
        logger.info("âœ… é€£ç·šæ± ç®¡ç†å™¨å·²é”åˆ°æ‰€æœ‰æ•ˆèƒ½æŒ‡æ¨™")
    else:
        logger.info("âŒ T2ä»»å‹™æ•ˆèƒ½è¦æ±‚ï¼šæœªé€šé")
        logger.info("âš ï¸  éœ€è¦å„ªåŒ–ä»¥ä¸‹æ–¹é¢ï¼š")
        logger.info("   1. èª¿æ•´é€£ç·šæ± é…ç½®åƒæ•¸")
        logger.info("   2. å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½")
        logger.info("   3. è€ƒæ…®ç¡¬é«”è³‡æºå‡ç´š")
    
    logger.info("\nå»ºè­°ï¼š")
    logger.info("- åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ä½¿ç”¨æœ€ä½³é…ç½®")
    logger.info("- æŒçºŒç›£æ§é€£ç·šæ± æ•ˆèƒ½æŒ‡æ¨™")
    logger.info("- æ ¹æ“šå¯¦éš›è² è¼‰èª¿æ•´é…ç½®åƒæ•¸")


def save_benchmark_report(results, filename=None):
    """å„²å­˜åŸºæº–æ¸¬è©¦å ±å‘Š"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"t2_connection_pool_benchmark_{timestamp}.json"
    
    report = {
        "benchmark_info": {
            "task_id": "T2",
            "test_name": "é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾© - é€£ç·šæ± æ•ˆèƒ½åŸºæº–æ¸¬è©¦",
            "timestamp": datetime.now().isoformat(),
            "description": "é©—è­‰ConnectionPoolManageræ˜¯å¦é”åˆ°T2ä»»å‹™çš„æ•ˆèƒ½ç›®æ¨™"
        },
        "test_results": results,
        "metadata": {
            "python_version": sys.version,
            "platform": sys.platform
        }
    }
    
    report_dir = os.path.join(project_root, "test_reports")
    os.makedirs(report_dir, exist_ok=True)
    
    report_path = os.path.join(report_dir, filename)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"åŸºæº–æ¸¬è©¦å ±å‘Šå·²å„²å­˜ï¼š{report_path}")
    return report_path


async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    try:
        logger.info("T2 - é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾©")
        logger.info("é€£ç·šæ± ç®¡ç†å™¨æ•ˆèƒ½é©—è­‰é–‹å§‹")
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        results = await run_full_benchmark()
        
        # åˆ†æçµæœ
        analyze_benchmark_results(results)
        
        # å„²å­˜å ±å‘Š
        report_path = save_benchmark_report(results)
        
        logger.info("\nåŸºæº–æ¸¬è©¦å®Œæˆï¼")
        logger.info(f"è©³ç´°å ±å‘Šå·²å„²å­˜è‡³ï¼š{report_path}")
        
    except KeyboardInterrupt:
        logger.info("\næ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        logger.error(f"åŸºæº–æ¸¬è©¦åŸ·è¡Œå¤±æ•—ï¼š{e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())