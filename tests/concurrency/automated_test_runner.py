#!/usr/bin/env python3
"""
T2 - ä½µç™¼æ¸¬è©¦è‡ªå‹•åŒ–å’ŒæŒçºŒé›†æˆç³»çµ±
æä¾›è‡ªå‹•åŒ–çš„æ¸¬è©¦åŸ·è¡Œã€çµæœåˆ†æå’ŒCI/CDé›†æˆåŠŸèƒ½
"""

import asyncio
import json
import logging
import os
import sys
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import subprocess

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from services.connection_pool.connection_pool_manager import ConnectionPoolManager
from services.connection_pool.models import PoolConfiguration
from error_rate_monitor import ErrorRateMonitor, ConcurrencyTestReporter, PerformanceThresholds

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ConcurrencyTestSuite:
    """è‡ªå‹•åŒ–ä½µç™¼æ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self, config_file: Optional[str] = None):
        """åˆå§‹åŒ–æ¸¬è©¦å¥—ä»¶"""
        self.config = self._load_config(config_file)
        self.results_dir = Path(self.config.get('results_dir', 'test_reports/concurrency'))
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        self.monitor = ErrorRateMonitor(
            thresholds=PerformanceThresholds(**self.config.get('thresholds', {}))
        )
        self.reporter = ConcurrencyTestReporter(str(self.results_dir))
        
        self.test_results = []
        self.overall_success = True
    
    def _load_config(self, config_file: Optional[str]) -> Dict[str, Any]:
        """åŠ è¼‰æ¸¬è©¦é…ç½®"""
        default_config = {
            "tests": [
                {
                    "name": "basic_concurrency",
                    "workers": 10,
                    "operations_per_worker": 20,
                    "timeout_seconds": 60
                },
                {
                    "name": "stress_test", 
                    "workers": 20,
                    "operations_per_worker": 15,
                    "timeout_seconds": 120
                },
                {
                    "name": "burst_load",
                    "workers": 25,
                    "operations_per_worker": 10,
                    "timeout_seconds": 90
                }
            ],
            "connection_pool": {
                "min_connections": 3,
                "max_connections": 30,
                "connection_timeout": 15.0,
                "acquire_timeout": 10.0
            },
            "thresholds": {
                "max_error_rate": 1.0,
                "max_p95_response_time_ms": 50.0,
                "min_success_rate": 99.0,
                "max_concurrent_failures": 3
            },
            "results_dir": "test_reports/concurrency",
            "database_path": None  # ä½¿ç”¨è‡¨æ™‚è³‡æ–™åº«
        }
        
        if config_file and Path(config_file).exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    default_config.update(loaded_config)
                    logger.info(f"å·²è¼‰å…¥é…ç½®æ–‡ä»¶ï¼š{config_file}")
            except Exception as e:
                logger.warning(f"ç„¡æ³•è¼‰å…¥é…ç½®æ–‡ä»¶ {config_file}ï¼š{e}ï¼Œä½¿ç”¨é»˜èªé…ç½®")
        
        return default_config
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰é…ç½®çš„æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œè‡ªå‹•åŒ–ä½µç™¼æ¸¬è©¦å¥—ä»¶")
        logger.info(f"æ¸¬è©¦æ•¸é‡ï¼š{len(self.config['tests'])}")
        logger.info(f"çµæœç›®éŒ„ï¼š{self.results_dir}")
        
        start_time = time.time()
        
        for i, test_config in enumerate(self.config['tests'], 1):
            logger.info(f"[{i}/{len(self.config['tests'])}] åŸ·è¡Œæ¸¬è©¦ï¼š{test_config['name']}")
            
            try:
                test_result = await self._run_single_test(test_config)
                self.test_results.append(test_result)
                
                # è¨˜éŒ„åˆ°ç›£æ§ç³»çµ±
                self.monitor.record_operation_batch(
                    successful_ops=test_result['successful_operations'],
                    failed_ops=test_result['failed_operations'],
                    response_times=test_result['response_times'],
                    concurrent_workers=test_config['workers'],
                    error_details=test_result['error_details'],
                    phase=test_config['name']
                )
                
                # æª¢æŸ¥æ¸¬è©¦æ˜¯å¦é€šé
                if not self._is_test_passing(test_result):
                    self.overall_success = False
                    logger.warning(f"âŒ æ¸¬è©¦ {test_config['name']} æœªé€šéæ¨™æº–")
                else:
                    logger.info(f"âœ… æ¸¬è©¦ {test_config['name']} é€šéæ¨™æº–")
                    
            except Exception as e:
                logger.error(f"âŒ æ¸¬è©¦ {test_config['name']} åŸ·è¡Œå¤±æ•—ï¼š{e}")
                self.overall_success = False
                
                # è¨˜éŒ„å¤±æ•—çš„æ¸¬è©¦
                failed_result = {
                    'test_name': test_config['name'],
                    'status': 'failed',
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                self.test_results.append(failed_result)
        
        total_duration = time.time() - start_time
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        final_report = self._generate_final_report(total_duration)
        
        logger.info("ğŸ“‹ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå®Œæˆ")
        logger.info(f"ç¸½è€—æ™‚ï¼š{total_duration:.2f} ç§’")
        logger.info(f"æ•´é«”çµæœï¼š{'é€šé' if self.overall_success else 'å¤±æ•—'}")
        
        return final_report
    
    async def _run_single_test(self, test_config: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œå–®å€‹ä½µç™¼æ¸¬è©¦"""
        import tempfile
        
        # å‰µå»ºè‡¨æ™‚è³‡æ–™åº«
        temp_db = tempfile.NamedTemporaryFile(suffix='.db', delete=False)
        db_path = temp_db.name
        temp_db.close()
        
        # é…ç½®é€£ç·šæ± 
        pool_config = PoolConfiguration(**self.config['connection_pool'])
        pool_manager = ConnectionPoolManager(db_path=db_path, config=pool_config)
        
        try:
            await pool_manager.start()
            
            # å»ºç«‹æ¸¬è©¦è¡¨
            async with pool_manager.connection() as conn:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS auto_test_data (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        worker_id INTEGER NOT NULL,
                        data TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                await conn.commit()
            
            # åŸ·è¡Œæ¸¬è©¦
            start_time = time.time()
            successful_operations = 0
            failed_operations = 0
            response_times = []
            error_details = {}
            
            async def test_worker(worker_id: int):
                """æ¸¬è©¦å·¥ä½œè€…"""
                nonlocal successful_operations, failed_operations, response_times, error_details
                
                for op_id in range(test_config['operations_per_worker']):
                    op_start = time.time()
                    
                    try:
                        async with pool_manager.connection() as conn:
                            # æ’å…¥æ“ä½œ
                            await conn.execute(
                                "INSERT INTO auto_test_data (worker_id, data) VALUES (?, ?)",
                                (worker_id, f"data_{worker_id}_{op_id}")
                            )
                            
                            # æŸ¥è©¢æ“ä½œ
                            async with conn.execute(
                                "SELECT COUNT(*) as count FROM auto_test_data WHERE worker_id = ?",
                                (worker_id,)
                            ) as cursor:
                                result = await cursor.fetchone()
                            
                            await conn.commit()
                            
                            successful_operations += 1
                            op_time = (time.time() - op_start) * 1000
                            response_times.append(op_time)
                    
                    except Exception as e:
                        failed_operations += 1
                        error_type = type(e).__name__
                        if error_type not in error_details:
                            error_details[error_type] = 0
                        error_details[error_type] += 1
                        
                        logger.debug(f"Worker {worker_id} æ“ä½œ {op_id} å¤±æ•—ï¼š{e}")
                    
                    # çŸ­æš«å»¶é²
                    await asyncio.sleep(0.01)
            
            # å•Ÿå‹•æ‰€æœ‰å·¥ä½œè€…
            timeout = test_config.get('timeout_seconds', 60)
            tasks = [test_worker(i) for i in range(test_config['workers'])]
            
            try:
                await asyncio.wait_for(asyncio.gather(*tasks), timeout=timeout)
            except asyncio.TimeoutError:
                logger.warning(f"æ¸¬è©¦ {test_config['name']} è¶…æ™‚ ({timeout}s)")
                failed_operations += 1
                error_details['timeout'] = 1
            
            duration = time.time() - start_time
            total_operations = successful_operations + failed_operations
            
            # è¨ˆç®—çµ±è¨ˆ
            success_rate = (successful_operations / total_operations * 100) if total_operations > 0 else 0
            error_rate = (failed_operations / total_operations * 100) if total_operations > 0 else 0
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            
            # è¨ˆç®—P95éŸ¿æ‡‰æ™‚é–“
            sorted_times = sorted(response_times)
            p95_index = int(0.95 * len(sorted_times)) if sorted_times else 0
            p95_response_time = sorted_times[p95_index] if sorted_times else 0
            
            return {
                'test_name': test_config['name'],
                'status': 'completed',
                'timestamp': datetime.now().isoformat(),
                'duration_seconds': duration,
                'concurrent_workers': test_config['workers'],
                'operations_per_worker': test_config['operations_per_worker'],
                'total_operations': total_operations,
                'successful_operations': successful_operations,
                'failed_operations': failed_operations,
                'success_rate_percent': success_rate,
                'error_rate_percent': error_rate,
                'average_response_time_ms': avg_response_time,
                'p95_response_time_ms': p95_response_time,
                'throughput_ops_per_sec': total_operations / duration if duration > 0 else 0,
                'response_times': response_times,
                'error_details': error_details
            }
            
        finally:
            await pool_manager.stop()
            
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            try:
                os.unlink(db_path)
            except OSError:
                pass
    
    def _is_test_passing(self, test_result: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ¸¬è©¦æ˜¯å¦é€šéæ¨™æº–"""
        if test_result.get('status') != 'completed':
            return False
        
        thresholds = self.config['thresholds']
        
        return (
            test_result['error_rate_percent'] <= thresholds['max_error_rate'] and
            test_result['p95_response_time_ms'] <= thresholds['max_p95_response_time_ms'] and
            test_result['success_rate_percent'] >= thresholds['min_success_rate']
        )
    
    def _generate_final_report(self, total_duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š"""
        # è¨ˆç®—ç¶œåˆçµ±è¨ˆ
        completed_tests = [r for r in self.test_results if r.get('status') == 'completed']
        
        if completed_tests:
            total_ops = sum(r['total_operations'] for r in completed_tests)
            total_successful = sum(r['successful_operations'] for r in completed_tests)
            total_failed = sum(r['failed_operations'] for r in completed_tests)
            
            avg_success_rate = sum(r['success_rate_percent'] for r in completed_tests) / len(completed_tests)
            avg_error_rate = sum(r['error_rate_percent'] for r in completed_tests) / len(completed_tests)
            avg_response_time = sum(r['average_response_time_ms'] for r in completed_tests) / len(completed_tests)
            max_p95_response_time = max(r['p95_response_time_ms'] for r in completed_tests)
        else:
            total_ops = total_successful = total_failed = 0
            avg_success_rate = avg_error_rate = avg_response_time = max_p95_response_time = 0
        
        passing_tests = sum(1 for r in self.test_results if self._is_test_passing(r))
        
        # ç”Ÿæˆç›£æ§å ±å‘Š
        test_config = {
            'test_type': 'automated_concurrency_suite',
            'total_tests': len(self.test_results),
            'configuration': self.config
        }
        
        monitor_report = self.reporter.generate_comprehensive_report(
            self.monitor, test_config, total_duration
        )
        
        final_report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'test_suite_version': '2.4.2',
                'total_duration_seconds': total_duration,
                'overall_success': self.overall_success
            },
            'test_summary': {
                'total_tests': len(self.test_results),
                'passing_tests': passing_tests,
                'failing_tests': len(self.test_results) - passing_tests,
                'success_rate_percent': (passing_tests / len(self.test_results) * 100) if self.test_results else 0
            },
            'performance_summary': {
                'total_operations': total_ops,
                'successful_operations': total_successful,
                'failed_operations': total_failed,
                'average_success_rate_percent': avg_success_rate,
                'average_error_rate_percent': avg_error_rate,
                'average_response_time_ms': avg_response_time,
                'max_p95_response_time_ms': max_p95_response_time
            },
            'individual_test_results': self.test_results,
            'monitoring_report': monitor_report,
            'ci_cd_integration': self._generate_ci_results()
        }
        
        # ä¿å­˜å ±å‘Š
        self._save_final_report(final_report)
        
        return final_report
    
    def _generate_ci_results(self) -> Dict[str, Any]:
        """ç”ŸæˆCI/CDé›†æˆçµæœ"""
        return {
            'exit_code': 0 if self.overall_success else 1,
            'junit_xml_available': False,  # å¯ä»¥æ“´å±•æ”¯æŒJUnit XMLæ ¼å¼
            'coverage_report': False,      # å¯ä»¥æ“´å±•æ”¯æŒè¦†è“‹ç‡å ±å‘Š
            'artifacts': [
                str(self.results_dir / f"concurrency_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            ],
            'badges': {
                'tests_passing': f"{sum(1 for r in self.test_results if self._is_test_passing(r))}/{len(self.test_results)}",
                'overall_status': 'passing' if self.overall_success else 'failing'
            }
        }
    
    def _save_final_report(self, report: Dict[str, Any]) -> str:
        """ä¿å­˜æœ€çµ‚å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"automated_concurrency_suite_{timestamp}.json"
        filepath = self.results_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"æœ€çµ‚å ±å‘Šå·²ä¿å­˜ï¼š{filepath}")
        return str(filepath)


class CIIntegration:
    """æŒçºŒé›†æˆç³»çµ±æ•´åˆ"""
    
    @staticmethod
    def create_github_workflow() -> str:
        """å‰µå»ºGitHub Actionså·¥ä½œæµç¨‹"""
        workflow_content = """name: T2 Concurrency Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨2é»åŸ·è¡Œ

jobs:
  concurrency-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run concurrency tests
      run: |
        python tests/concurrency/automated_test_runner.py --mode=ci --timeout=1800
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: concurrency-test-results
        path: test_reports/concurrency/
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const reportPath = 'test_reports/concurrency/ci_summary.json';
          if (fs.existsSync(reportPath)) {
            const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
            const comment = `## ğŸ§ª T2 ä½µç™¼æ¸¬è©¦çµæœ
            
            - **ç¸½æ¸¬è©¦æ•¸**: ${report.total_tests}
            - **é€šéæ¸¬è©¦**: ${report.passing_tests}
            - **å¤±æ•—æ¸¬è©¦**: ${report.failing_tests}
            - **æ•´é«”ç‹€æ…‹**: ${report.overall_success ? 'âœ… é€šé' : 'âŒ å¤±æ•—'}
            - **éŒ¯èª¤ç‡**: ${report.average_error_rate_percent.toFixed(2)}%
            - **å¹³å‡éŸ¿æ‡‰æ™‚é–“**: ${report.average_response_time_ms.toFixed(2)}ms
            
            è©³ç´°å ±å‘Šè«‹æŸ¥çœ‹ Actions artifactsã€‚
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
"""
        
        workflow_file = Path(".github/workflows/concurrency-tests.yml")
        workflow_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(workflow_file, 'w', encoding='utf-8') as f:
            f.write(workflow_content)
        
        return str(workflow_file)
    
    @staticmethod
    def create_docker_compose() -> str:
        """å‰µå»ºDocker Composeé…ç½®ç”¨æ–¼æ¸¬è©¦ç’°å¢ƒ"""
        compose_content = """version: '3.8'

services:
  concurrency-test:
    build: .
    environment:
      - TEST_MODE=concurrency
      - RESULTS_DIR=/app/test_reports
    volumes:
      - ./test_reports:/app/test_reports
      - ./tests:/app/tests
    command: python tests/concurrency/automated_test_runner.py --mode=docker
    
  test-db:
    image: sqlite:latest
    volumes:
      - test_data:/var/lib/sqlite
    
volumes:
  test_data:
"""
        
        compose_file = Path("docker-compose.test.yml")
        with open(compose_file, 'w', encoding='utf-8') as f:
            f.write(compose_content)
        
        return str(compose_file)


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='T2 ä½µç™¼æ¸¬è©¦è‡ªå‹•åŒ–åŸ·è¡Œå™¨')
    parser.add_argument('--mode', choices=['dev', 'ci', 'docker'], default='dev',
                       help='åŸ·è¡Œæ¨¡å¼')
    parser.add_argument('--config', type=str, 
                       help='é…ç½®æ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--timeout', type=int, default=1800,
                       help='ç¸½è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰')
    parser.add_argument('--create-ci-files', action='store_true',
                       help='å‰µå»ºCI/CDé…ç½®æ–‡ä»¶')
    
    args = parser.parse_args()
    
    if args.create_ci_files:
        logger.info("å‰µå»ºCI/CDé…ç½®æ–‡ä»¶...")
        workflow_file = CIIntegration.create_github_workflow()
        compose_file = CIIntegration.create_docker_compose()
        logger.info(f"âœ… GitHub Actionså·¥ä½œæµç¨‹å·²å‰µå»ºï¼š{workflow_file}")
        logger.info(f"âœ… Docker Composeé…ç½®å·²å‰µå»ºï¼š{compose_file}")
        return
    
    logger.info(f"ä»¥ {args.mode} æ¨¡å¼åŸ·è¡Œä½µç™¼æ¸¬è©¦")
    
    try:
        test_suite = ConcurrencyTestSuite(args.config)
        
        # è¨­å®šè¶…æ™‚
        report = await asyncio.wait_for(
            test_suite.run_all_tests(), 
            timeout=args.timeout
        )
        
        # æ ¹æ“šæ¨¡å¼è¼¸å‡ºä¸åŒæ ¼å¼çš„çµæœ
        if args.mode == 'ci':
            # CIæ¨¡å¼ï¼šè¼¸å‡ºç°¡æ½”çµæœä¸¦è¨­å®šé€€å‡ºç¢¼
            ci_results = report['ci_cd_integration']
            summary = {
                'total_tests': report['test_summary']['total_tests'],
                'passing_tests': report['test_summary']['passing_tests'], 
                'failing_tests': report['test_summary']['failing_tests'],
                'overall_success': report['report_metadata']['overall_success'],
                'average_error_rate_percent': report['performance_summary']['average_error_rate_percent'],
                'average_response_time_ms': report['performance_summary']['average_response_time_ms']
            }
            
            # ç‚ºCIç³»çµ±ä¿å­˜ç°¡åŒ–æ‘˜è¦
            summary_file = Path('test_reports/concurrency/ci_summary.json')
            summary_file.parent.mkdir(parents=True, exist_ok=True)
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2)
            
            if report['report_metadata']['overall_success']:
                logger.info("ğŸ‰ æ‰€æœ‰ä½µç™¼æ¸¬è©¦é€šéï¼")
                sys.exit(0)
            else:
                logger.error("âŒ éƒ¨åˆ†ä½µç™¼æ¸¬è©¦å¤±æ•—")
                sys.exit(1)
        
        else:
            # é–‹ç™¼æ¨¡å¼ï¼šé¡¯ç¤ºè©³ç´°çµæœ
            logger.info("=" * 80)
            logger.info("ğŸ“Š æ¸¬è©¦å¥—ä»¶åŸ·è¡Œæ‘˜è¦")
            logger.info("=" * 80)
            
            test_summary = report['test_summary']
            perf_summary = report['performance_summary']
            
            logger.info(f"æ¸¬è©¦ç¸½æ•¸ï¼š{test_summary['total_tests']}")
            logger.info(f"é€šéæ¸¬è©¦ï¼š{test_summary['passing_tests']}")
            logger.info(f"å¤±æ•—æ¸¬è©¦ï¼š{test_summary['failing_tests']}")
            logger.info(f"æˆåŠŸç‡ï¼š{test_summary['success_rate_percent']:.1f}%")
            
            logger.info(f"\nğŸ“ˆ æ•ˆèƒ½æ‘˜è¦ï¼š")
            logger.info(f"ç¸½æ“ä½œæ•¸ï¼š{perf_summary['total_operations']}")
            logger.info(f"å¹³å‡éŒ¯èª¤ç‡ï¼š{perf_summary['average_error_rate_percent']:.2f}%")
            logger.info(f"å¹³å‡éŸ¿æ‡‰æ™‚é–“ï¼š{perf_summary['average_response_time_ms']:.2f}ms")
            logger.info(f"æœ€å¤§P95éŸ¿æ‡‰æ™‚é–“ï¼š{perf_summary['max_p95_response_time_ms']:.2f}ms")
            
            if report['report_metadata']['overall_success']:
                logger.info("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½é€šéäº†T2æ¨™æº–ï¼")
            else:
                logger.warning("\nâš ï¸ éƒ¨åˆ†æ¸¬è©¦éœ€è¦å„ªåŒ–")
        
    except asyncio.TimeoutError:
        logger.error(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œè¶…æ™‚ ({args.timeout}ç§’)")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())