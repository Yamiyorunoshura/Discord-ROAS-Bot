#!/usr/bin/env python3
"""
T2 ä½µç™¼æ¸¬è©¦è‡ªå‹•åŒ–åŸ·è¡Œè…³æœ¬
æä¾›å‘½ä»¤è¡Œä»‹é¢åŸ·è¡Œä½µç™¼æ¸¬è©¦å¥—ä»¶

åŠŸèƒ½ï¼š
- åŸ·è¡ŒåŸºç¤å’Œé€²éšä½µç™¼æ¸¬è©¦
- ç”Ÿæˆè©³ç´°å ±å‘Š
- CI/CD æ•´åˆæ”¯æ´
- æ¸¬è©¦çµæœé©—è­‰
"""

import sys
import asyncio
import argparse
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# è¨­å®šå°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from test_connection_pool import (
    ConnectionPoolTestSuite,
    PerformanceBenchmark,
    ConcurrencyTestResult,
    run_full_concurrency_test_suite
)
from test_advanced_scenarios import (
    AdvancedConcurrencyTestSuite,
    AdvancedPerformanceAnalyzer,
    run_advanced_concurrency_tests
)


class ConcurrencyTestRunner:
    """ä½µç™¼æ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, results_dir: Optional[str] = None):
        """åˆå§‹åŒ–æ¸¬è©¦åŸ·è¡Œå™¨"""
        self.results_dir = Path(results_dir) if results_dir else Path("test_reports/concurrency")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # è¨­å®šæ—¥èªŒ
        self.setup_logging()
        
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒ"""
        log_file = self.results_dir / "concurrency_test_runner.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def run_basic_tests(self) -> List[ConcurrencyTestResult]:
        """åŸ·è¡ŒåŸºç¤ä½µç™¼æ¸¬è©¦"""
        self.logger.info("é–‹å§‹åŸ·è¡ŒåŸºç¤ä½µç™¼æ¸¬è©¦...")
        
        suite = ConnectionPoolTestSuite()
        benchmark = PerformanceBenchmark(str(self.results_dir))
        
        try:
            await suite.setup()
            
            results = []
            
            # 10å·¥ä½œè€…æ¸¬è©¦
            result_10 = await suite.test_10_workers_concurrent()
            results.append(result_10)
            benchmark.save_result(result_10)
            
            # 20å·¥ä½œè€…æ¥µé™æ¸¬è©¦
            result_20 = await suite.test_20_workers_extreme_load()
            results.append(result_20)
            benchmark.save_result(result_20)
            
            # é€£ç·šå£“åŠ›æ¸¬è©¦
            result_stress = await suite.test_connection_stress()
            results.append(result_stress)
            benchmark.save_result(result_stress)
            
            # ç©©å®šæ€§æ¸¬è©¦ (çŸ­ç‰ˆæœ¬ç”¨æ–¼CI)
            result_stability = await suite.test_long_running_stability(duration_minutes=1)
            results.append(result_stability)
            benchmark.save_result(result_stability)
            
            self.logger.info(f"åŸºç¤æ¸¬è©¦å®Œæˆï¼Œå…±åŸ·è¡Œ {len(results)} å€‹æ¸¬è©¦")
            return results
            
        finally:
            await suite.cleanup()
    
    async def run_advanced_tests(self) -> List[ConcurrencyTestResult]:
        """åŸ·è¡Œé€²éšä½µç™¼æ¸¬è©¦"""
        self.logger.info("é–‹å§‹åŸ·è¡Œé€²éšä½µç™¼æ¸¬è©¦...")
        
        suite = AdvancedConcurrencyTestSuite()
        
        try:
            await suite.setup()
            
            results = []
            
            # æ··åˆè®€å¯«æ¸¬è©¦
            result_mixed = await suite.test_mixed_read_write_operations()
            results.append(result_mixed)
            
            # äº‹å‹™å£“åŠ›æ¸¬è©¦
            result_tx = await suite.test_transaction_stress()
            results.append(result_tx)
            
            # é€£ç·šæ± è€—ç›¡æ¸¬è©¦
            result_pool = await suite.test_connection_pool_exhaustion()
            results.append(result_pool)
            
            # çªç™¼è² è¼‰æ¸¬è©¦
            result_burst = await suite.test_load_burst_patterns()
            results.append(result_burst)
            
            self.logger.info(f"é€²éšæ¸¬è©¦å®Œæˆï¼Œå…±åŸ·è¡Œ {len(results)} å€‹æ¸¬è©¦")
            return results
            
        finally:
            await suite.cleanup()
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """åŸ·è¡Œæ‰€æœ‰ä½µç™¼æ¸¬è©¦"""
        start_time = time.time()
        
        self.logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´ä½µç™¼æ¸¬è©¦å¥—ä»¶...")
        
        # åŸ·è¡ŒåŸºç¤æ¸¬è©¦
        basic_results = await self.run_basic_tests()
        
        # åŸ·è¡Œé€²éšæ¸¬è©¦
        advanced_results = await self.run_advanced_tests()
        
        all_results = basic_results + advanced_results
        
        # ç¶œåˆåˆ†æ
        analyzer = AdvancedPerformanceAnalyzer()
        analysis = analyzer.analyze_advanced_results(all_results)
        
        # ç”Ÿæˆç¶œåˆå ±å‘Š
        benchmark = PerformanceBenchmark(str(self.results_dir))
        comprehensive_report = benchmark.generate_report(all_results)
        
        # åˆä½µåˆ†æçµæœ
        final_report = {
            "test_execution": {
                "start_time": start_time,
                "end_time": time.time(),
                "duration_seconds": time.time() - start_time,
                "total_tests": len(all_results),
                "basic_tests": len(basic_results),
                "advanced_tests": len(advanced_results)
            },
            "basic_report": comprehensive_report,
            "advanced_analysis": analysis,
            "test_results": [self._result_to_dict(r) for r in all_results]
        }
        
        # å„²å­˜æœ€çµ‚å ±å‘Š
        report_file = self.results_dir / f"comprehensive_concurrency_report_{int(time.time())}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å®Œæ•´æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå®Œæˆï¼Œå ±å‘Šå„²å­˜è‡³: {report_file}")
        
        return final_report
    
    def _result_to_dict(self, result: ConcurrencyTestResult) -> Dict[str, Any]:
        """è½‰æ›æ¸¬è©¦çµæœç‚ºå­—å…¸æ ¼å¼"""
        return {
            "test_name": result.test_name,
            "worker_count": result.worker_count,
            "operations_per_worker": result.operations_per_worker,
            "total_operations": result.total_operations,
            "successful_operations": result.successful_operations,
            "failed_operations": result.failed_operations,
            "success_rate": result.success_rate,
            "error_rate": result.error_rate,
            "avg_response_time_ms": result.avg_response_time_ms,
            "p95_response_time_ms": result.p95_response_time_ms,
            "p99_response_time_ms": result.p99_response_time_ms,
            "total_duration_s": result.total_duration_s,
            "operations_per_second": result.operations_per_second,
            "memory_usage_mb": result.memory_usage_mb,
            "peak_memory_mb": result.peak_memory_mb,
            "errors": result.errors,
            "timestamp": result.timestamp
        }
    
    def validate_test_results(self, results: List[ConcurrencyTestResult]) -> Dict[str, Any]:
        """é©—è­‰æ¸¬è©¦çµæœæ˜¯å¦ç¬¦åˆ T2 æ¨™æº–"""
        validation = {
            "total_tests": len(results),
            "passed_tests": 0,
            "failed_tests": 0,
            "validation_details": [],
            "overall_pass": True,
            "error_summary": []
        }
        
        for result in results:
            test_validation = {
                "test_name": result.test_name,
                "checks": {
                    "error_rate_ok": result.error_rate <= 0.01,
                    "response_time_ok": result.p95_response_time_ms <= 50,
                    "success_rate_ok": result.success_rate >= 0.99
                }
            }
            
            test_validation["overall_pass"] = all(test_validation["checks"].values())
            
            if test_validation["overall_pass"]:
                validation["passed_tests"] += 1
            else:
                validation["failed_tests"] += 1
                validation["overall_pass"] = False
                
                # è¨˜éŒ„å¤±æ•—åŸå› 
                failures = []
                if not test_validation["checks"]["error_rate_ok"]:
                    failures.append(f"éŒ¯èª¤ç‡ {result.error_rate:.2%} > 1%")
                if not test_validation["checks"]["response_time_ok"]:
                    failures.append(f"P95å›æ‡‰æ™‚é–“ {result.p95_response_time_ms:.2f}ms > 50ms")
                if not test_validation["checks"]["success_rate_ok"]:
                    failures.append(f"æˆåŠŸç‡ {result.success_rate:.2%} < 99%")
                
                validation["error_summary"].append({
                    "test": result.test_name,
                    "failures": failures
                })
            
            validation["validation_details"].append(test_validation)
        
        return validation
    
    def print_summary(self, report: Dict[str, Any]):
        """åˆ—å°æ¸¬è©¦æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“‹ T2 ä½µç™¼æ¸¬è©¦å¥—ä»¶åŸ·è¡Œæ‘˜è¦")
        print("="*80)
        
        execution = report["test_execution"]
        print(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {execution['duration_seconds']:.2f} ç§’")
        print(f"ğŸ“Š ç¸½æ¸¬è©¦æ•¸: {execution['total_tests']} ({execution['basic_tests']} åŸºç¤ + {execution['advanced_tests']} é€²éš)")
        
        # åŸºç¤å ±å‘Šæ‘˜è¦
        basic_report = report["basic_report"]["test_summary"]
        print(f"âœ… åŸºç¤æ¸¬è©¦é€šéç‡: {basic_report['tests_meeting_criteria']}/{basic_report['total_tests']}")
        print(f"ğŸ“ˆ æ•´é«”æˆåŠŸç‡: {basic_report['overall_success_rate']:.2%}")
        print(f"ğŸ“‰ æ•´é«”éŒ¯èª¤ç‡: {basic_report['overall_error_rate']:.2%}")
        
        # é€²éšåˆ†ææ‘˜è¦
        advanced_analysis = report["advanced_analysis"]["overall_performance"]
        print(f"ğŸ”¬ é€²éšæ¸¬è©¦é€šé: {advanced_analysis['scenarios_passing']}/{advanced_analysis['scenarios_total']}")
        print(f"ğŸ“Š å¹³å‡æˆåŠŸç‡: {advanced_analysis['average_success_rate']:.2%}")
        print(f"âš¡ å¹³å‡P95å›æ‡‰æ™‚é–“: {advanced_analysis['average_p95_response_time_ms']:.2f}ms")
        
        # å»ºè­°
        recommendations = report["advanced_analysis"]["recommendations"]
        if recommendations:
            print("\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for rec in recommendations[:3]:  # é¡¯ç¤ºå‰3å€‹å»ºè­°
                print(f"  â€¢ {rec}")
        
        print("="*80)


def create_ci_script():
    """å»ºç«‹ CI è…³æœ¬"""
    ci_script_content = """#!/bin/bash
# T2 ä½µç™¼æ¸¬è©¦ CI è…³æœ¬

set -e  # é‡åˆ°éŒ¯èª¤ç«‹å³é€€å‡º

echo "ğŸš€ é–‹å§‹åŸ·è¡Œ T2 ä½µç™¼æ¸¬è©¦..."

# ç¢ºä¿æ¸¬è©¦å ±å‘Šç›®éŒ„å­˜åœ¨
mkdir -p test_reports/concurrency

# åŸ·è¡Œä½µç™¼æ¸¬è©¦
python tests/concurrency/run_concurrency_tests.py --mode=ci --timeout=600

# æª¢æŸ¥æ¸¬è©¦çµæœ
if [ -f "test_reports/concurrency/ci_test_result.json" ]; then
    # è§£ææ¸¬è©¦çµæœ
    python -c "
import json
with open('test_reports/concurrency/ci_test_result.json', 'r') as f:
    result = json.load(f)
    
if result.get('overall_pass', False):
    print('âœ… æ‰€æœ‰ä½µç™¼æ¸¬è©¦é€šé')
    exit(0)
else:
    print('âŒ ä½µç™¼æ¸¬è©¦å¤±æ•—')
    failed_tests = result.get('failed_tests', 0)
    print(f'å¤±æ•—æ¸¬è©¦æ•¸: {failed_tests}')
    exit(1)
"
else
    echo "âŒ æ¸¬è©¦çµæœæª”æ¡ˆä¸å­˜åœ¨"
    exit(1
fi

echo "ğŸ‰ T2 ä½µç™¼æ¸¬è©¦å®Œæˆ"
"""
    
    ci_script_path = Path("tests/concurrency/ci_test.sh")
    with open(ci_script_path, 'w') as f:
        f.write(ci_script_content)
    
    # è¨­å®šåŸ·è¡Œæ¬Šé™
    import stat
    ci_script_path.chmod(ci_script_path.stat().st_mode | stat.S_IEXEC)
    
    return ci_script_path


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="T2 ä½µç™¼æ¸¬è©¦åŸ·è¡Œå™¨")
    parser.add_argument(
        "--mode",
        choices=["basic", "advanced", "all", "ci"],
        default="all",
        help="æ¸¬è©¦æ¨¡å¼"
    )
    parser.add_argument(
        "--results-dir",
        type=str,
        default="test_reports/concurrency",
        help="æ¸¬è©¦çµæœç›®éŒ„"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=1800,  # 30åˆ†é˜
        help="æ¸¬è©¦è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰"
    )
    parser.add_argument(
        "--create-ci-script",
        action="store_true",
        help="å»ºç«‹ CI è…³æœ¬"
    )
    
    args = parser.parse_args()
    
    # å»ºç«‹ CI è…³æœ¬
    if args.create_ci_script:
        script_path = create_ci_script()
        print(f"âœ… CI è…³æœ¬å·²å»ºç«‹: {script_path}")
        return
    
    runner = ConcurrencyTestRunner(args.results_dir)
    
    try:
        # è¨­å®šè¶…æ™‚
        if args.mode == "basic":
            results = await asyncio.wait_for(runner.run_basic_tests(), timeout=args.timeout)
            # ç”ŸæˆåŸºç¤å ±å‘Š
            benchmark = PerformanceBenchmark(args.results_dir)
            report = benchmark.generate_report(results)
            
        elif args.mode == "advanced":
            results = await asyncio.wait_for(runner.run_advanced_tests(), timeout=args.timeout)
            # ç”Ÿæˆé€²éšåˆ†æ
            analyzer = AdvancedPerformanceAnalyzer()
            analysis = analyzer.analyze_advanced_results(results)
            report = {"advanced_analysis": analysis, "test_results": results}
            
        else:  # all æˆ– ci æ¨¡å¼
            report = await asyncio.wait_for(runner.run_all_tests(), timeout=args.timeout)
            results = [ConcurrencyTestResult(**r) for r in report["test_results"]]
        
        # é©—è­‰çµæœ
        validation = runner.validate_test_results(results)
        
        if args.mode == "ci":
            # CI æ¨¡å¼ï¼šå„²å­˜ç°¡åŒ–çµæœ
            ci_result = {
                "overall_pass": validation["overall_pass"],
                "passed_tests": validation["passed_tests"],
                "failed_tests": validation["failed_tests"],
                "error_summary": validation["error_summary"],
                "timestamp": time.time()
            }
            
            ci_result_file = Path(args.results_dir) / "ci_test_result.json"
            with open(ci_result_file, 'w') as f:
                json.dump(ci_result, f, indent=2)
            
            # è¼¸å‡º CI çµæœ
            if validation["overall_pass"]:
                print("âœ… æ‰€æœ‰ä½µç™¼æ¸¬è©¦é€šé T2 æ¨™æº–")
                sys.exit(0)
            else:
                print(f"âŒ {validation['failed_tests']} å€‹æ¸¬è©¦æœªé€šé T2 æ¨™æº–")
                for error in validation["error_summary"]:
                    print(f"  {error['test']}: {', '.join(error['failures'])}")
                sys.exit(1)
        else:
            # äº’å‹•æ¨¡å¼ï¼šé¡¯ç¤ºè©³ç´°å ±å‘Š
            runner.print_summary(report)
            
            if validation["overall_pass"]:
                print("ğŸ‰ æ‰€æœ‰æ¸¬è©¦éƒ½ç¬¦åˆ T2 æ•ˆèƒ½æ¨™æº–ï¼")
            else:
                print(f"âš ï¸  {validation['failed_tests']} å€‹æ¸¬è©¦éœ€è¦å„ªåŒ–")
    
    except asyncio.TimeoutError:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œè¶…æ™‚ ({args.timeout} ç§’)")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())