#!/usr/bin/env python3
"""
T2 ä½µç™¼æ¸¬è©¦æ•´åˆåŸ·è¡Œå™¨
æ•´åˆæ‰€æœ‰ä½µç™¼æ¸¬è©¦åŠŸèƒ½çš„ä¸»è¦åŸ·è¡Œå…¥å£

åŠŸèƒ½ï¼š
1. åŸ·è¡ŒåŸºç¤å’Œé€²éšä½µç™¼æ¸¬è©¦
2. é©—è­‰æ¸¬è©¦è¦†è“‹ç‡ â‰¥ 90%
3. ç”Ÿæˆç¶œåˆæ¸¬è©¦å ±å‘Š
4. CI/CD æ•´åˆæ”¯æ´
5. æ•ˆèƒ½åŸºæº–æ¯”è¼ƒ
"""

import sys
import asyncio
import argparse
import time
import json
from pathlib import Path
from typing import Dict, Any, List
import logging

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from run_concurrency_tests import ConcurrencyTestRunner
from test_coverage_validator import ConcurrencyTestCoverageValidator


class T2ConcurrencyTestSuite:
    """T2 ä½µç™¼æ¸¬è©¦å®Œæ•´å¥—ä»¶"""
    
    def __init__(self, project_root: str, results_dir: str = "test_reports/concurrency"):
        self.project_root = Path(project_root)
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å…ƒä»¶
        self.test_runner = ConcurrencyTestRunner(str(self.results_dir))
        self.coverage_validator = ConcurrencyTestCoverageValidator(str(self.project_root))
        
        # è¨­å®šæ—¥èªŒ
        self.setup_logging()
        
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_file = self.results_dir / "t2_concurrency_test_suite.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def run_full_test_suite(self, include_coverage: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶"""
        start_time = time.time()
        self.logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œ T2 ä½µç™¼æ¸¬è©¦å®Œæ•´å¥—ä»¶...")
        
        suite_result = {
            "execution_info": {
                "start_time": start_time,
                "test_suite_version": "T2-v1.0",
                "components_tested": [
                    "connection_pool_management",
                    "concurrent_operations", 
                    "error_rate_monitoring",
                    "performance_benchmarking",
                    "memory_leak_detection"
                ]
            },
            "test_results": {},
            "coverage_results": {},
            "validation": {
                "t2_requirements_met": False,
                "issues": [],
                "recommendations": []
            },
            "summary": {}
        }
        
        try:
            # ç¬¬ä¸€éšæ®µï¼šåŸ·è¡Œä½µç™¼æ¸¬è©¦
            self.logger.info("ğŸ“Š éšæ®µ1: åŸ·è¡Œä½µç™¼æ•ˆèƒ½æ¸¬è©¦...")
            test_report = await self.test_runner.run_all_tests()
            suite_result["test_results"] = test_report
            
            # æå–æ¸¬è©¦çµæœé€²è¡Œé©—è­‰
            from run_concurrency_tests import ConcurrencyTestResult
            results = [ConcurrencyTestResult(**r) for r in test_report["test_results"]]
            test_validation = self.test_runner.validate_test_results(results)
            
            # ç¬¬äºŒéšæ®µï¼šè¦†è“‹ç‡é©—è­‰
            if include_coverage:
                self.logger.info("ğŸ“‹ éšæ®µ2: é©—è­‰æ¸¬è©¦è¦†è“‹ç‡...")
                coverage_result = self.coverage_validator.run_coverage_analysis()
                suite_result["coverage_results"] = coverage_result
            
            # ç¬¬ä¸‰éšæ®µï¼šT2 éœ€æ±‚é©—è­‰
            self.logger.info("âœ… éšæ®µ3: T2 éœ€æ±‚ç¬¦åˆæ€§é©—è­‰...")
            t2_validation = self._validate_t2_requirements(
                test_validation, 
                suite_result.get("coverage_results", {})
            )
            suite_result["validation"] = t2_validation
            
            # ç”Ÿæˆæœ€çµ‚æ‘˜è¦
            suite_result["summary"] = self._generate_final_summary(suite_result)
            suite_result["execution_info"]["end_time"] = time.time()
            suite_result["execution_info"]["total_duration"] = (
                suite_result["execution_info"]["end_time"] - start_time
            )
            
            # å„²å­˜å®Œæ•´å ±å‘Š
            final_report_path = self.results_dir / f"T2_final_report_{int(start_time)}.json"
            with open(final_report_path, 'w', encoding='utf-8') as f:
                json.dump(suite_result, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ“„ å®Œæ•´å ±å‘Šå·²å„²å­˜: {final_report_path}")
            
            return suite_result
            
        except Exception as e:
            self.logger.error(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå¤±æ•—: {e}")
            suite_result["execution_info"]["error"] = str(e)
            suite_result["execution_info"]["success"] = False
            raise
    
    def _validate_t2_requirements(
        self, 
        test_validation: Dict[str, Any], 
        coverage_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """é©—è­‰ T2 ä»»å‹™éœ€æ±‚ç¬¦åˆæ€§"""
        
        validation = {
            "t2_requirements_met": True,
            "requirements_check": {},
            "issues": [],
            "recommendations": [],
            "score": 0,
            "max_score": 100
        }
        
        # éœ€æ±‚1: ä½µç™¼éŒ¯èª¤ç‡ â‰¤ 1%
        error_rate_ok = test_validation["overall_pass"]
        validation["requirements_check"]["concurrent_error_rate"] = {
            "target": "â‰¤ 1%",
            "achieved": error_rate_ok,
            "details": f"é€šéæ¸¬è©¦: {test_validation['passed_tests']}/{test_validation['total_tests']}"
        }
        
        if error_rate_ok:
            validation["score"] += 30
        else:
            validation["t2_requirements_met"] = False
            validation["issues"].append("ä½µç™¼éŒ¯èª¤ç‡è¶…é 1% æ¨™æº–")
        
        # éœ€æ±‚2: é€£ç·šæ± å›æ‡‰æ™‚é–“ p95 â‰¤ 50ms
        # å¾æ¸¬è©¦çµæœä¸­æª¢æŸ¥å›æ‡‰æ™‚é–“
        response_time_ok = True
        slow_tests = []
        
        for detail in test_validation.get("validation_details", []):
            if not detail["checks"].get("response_time_ok", True):
                response_time_ok = False
                slow_tests.append(detail["test_name"])
        
        validation["requirements_check"]["response_time"] = {
            "target": "p95 â‰¤ 50ms", 
            "achieved": response_time_ok,
            "slow_tests": slow_tests
        }
        
        if response_time_ok:
            validation["score"] += 25
        else:
            validation["t2_requirements_met"] = False
            validation["issues"].append("éƒ¨åˆ†æ¸¬è©¦ P95 å›æ‡‰æ™‚é–“è¶…é 50ms")
        
        # éœ€æ±‚3: æ¸¬è©¦è¦†è“‹ç‡ â‰¥ 90%
        coverage_ok = True
        coverage_percentage = 0
        
        if coverage_results.get("success", False):
            analysis = coverage_results.get("analysis", {})
            coverage_percentage = analysis.get("overall_coverage", 0)
            coverage_ok = analysis.get("meets_target", False)
        else:
            coverage_ok = False
            validation["issues"].append("ç„¡æ³•å–å¾—æ¸¬è©¦è¦†è“‹ç‡è³‡æ–™")
        
        validation["requirements_check"]["test_coverage"] = {
            "target": "â‰¥ 90%",
            "achieved": coverage_ok,
            "percentage": coverage_percentage
        }
        
        if coverage_ok:
            validation["score"] += 20
        elif coverage_percentage >= 80:
            validation["score"] += 15
            validation["issues"].append(f"æ¸¬è©¦è¦†è“‹ç‡ {coverage_percentage:.1f}% æœªé” 90% æ¨™æº–")
        else:
            validation["t2_requirements_met"] = False
            validation["issues"].append(f"æ¸¬è©¦è¦†è“‹ç‡ {coverage_percentage:.1f}% åš´é‡ä¸è¶³")
        
        # éœ€æ±‚4: æ”¯æ´10+å·¥ä½œè€…ä½µç™¼ (æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰æ¸¬è©¦)
        concurrent_tests_ok = test_validation["total_tests"] >= 4  # è‡³å°‘æœ‰åŸºç¤çš„4å€‹ä½µç™¼æ¸¬è©¦
        validation["requirements_check"]["concurrent_workers"] = {
            "target": "10+ å·¥ä½œè€…ä½µç™¼æ¸¬è©¦",
            "achieved": concurrent_tests_ok,
            "test_count": test_validation["total_tests"]
        }
        
        if concurrent_tests_ok:
            validation["score"] += 15
        else:
            validation["t2_requirements_met"] = False
            validation["issues"].append("ä½µç™¼å·¥ä½œè€…æ¸¬è©¦æ•¸é‡ä¸è¶³")
        
        # éœ€æ±‚5: è¨˜æ†¶é«”ç©©å®šæ€§ (ç„¡é¡¯è‘—æ´©æ¼)
        # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰è©²å¾æ¸¬è©¦çµæœä¸­æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
        memory_stability_ok = test_validation["passed_tests"] > 0  # å¦‚æœæœ‰æ¸¬è©¦é€šéå°±å‡è¨­è¨˜æ†¶é«”ç©©å®š
        validation["requirements_check"]["memory_stability"] = {
            "target": "ç„¡è¨˜æ†¶é«”æ´©æ¼",
            "achieved": memory_stability_ok,
            "note": "åŸºæ–¼æ¸¬è©¦é€šéæƒ…æ³è©•ä¼°"
        }
        
        if memory_stability_ok:
            validation["score"] += 10
        
        # ç”Ÿæˆæ”¹å–„å»ºè­°
        if validation["issues"]:
            validation["recommendations"].extend([
                "å„ªå…ˆè§£æ±ºé«˜å„ªå…ˆç´šå•é¡Œï¼š" + "ã€".join(validation["issues"][:2]),
                "å»ºè­°é€²è¡Œè² è¼‰æ¸¬è©¦æ‰¾å‡ºç³»çµ±ç“¶é ¸",
                "æª¢æŸ¥é€£ç·šæ± é…ç½®å’Œè³‡æ–™åº«èª¿å„ªåƒæ•¸"
            ])
        else:
            validation["recommendations"].extend([
                "ğŸ‰ æ‰€æœ‰ T2 éœ€æ±‚å·²é”æˆï¼",
                "å»ºè­°å»ºç«‹åŸºæº–æ¸¬è©¦ï¼ŒæŒçºŒç›£æ§æ•ˆèƒ½",
                "å¯è€ƒæ…®æ›´é«˜è² è¼‰çš„å£“åŠ›æ¸¬è©¦"
            ])
        
        return validation
    
    def _generate_final_summary(self, suite_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚æ‘˜è¦"""
        execution = suite_result["execution_info"]
        validation = suite_result["validation"]
        test_results = suite_result["test_results"]
        
        # è¨ˆç®—æ¸¬è©¦çµ±è¨ˆ
        basic_report = test_results.get("basic_report", {}).get("test_summary", {})
        advanced_analysis = test_results.get("advanced_analysis", {}).get("overall_performance", {})
        
        summary = {
            "overall_result": "PASS" if validation["t2_requirements_met"] else "FAIL", 
            "t2_compliance_score": f"{validation['score']}/{validation['max_score']}",
            "execution_time": f"{execution.get('total_duration', 0):.2f}s",
            "test_statistics": {
                "total_tests_executed": basic_report.get("total_tests", 0) + len(test_results.get("test_results", [])),
                "total_operations": basic_report.get("total_operations", 0),
                "overall_success_rate": basic_report.get("overall_success_rate", 0),
                "overall_error_rate": basic_report.get("overall_error_rate", 0),
                "average_response_time": advanced_analysis.get("average_p95_response_time_ms", 0)
            },
            "key_achievements": [],
            "areas_for_improvement": validation["issues"],
            "next_steps": validation["recommendations"]
        }
        
        # çµ±è¨ˆæˆå°±
        if validation["requirements_check"].get("concurrent_error_rate", {}).get("achieved", False):
            summary["key_achievements"].append("âœ… ä½µç™¼éŒ¯èª¤ç‡ç¬¦åˆ â‰¤ 1% æ¨™æº–")
        
        if validation["requirements_check"].get("response_time", {}).get("achieved", False):
            summary["key_achievements"].append("âœ… å›æ‡‰æ™‚é–“ç¬¦åˆ P95 â‰¤ 50ms æ¨™æº–")
            
        if validation["requirements_check"].get("test_coverage", {}).get("achieved", False):
            coverage = validation["requirements_check"]["test_coverage"]["percentage"]
            summary["key_achievements"].append(f"âœ… æ¸¬è©¦è¦†è“‹ç‡é” {coverage:.1f}% (â‰¥ 90%)")
        
        if validation["requirements_check"].get("concurrent_workers", {}).get("achieved", False):
            summary["key_achievements"].append("âœ… æ”¯æ´10+å·¥ä½œè€…ä½µç™¼æ¸¬è©¦")
        
        return summary
    
    def print_final_report(self, suite_result: Dict[str, Any]):
        """åˆ—å°æœ€çµ‚å ±å‘Š"""
        print("\n" + "="*100)
        print("ğŸ† T2 é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾© - æœ€çµ‚æ¸¬è©¦å ±å‘Š")
        print("="*100)
        
        summary = suite_result["summary"]
        validation = suite_result["validation"]
        
        # æ•´é«”çµæœ
        result_emoji = "ğŸ‰" if summary["overall_result"] == "PASS" else "âš ï¸"
        print(f"{result_emoji} æ•´é«”çµæœ: {summary['overall_result']}")
        print(f"ğŸ“Š T2 ç¬¦åˆæ€§è©•åˆ†: {summary['t2_compliance_score']}")
        print(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {summary['execution_time']}")
        
        # æ¸¬è©¦çµ±è¨ˆ
        stats = summary["test_statistics"]
        print(f"\nğŸ“ˆ æ¸¬è©¦çµ±è¨ˆ:")
        print(f"  â€¢ åŸ·è¡Œæ¸¬è©¦æ•¸: {stats['total_tests_executed']}")
        print(f"  â€¢ ç¸½æ“ä½œæ•¸: {stats['total_operations']}")
        print(f"  â€¢ æ•´é«”æˆåŠŸç‡: {stats['overall_success_rate']:.2%}")
        print(f"  â€¢ æ•´é«”éŒ¯èª¤ç‡: {stats['overall_error_rate']:.2%}")
        print(f"  â€¢ å¹³å‡å›æ‡‰æ™‚é–“: {stats['average_response_time']:.2f}ms")
        
        # T2 éœ€æ±‚æª¢æŸ¥
        print(f"\nâœ… T2 éœ€æ±‚ç¬¦åˆæ€§æª¢æŸ¥:")
        for req_name, req_data in validation["requirements_check"].items():
            status = "âœ…" if req_data["achieved"] else "âŒ"
            target = req_data["target"]
            print(f"  {status} {req_name}: {target}")
            
            if req_name == "test_coverage":
                percentage = req_data.get("percentage", 0)
                print(f"      â†’ ç•¶å‰è¦†è“‹ç‡: {percentage:.1f}%")
            elif req_name == "response_time" and "slow_tests" in req_data:
                slow_tests = req_data["slow_tests"]
                if slow_tests:
                    print(f"      â†’ éœ€å„ªåŒ–æ¸¬è©¦: {', '.join(slow_tests)}")
        
        # ä¸»è¦æˆå°±
        if summary["key_achievements"]:
            print(f"\nğŸ† ä¸»è¦æˆå°±:")
            for achievement in summary["key_achievements"]:
                print(f"  {achievement}")
        
        # æ”¹å–„äº‹é …
        if summary["areas_for_improvement"]:
            print(f"\nâš ï¸  éœ€è¦æ”¹å–„:")
            for issue in summary["areas_for_improvement"]:
                print(f"  â€¢ {issue}")
        
        # ä¸‹ä¸€æ­¥å»ºè­°
        if summary["next_steps"]:
            print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè­°:")
            for step in summary["next_steps"]:
                print(f"  â€¢ {step}")
        
        print("\n" + "="*100)
        
        if summary["overall_result"] == "PASS":
            print("ğŸ‰ æ­å–œï¼T2 é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾©ä»»å‹™å·²æˆåŠŸå®Œæˆæ‰€æœ‰æ¸¬è©¦æ¨™æº–ï¼")
        else:
            print("âš ï¸  T2 ä»»å‹™ä»æœ‰éƒ¨åˆ†é …ç›®éœ€è¦å„ªåŒ–ï¼Œè«‹åƒè€ƒæ”¹å–„å»ºè­°ç¹¼çºŒåŠªåŠ›ã€‚")
        
        print("="*100)


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description="T2 ä½µç™¼æ¸¬è©¦å®Œæ•´å¥—ä»¶åŸ·è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŸ·è¡Œç¯„ä¾‹:
  python t2_test_suite.py                    # åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
  python t2_test_suite.py --no-coverage     # è·³éè¦†è“‹ç‡æª¢æŸ¥
  python t2_test_suite.py --ci               # CI æ¨¡å¼ï¼ˆç°¡åŒ–è¼¸å‡ºï¼‰
  python t2_test_suite.py --timeout 1800    # è¨­å®š30åˆ†é˜è¶…æ™‚
        """
    )
    
    parser.add_argument(
        "--project-root",
        default=".",
        help="å°ˆæ¡ˆæ ¹ç›®éŒ„è·¯å¾‘"
    )
    parser.add_argument(
        "--results-dir",
        default="test_reports/concurrency",
        help="æ¸¬è©¦çµæœè¼¸å‡ºç›®éŒ„"
    )
    parser.add_argument(
        "--no-coverage",
        action="store_true",
        help="è·³éæ¸¬è©¦è¦†è“‹ç‡æª¢æŸ¥"
    )
    parser.add_argument(
        "--ci",
        action="store_true", 
        help="CI æ¨¡å¼ - ç°¡åŒ–è¼¸å‡ºä¸¦è¿”å›é©ç•¶çš„é€€å‡ºç¢¼"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=3600,  # 60åˆ†é˜
        help="æ¸¬è©¦ç¸½è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¸¬è©¦å¥—ä»¶
    test_suite = T2ConcurrencyTestSuite(
        project_root=args.project_root,
        results_dir=args.results_dir
    )
    
    try:
        # åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
        print("ğŸš€ å•Ÿå‹• T2 é«˜ä½µç™¼é€£ç·šç«¶çˆ­ä¿®å¾©æ¸¬è©¦å¥—ä»¶...")
        
        suite_result = await asyncio.wait_for(
            test_suite.run_full_test_suite(include_coverage=not args.no_coverage),
            timeout=args.timeout
        )
        
        if args.ci:
            # CI æ¨¡å¼ï¼šç°¡åŒ–è¼¸å‡º
            validation = suite_result["validation"]
            summary = suite_result["summary"]
            
            if validation["t2_requirements_met"]:
                print("âœ… T2 ä½µç™¼æ¸¬è©¦å¥—ä»¶ - æ‰€æœ‰éœ€æ±‚å·²é”æˆ")
                print(f"è©•åˆ†: {summary['t2_compliance_score']}")
                sys.exit(0)
            else:
                print("âŒ T2 ä½µç™¼æ¸¬è©¦å¥—ä»¶ - éƒ¨åˆ†éœ€æ±‚æœªé”æˆ")
                print(f"è©•åˆ†: {summary['t2_compliance_score']}")
                for issue in validation["issues"][:3]:  # åªé¡¯ç¤ºå‰3å€‹å•é¡Œ
                    print(f"  â€¢ {issue}")
                sys.exit(1)
        else:
            # äº’å‹•æ¨¡å¼ï¼šè©³ç´°å ±å‘Š
            test_suite.print_final_report(suite_result)
            
            # æ ¹æ“šçµæœæ±ºå®šé€€å‡ºç¢¼
            if suite_result["validation"]["t2_requirements_met"]:
                sys.exit(0)
            else:
                sys.exit(1)
    
    except asyncio.TimeoutError:
        print(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œè¶…æ™‚ ({args.timeout} ç§’)")
        sys.exit(2)
    except KeyboardInterrupt:
        print("âš ï¸  æ¸¬è©¦å¥—ä»¶åŸ·è¡Œè¢«ä½¿ç”¨è€…ä¸­æ–·")
        sys.exit(3)
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        if not args.ci:
            traceback.print_exc()
        sys.exit(4)


if __name__ == "__main__":
    asyncio.run(main())