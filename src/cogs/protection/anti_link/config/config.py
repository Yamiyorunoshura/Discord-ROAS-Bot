"""
åæƒ¡æ„é€£çµä¿è­·æ¨¡çµ„é…ç½®æ–‡ä»¶
åŒ…å«æ‰€æœ‰é è¨­å€¼ã€å¸¸æ•¸å®šç¾©å’Œå·¥å…·å‡½æ•¸
"""

import re
import urllib.parse as up
from typing import Any

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é è¨­é…ç½®å€¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULTS = {
    "enabled": "true",
    "whitelist": "",
    "blacklist": "",
    "delete_message": "ğŸš« åµæ¸¬åˆ°æƒ¡æ„é€£çµ,å·²è‡ªå‹•åˆªé™¤",
    "notify_channel": "",
    "whitelist_admins": "true",
    "check_embeds": "true",
    "auto_update": "true",
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸æ•¸å®šç¾©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# URL æª¢æ¸¬æ­£å‰‡è¡¨é”å¼
URL_PATTERN = re.compile(r"https?://[A-Za-z0-9\.\-_%]+\.[A-Za-z]{2,}[^ <]*", re.I)

# é è¨­ç™½åå–®(å®‰å…¨çš„ç¶²åŸŸ)
DEFAULT_WHITELIST = {
    "discord.com",
    "discord.gg",
    "youtube.com",
    "youtu.be",
    "youtube-nocookie.com",
    "github.com",
    "gist.github.com",
    "google.com",
    "googleapis.com",
    "twitter.com",
    "t.co",
    "reddit.com",
    "redd.it",
    "imgur.com",
    "i.imgur.com",
    "twitch.tv",
    "clips.twitch.tv",
    "steamcommunity.com",
    "store.steampowered.com",
    "wikipedia.org",
    "wikimedia.org",
    "microsoft.com",
    "office.com",
    "apple.com",
    "icloud.com",
    "dropbox.com",
    "drive.google.com",
    "zoom.us",
    "teams.microsoft.com",
}

# å¨è„…æƒ…è³‡ä¾†æºé…ç½®
THREAT_FEEDS = {
    "URLHaus": {
        "url": "https://urlhaus.abuse.ch/downloads/hostfile/",
        "format": "text",
        "enabled": True,
        "description": "URLHaus æƒ¡æ„ URL è³‡æ–™åº«",
    },
    "OpenPhish": {
        "url": "https://openphish.com/feed.txt",
        "format": "text",
        "enabled": True,
        "description": "OpenPhish é‡£é­šç¶²ç«™è³‡æ–™åº«",
    },
    "URLHaus-CSV": {
        "url": "https://urlhaus.abuse.ch/downloads/csv_recent/",
        "format": "csv",
        "enabled": False,  # é è¨­åœç”¨,å› ç‚ºè³‡æ–™é‡è¼ƒå¤§
        "description": "URLHaus CSV æ ¼å¼è³‡æ–™",
    },
}

# éŒ¯èª¤ä»£ç¢¼æ˜ å°„
ERROR_CODES = {
    "CONFIG_ERROR": "ANTI_LINK_CONFIG_ERROR",
    "DATABASE_ERROR": "ANTI_LINK_DATABASE_ERROR",
    "NETWORK_ERROR": "ANTI_LINK_NETWORK_ERROR",
    "PARSE_ERROR": "ANTI_LINK_PARSE_ERROR",
    "PERMISSION_ERROR": "ANTI_LINK_PERMISSION_ERROR",
    "MESSAGE_HANDLER_ERROR": "ANTI_LINK_MESSAGE_HANDLER_ERROR",
    "MALICIOUS_LINK_HANDLER_ERROR": "ANTI_LINK_MALICIOUS_LINK_HANDLER_ERROR",
    "BLACKLIST_UPDATE_ERROR": "ANTI_LINK_BLACKLIST_UPDATE_ERROR",
    "MANUAL_UPDATE_ERROR": "ANTI_LINK_MANUAL_UPDATE_ERROR",
    "PANEL_ERROR": "ANTI_LINK_PANEL_ERROR",
}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å·¥å…·å‡½æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_domain(url: str) -> str:
    """
    å¾ URL ä¸­æå–ç¶²åŸŸåç¨±

    Args:
        url: è¦è§£æçš„ URL

    Returns:
        str: ç¶²åŸŸåç¨±,å¦‚æœè§£æå¤±æ•—è¿”å›ç©ºå­—ä¸²
    """
    try:
        # ç¢ºä¿ URL æœ‰å”è­°
        if not url.startswith(("http://", "https://")):
            url = "http://" + url

        parsed = up.urlparse(url)
        domain = parsed.netloc.lower()

        # ç§»é™¤ç«¯å£è™Ÿ
        if ":" in domain:
            domain = domain.split(":")[0]

        # ç§»é™¤ www. å‰ç¶´
        if domain.startswith("www."):
            domain = domain[4:]

        return domain
    except Exception:
        return ""


def normalize_domain(domain: str) -> str:
    """
    æ¨™æº–åŒ–ç¶²åŸŸåç¨±

    Args:
        domain: åŸå§‹ç¶²åŸŸåç¨±

    Returns:
        str: æ¨™æº–åŒ–å¾Œçš„ç¶²åŸŸåç¨±
    """
    try:
        domain = domain.lower().strip()

        # ç§»é™¤å”è­°
        if domain.startswith(("http://", "https://")):
            domain = up.urlparse(domain).netloc

        # ç§»é™¤ç«¯å£è™Ÿ
        if ":" in domain:
            domain = domain.split(":")[0]

        # ç§»é™¤ www. å‰ç¶´
        if domain.startswith("www."):
            domain = domain[4:]

        # ç§»é™¤è·¯å¾‘
        if "/" in domain:
            domain = domain.split("/")[0]

        return domain
    except Exception:
        return ""


def is_whitelisted(domain: str, whitelist: set[str]) -> bool:
    """
    æª¢æŸ¥ç¶²åŸŸæ˜¯å¦åœ¨ç™½åå–®ä¸­

    Args:
        domain: è¦æª¢æŸ¥çš„ç¶²åŸŸ
        whitelist: ç™½åå–®é›†åˆ

    Returns:
        bool: æ˜¯å¦åœ¨ç™½åå–®ä¸­
    """
    try:
        domain = normalize_domain(domain)
        if not domain:
            return False

        # ç›´æ¥åŒ¹é…
        if domain in whitelist:
            return True

        # æª¢æŸ¥å­ç¶²åŸŸ
        return any(domain.endswith("." + whitelisted) for whitelisted in whitelist)
    except Exception:
        return False


def parse_domain_list(domain_str: str) -> set[str]:
    """
    è§£æç¶²åŸŸåˆ—è¡¨å­—ä¸²

    Args:
        domain_str: ä»¥é€—è™Ÿåˆ†éš”çš„ç¶²åŸŸåˆ—è¡¨

    Returns:
        Set[str]: æ¨™æº–åŒ–å¾Œçš„ç¶²åŸŸé›†åˆ
    """
    try:
        if not domain_str:
            return set()

        domains = set()
        for domain in domain_str.split(","):
            normalized = normalize_domain(domain.strip())
            if normalized:
                domains.add(normalized)

        return domains
    except Exception:
        return set()


def validate_domain(domain: str) -> bool:
    """
    é©—è­‰ç¶²åŸŸæ ¼å¼æ˜¯å¦æ­£ç¢º

    Args:
        domain: è¦é©—è­‰çš„ç¶²åŸŸ

    Returns:
        bool: æ˜¯å¦ç‚ºæœ‰æ•ˆç¶²åŸŸ
    """
    try:
        domain = normalize_domain(domain)
        if not domain:
            return False

        # åŸºæœ¬æ ¼å¼æª¢æŸ¥
        if not re.match(
            r"^[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?(\.[a-zA-Z0-9]([a-zA-Z0-9\-]{0,61}[a-zA-Z0-9])?)*$",
            domain,
        ):
            return False

        # æª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ TLD
        if "." not in domain:
            return False

        # æª¢æŸ¥ TLD é•·åº¦
        tld = domain.split(".")[-1]
        return not len(tld) < 2
    except Exception:
        return False


def get_domain_info(domain: str) -> dict[str, Any]:
    """
    ç²å–ç¶²åŸŸè©³ç´°è³‡è¨Š

    Args:
        domain: ç¶²åŸŸåç¨±

    Returns:
        Dict[str, Any]: ç¶²åŸŸè³‡è¨Š
    """
    try:
        import tldextract

        extracted = tldextract.extract(domain)
        return {
            "domain": extracted.domain,
            "suffix": extracted.suffix,
            "subdomain": extracted.subdomain,
            "registered_domain": extracted.registered_domain,
            "fqdn": extracted.fqdn,
            "is_valid": bool(extracted.domain and extracted.suffix),
        }
    except Exception:
        return {
            "domain": "",
            "suffix": "",
            "subdomain": "",
            "registered_domain": "",
            "fqdn": domain,
            "is_valid": False,
        }


def format_domain_list(domains: set[str], max_length: int = 1000) -> str:
    """
    æ ¼å¼åŒ–ç¶²åŸŸåˆ—è¡¨ç‚ºå­—ä¸²

    Args:
        domains: ç¶²åŸŸé›†åˆ
        max_length: æœ€å¤§å­—ä¸²é•·åº¦

    Returns:
        str: æ ¼å¼åŒ–å¾Œçš„å­—ä¸²
    """
    try:
        if not domains:
            return "(ç„¡)"

        sorted_domains = sorted(domains)
        result = []
        current_length = 0

        for domain in sorted_domains:
            line = f"â€¢ {domain}"
            if current_length + len(line) + 1 > max_length:
                remaining = len(sorted_domains) - len(result)
                if remaining > 0:
                    result.append(f"... é‚„æœ‰ {remaining} å€‹ç¶²åŸŸ")
                break

            result.append(line)
            current_length += len(line) + 1

        return "\n".join(result)
    except Exception:
        return "(è§£æéŒ¯èª¤)"


def get_config_description(key: str) -> str:
    """
    ç²å–é…ç½®é …ç›®çš„æè¿°

    Args:
        key: é…ç½®éµå

    Returns:
        str: é…ç½®æè¿°
    """
    descriptions = {
        "enabled": "æ˜¯å¦å•Ÿç”¨åæƒ¡æ„é€£çµä¿è­·",
        "whitelist": "ç™½åå–®ç¶²åŸŸåˆ—è¡¨(ä»¥é€—è™Ÿåˆ†éš”)",
        "blacklist": "æ‰‹å‹•é»‘åå–®ç¶²åŸŸåˆ—è¡¨(ä»¥é€—è™Ÿåˆ†éš”)",
        "delete_message": "åˆªé™¤æƒ¡æ„é€£çµæ™‚é¡¯ç¤ºçš„è¨Šæ¯",
        "notify_channel": "ç®¡ç†å“¡é€šçŸ¥é »é“ ID",
        "whitelist_admins": "æ˜¯å¦å°‡ç®¡ç†å“¡åŠ å…¥ç™½åå–®",
        "check_embeds": "æ˜¯å¦æª¢æŸ¥åµŒå…¥å…§å®¹ä¸­çš„é€£çµ",
        "auto_update": "æ˜¯å¦è‡ªå‹•æ›´æ–°å¨è„…æƒ…è³‡",
    }

    return descriptions.get(key, "æœªçŸ¥é…ç½®é …ç›®")


def get_stats_description(key: str) -> str:
    """
    ç²å–çµ±è¨ˆé …ç›®çš„æè¿°

    Args:
        key: çµ±è¨ˆéµå

    Returns:
        str: çµ±è¨ˆæè¿°
    """
    descriptions = {
        "links_blocked": "å·²é˜»æ­¢çš„æƒ¡æ„é€£çµæ•¸é‡",
        "messages_deleted": "å·²åˆªé™¤çš„è¨Šæ¯æ•¸é‡",
        "whitelist_hits": "ç™½åå–®å‘½ä¸­æ¬¡æ•¸",
        "blacklist_hits": "é»‘åå–®å‘½ä¸­æ¬¡æ•¸",
        "remote_blacklist_hits": "é ç«¯é»‘åå–®å‘½ä¸­æ¬¡æ•¸",
        "false_positives": "èª¤å ±æ¬¡æ•¸",
        "manual_additions": "æ‰‹å‹•æ·»åŠ çš„é»‘åå–®é …ç›®",
        "manual_removals": "æ‰‹å‹•ç§»é™¤çš„é»‘åå–®é …ç›®",
    }

    return descriptions.get(key, "æœªçŸ¥çµ±è¨ˆé …ç›®")
