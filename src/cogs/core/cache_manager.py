"""
ğŸ—„ï¸ å¤šç´šç·©å­˜ç®¡ç†ç³»çµ±
Discord ADR Bot v1.6 - çµ±ä¸€ç·©å­˜æ¶æ§‹

æä¾›ä¼æ¥­ç´šçš„å¤šç´šç·©å­˜åŠŸèƒ½:
- L1å…§å­˜ç·©å­˜(é«˜é€Ÿè¨ªå•)
- L2æŒä¹…åŒ–ç·©å­˜(æ•¸æ“šæŒä¹…æ€§)
- æ™ºèƒ½ç·©å­˜ç­–ç•¥(LRUã€TTLã€LFU)
- ç·©å­˜åŒæ­¥æ©Ÿåˆ¶
- å®Œæ•´çš„ç›£æ§æŒ‡æ¨™
- æ™ºèƒ½ç·©å­˜é ç†±
- æ€§èƒ½å„ªåŒ–å’Œåˆ†æ

ä½œè€…:Discord ADR Bot æ¶æ§‹å¸«
ç‰ˆæœ¬:v1.6
"""

import asyncio
import functools
import hashlib
import json
import logging
import pickle
import sqlite3
import threading
import time
import weakref
from abc import ABC, abstractmethod
from collections import OrderedDict, defaultdict, deque
from collections.abc import Callable
from contextlib import asynccontextmanager, suppress
from dataclasses import dataclass, field
from enum import Enum
from typing import (
    Any,
    TypeVar,
)

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# é¡å‹è®Šé‡
K = TypeVar("K")  # Key type
V = TypeVar("V")  # Value type


class CacheStrategy(Enum):
    """ç·©å­˜ç­–ç•¥æšèˆ‰"""

    LRU = "lru"  # æœ€è¿‘æœ€å°‘ä½¿ç”¨
    LFU = "lfu"  # æœ€ä¸å¸¸ç”¨
    TTL = "ttl"  # åŸºæ–¼æ™‚é–“
    FIFO = "fifo"  # å…ˆé€²å…ˆå‡º
    HYBRID = "hybrid"  # æ··åˆç­–ç•¥
    ADAPTIVE = "adaptive"  # è‡ªé©æ‡‰ç­–ç•¥


class CacheLevel(Enum):
    """ç·©å­˜ç´šåˆ¥æšèˆ‰"""

    L1 = 1  # å…§å­˜ç·©å­˜
    L2 = 2  # æŒä¹…åŒ–ç·©å­˜
    BOTH = 3  # å…©ç´šç·©å­˜


class CacheOperation(Enum):
    """ç·©å­˜æ“ä½œæšèˆ‰"""

    GET = "get"
    SET = "set"
    DELETE = "delete"
    CLEAR = "clear"
    EXPIRE = "expire"
    PRELOAD = "preload"


class CachePattern(Enum):
    """ç·©å­˜æ¨¡å¼æšèˆ‰"""

    READ_THROUGH = "read_through"  # è®€ç©¿é€
    WRITE_THROUGH = "write_through"  # å¯«ç©¿é€
    WRITE_BEHIND = "write_behind"  # å¯«å›
    CACHE_ASIDE = "cache_aside"  # æ—è·¯ç·©å­˜


@dataclass
class CacheEntry:
    """ç·©å­˜æ¢ç›®"""

    key: str
    value: Any
    timestamp: float = field(default_factory=time.time)
    access_count: int = 0
    last_access: float = field(default_factory=time.time)
    ttl: float | None = None
    size: int = 0
    metadata: dict[str, Any] = field(default_factory=dict)
    hit_count: int = 0
    miss_count: int = 0
    load_time: float = 0.0

    def __post_init__(self):
        if self.size == 0:
            self.size = self._calculate_size()

    def _calculate_size(self) -> int:
        """è¨ˆç®—æ¢ç›®å¤§å°"""
        try:
            return len(pickle.dumps(self.value))
        except:
            return len(str(self.value).encode("utf-8"))

    def is_expired(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦éæœŸ"""
        if self.ttl is None:
            return False
        return time.time() - self.timestamp > self.ttl

    def touch(self):
        """æ›´æ–°è¨ªå•æ™‚é–“"""
        self.last_access = time.time()
        self.access_count += 1
        self.hit_count += 1

    def get_age(self) -> float:
        """ç²å–æ¢ç›®å¹´é½¡(ç§’)"""
        return time.time() - self.timestamp

    def get_idle_time(self) -> float:
        """ç²å–ç©ºé–’æ™‚é–“(ç§’)"""
        return time.time() - self.last_access

    def get_frequency_score(self) -> float:
        """ç²å–é »ç‡åˆ†æ•¸"""
        age = self.get_age()
        return self.access_count / (age + 1)  # é¿å…é™¤é›¶


@dataclass
class CacheStats:
    """ç·©å­˜çµ±è¨ˆ"""

    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    evictions: int = 0
    expired: int = 0
    total_size: int = 0
    entry_count: int = 0
    preloads: int = 0
    preload_hits: int = 0
    avg_load_time: float = 0.0
    peak_size: int = 0

    @property
    def hit_rate(self) -> float:
        """å‘½ä¸­ç‡"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    @property
    def miss_rate(self) -> float:
        """æœªå‘½ä¸­ç‡"""
        return 1.0 - self.hit_rate

    @property
    def preload_effectiveness(self) -> float:
        """é è¼‰å…¥æœ‰æ•ˆæ€§"""
        return self.preload_hits / self.preloads if self.preloads > 0 else 0.0

    def reset(self):
        """é‡ç½®çµ±è¨ˆ"""
        self.__dict__.update(CacheStats().__dict__)


class CacheAnalyzer:
    """ç·©å­˜åˆ†æå™¨"""

    def __init__(self, max_history: int = 1000):
        self.access_history: deque = deque(maxlen=max_history)
        self.pattern_stats: dict[str, int] = defaultdict(int)
        self.hot_keys: dict[str, int] = defaultdict(int)
        self.cold_keys: set[str] = set()
        self._lock = asyncio.Lock()

    async def record_access(self, key: str, operation: CacheOperation, hit: bool):
        """è¨˜éŒ„è¨ªå•"""
        async with self._lock:
            access_time = time.time()
            self.access_history.append(
                {
                    "key": key,
                    "operation": operation.value,
                    "hit": hit,
                    "timestamp": access_time,
                }
            )

            # æ›´æ–°ç†±é»éµçµ±è¨ˆ
            if hit:
                self.hot_keys[key] += 1
            else:
                self.cold_keys.add(key)

    async def analyze_patterns(self) -> dict[str, Any]:
        """åˆ†æè¨ªå•æ¨¡å¼"""
        async with self._lock:
            if not self.access_history:
                return {"error": "æ²’æœ‰è¨ªå•æ­·å²"}

            # åˆ†ææœ€è¿‘çš„è¨ªå•æ¨¡å¼
            recent_accesses = list(self.access_history)[-100:]  # æœ€è¿‘100æ¬¡è¨ªå•

            # è¨ˆç®—å„ç¨®æŒ‡æ¨™
            total_accesses = len(recent_accesses)
            hits = sum(1 for access in recent_accesses if access["hit"])
            hit_rate = hits / total_accesses if total_accesses > 0 else 0

            # ç†±é»éµåˆ†æ
            top_hot_keys = sorted(
                self.hot_keys.items(), key=lambda x: x[1], reverse=True
            )[:10]

            # è¨ªå•é »ç‡åˆ†æ
            key_frequency = defaultdict(int)
            for access in recent_accesses:
                key_frequency[access["key"]] += 1

            return {
                "total_accesses": total_accesses,
                "hit_rate": hit_rate,
                "top_hot_keys": top_hot_keys,
                "unique_keys_accessed": len(key_frequency),
                "cold_keys_count": len(self.cold_keys),
                "avg_accesses_per_key": sum(key_frequency.values()) / len(key_frequency)
                if key_frequency
                else 0,
            }

    async def get_preload_suggestions(self) -> list[str]:
        """ç²å–é è¼‰å…¥å»ºè­°"""
        async with self._lock:
            # åŸºæ–¼ç†±é»éµå’Œè¨ªå•æ¨¡å¼å»ºè­°é è¼‰å…¥
            suggestions = []

            # é«˜é »è¨ªå•ä½†ç¶“å¸¸éŒ¯å¤±çš„éµ
            for key, count in self.hot_keys.items():
                if count > 5 and key in self.cold_keys:  # é–¾å€¼å¯èª¿æ•´
                    suggestions.append(key)

            return suggestions[:20]  # é™åˆ¶å»ºè­°æ•¸é‡


class SmartCacheStrategy:
    """æ™ºèƒ½ç·©å­˜ç­–ç•¥"""

    def __init__(self, analyzer: CacheAnalyzer):
        self.analyzer = analyzer
        self.strategy_weights = {
            CacheStrategy.LRU: 0.3,
            CacheStrategy.LFU: 0.3,
            CacheStrategy.TTL: 0.2,
            CacheStrategy.ADAPTIVE: 0.2,
        }

    async def should_evict(
        self, entries: dict[str, CacheEntry], max_size: int
    ) -> list[str]:
        """æ™ºèƒ½é¸æ“‡éœ€è¦æ·˜æ±°çš„æ¢ç›®"""
        if len(entries) <= max_size:
            return []

        # ç²å–åˆ†æçµæœ
        await self.analyzer.analyze_patterns()

        # è¨ˆç®—æ¯å€‹æ¢ç›®çš„æ·˜æ±°åˆ†æ•¸
        eviction_scores = {}

        for key, entry in entries.items():
            score = 0.0

            # LRU åˆ†æ•¸(å¹´é½¡)
            age_score = entry.get_age() / 3600  # æ¨™æº–åŒ–åˆ°å°æ™‚
            score += age_score * self.strategy_weights[CacheStrategy.LRU]

            # LFU åˆ†æ•¸(é »ç‡çš„å€’æ•¸)
            freq_score = 1.0 / (entry.get_frequency_score() + 0.01)  # é¿å…é™¤é›¶
            score += freq_score * self.strategy_weights[CacheStrategy.LFU]

            # TTL åˆ†æ•¸
            if entry.ttl:
                ttl_score = (time.time() - entry.timestamp) / entry.ttl
                score += ttl_score * self.strategy_weights[CacheStrategy.TTL]

            # è‡ªé©æ‡‰åˆ†æ•¸(åŸºæ–¼è¨ªå•æ¨¡å¼)
            if key in self.analyzer.cold_keys:
                score += 1.0 * self.strategy_weights[CacheStrategy.ADAPTIVE]

            eviction_scores[key] = score

        # é¸æ“‡åˆ†æ•¸æœ€é«˜çš„æ¢ç›®é€²è¡Œæ·˜æ±°
        sorted_entries = sorted(
            eviction_scores.items(), key=lambda x: x[1], reverse=True
        )
        evict_count = len(entries) - max_size + 1  # è‡³å°‘æ·˜æ±°ä¸€å€‹

        return [key for key, _ in sorted_entries[:evict_count]]


class CachePreloader:
    """ç·©å­˜é è¼‰å…¥å™¨"""

    def __init__(self, cache_ref: weakref.ref, analyzer: CacheAnalyzer):
        self.cache_ref = cache_ref
        self.analyzer = analyzer
        self.preload_tasks: set[asyncio.Task] = set()
        self.preload_functions: dict[str, Callable] = {}
        self._lock = asyncio.Lock()

    def register_preload_function(self, pattern: str, func: Callable):
        """è¨»å†Šé è¼‰å…¥å‡½æ•¸"""
        self.preload_functions[pattern] = func

    async def start_preloading(self):
        """é–‹å§‹é è¼‰å…¥ä»»å‹™"""
        async with self._lock:
            # ç²å–é è¼‰å…¥å»ºè­°
            suggestions = await self.analyzer.get_preload_suggestions()

            for key in suggestions:
                # æ‰¾åˆ°åŒ¹é…çš„é è¼‰å…¥å‡½æ•¸
                for pattern, func in self.preload_functions.items():
                    if pattern in key or key.startswith(pattern):
                        task = asyncio.create_task(self._preload_key(key, func))
                        self.preload_tasks.add(task)
                        task.add_done_callback(self.preload_tasks.discard)
                        break

    async def _preload_key(self, key: str, loader_func: Callable):
        """é è¼‰å…¥æŒ‡å®šéµ"""
        try:
            cache = self.cache_ref()
            if cache is None:
                return

            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if await cache.exists(key):
                return

            # è¼‰å…¥æ•¸æ“š
            start_time = time.time()
            data = await loader_func(key)
            load_time = time.time() - start_time

            # è¨­ç½®åˆ°ç·©å­˜
            if data is not None:
                await cache.set(key, data, level=CacheLevel.BOTH)

                # è¨˜éŒ„é è¼‰å…¥çµ±è¨ˆ
                if hasattr(cache, "l1_backend") and hasattr(cache.l1_backend, "stats"):
                    cache.l1_backend.stats.preloads += 1

                logger.debug(f"é è¼‰å…¥å®Œæˆ: {key} (è€—æ™‚: {load_time:.3f}s)")

        except Exception as e:
            logger.error(f"é è¼‰å…¥å¤±æ•— {key}: {e}")


class CacheBackend(ABC):
    """ç·©å­˜å¾Œç«¯æŠ½è±¡åŸºé¡"""

    @abstractmethod
    async def get(self, key: str) -> CacheEntry | None:
        """ç²å–ç·©å­˜æ¢ç›®"""
        pass

    @abstractmethod
    async def set(self, key: str, entry: CacheEntry) -> bool:
        """è¨­ç½®ç·©å­˜æ¢ç›®"""
        pass

    @abstractmethod
    async def delete(self, key: str) -> bool:
        """åˆªé™¤ç·©å­˜æ¢ç›®"""
        pass

    @abstractmethod
    async def clear(self) -> bool:
        """æ¸…ç©ºç·©å­˜"""
        pass

    @abstractmethod
    async def keys(self) -> list[str]:
        """ç²å–æ‰€æœ‰éµ"""
        pass

    @abstractmethod
    async def size(self) -> int:
        """ç²å–ç·©å­˜å¤§å°"""
        pass


class MemoryCacheBackend(CacheBackend):
    """å…§å­˜ç·©å­˜å¾Œç«¯(L1)"""

    def __init__(
        self, max_size: int = 1000, strategy: CacheStrategy = CacheStrategy.LRU
    ):
        self.max_size = max_size
        self.strategy = strategy
        self._cache: dict[str, CacheEntry] = {}
        self._access_order: OrderedDict[str, float] = OrderedDict()
        self._lock = asyncio.Lock()
        self.stats = CacheStats()
        self.analyzer = CacheAnalyzer()
        self.smart_strategy = SmartCacheStrategy(self.analyzer)

    async def get(self, key: str) -> CacheEntry | None:
        """ç²å–ç·©å­˜æ¢ç›®"""
        async with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                self.stats.misses += 1
                await self.analyzer.record_access(key, CacheOperation.GET, False)
                return None

            # æª¢æŸ¥éæœŸ
            if entry.is_expired():
                await self._remove_entry(key)
                self.stats.misses += 1
                self.stats.expired += 1
                await self.analyzer.record_access(key, CacheOperation.GET, False)
                return None

            # æ›´æ–°è¨ªå•ä¿¡æ¯
            entry.touch()
            self._update_access_order(key)
            self.stats.hits += 1
            await self.analyzer.record_access(key, CacheOperation.GET, True)

            return entry

    async def set(self, key: str, entry: CacheEntry) -> bool:
        """è¨­ç½®ç·©å­˜æ¢ç›®"""
        async with self._lock:
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°
            if key not in self._cache and len(self._cache) >= self.max_size:
                await self._evict_entries()

            # è¨­ç½®æ¢ç›®
            old_entry = self._cache.get(key)
            self._cache[key] = entry
            self._update_access_order(key)

            # æ›´æ–°çµ±è¨ˆ
            if old_entry is None:
                self.stats.sets += 1
                self.stats.entry_count += 1

            self.stats.total_size = sum(e.size for e in self._cache.values())
            self.stats.peak_size = max(self.stats.peak_size, self.stats.total_size)

            await self.analyzer.record_access(key, CacheOperation.SET, True)

            return True

    async def delete(self, key: str) -> bool:
        """åˆªé™¤ç·©å­˜æ¢ç›®"""
        async with self._lock:
            if key in self._cache:
                await self._remove_entry(key)
                self.stats.deletes += 1
                await self.analyzer.record_access(key, CacheOperation.DELETE, True)
                return True
            return False

    async def clear(self) -> bool:
        """æ¸…ç©ºç·©å­˜"""
        async with self._lock:
            self._cache.clear()
            self._access_order.clear()
            self.stats.entry_count = 0
            self.stats.total_size = 0
            return True

    async def keys(self) -> list[str]:
        """ç²å–æ‰€æœ‰éµ"""
        async with self._lock:
            return list(self._cache.keys())

    async def size(self) -> int:
        """ç²å–ç·©å­˜å¤§å°"""
        async with self._lock:
            return len(self._cache)

    def _update_access_order(self, key: str):
        """æ›´æ–°è¨ªå•é †åº"""
        self._access_order[key] = time.time()
        self._access_order.move_to_end(key)

    async def _remove_entry(self, key: str):
        """ç§»é™¤æ¢ç›®"""
        if key in self._cache:
            entry = self._cache.pop(key)
            self._access_order.pop(key, None)
            self.stats.entry_count -= 1
            self.stats.total_size -= entry.size

    async def _evict_entries(self):
        """æ·˜æ±°æ¢ç›®"""
        if self.strategy == CacheStrategy.ADAPTIVE:
            # ä½¿ç”¨æ™ºèƒ½ç­–ç•¥
            keys_to_evict = await self.smart_strategy.should_evict(
                self._cache, self.max_size - 1
            )
            for key in keys_to_evict:
                await self._remove_entry(key)
                self.stats.evictions += 1
        # ä½¿ç”¨å‚³çµ±ç­–ç•¥
        elif self.strategy == CacheStrategy.LRU:
            # ç§»é™¤æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„
            oldest_key = next(iter(self._access_order))
            await self._remove_entry(oldest_key)
            self.stats.evictions += 1
        elif self.strategy == CacheStrategy.LFU:
            # ç§»é™¤æœ€ä¸å¸¸ç”¨çš„
            min_freq_key = min(
                self._cache.keys(), key=lambda k: self._cache[k].access_count
            )
            await self._remove_entry(min_freq_key)
            self.stats.evictions += 1

    async def get_detailed_stats(self) -> dict[str, Any]:
        """ç²å–è©³ç´°çµ±è¨ˆ"""
        async with self._lock:
            base_stats = {
                "hits": self.stats.hits,
                "misses": self.stats.misses,
                "hit_rate": self.stats.hit_rate,
                "sets": self.stats.sets,
                "deletes": self.stats.deletes,
                "evictions": self.stats.evictions,
                "expired": self.stats.expired,
                "entry_count": self.stats.entry_count,
                "total_size": self.stats.total_size,
                "peak_size": self.stats.peak_size,
                "avg_entry_size": self.stats.total_size / self.stats.entry_count
                if self.stats.entry_count > 0
                else 0,
            }

            # æ·»åŠ åˆ†æçµæœ
            patterns = await self.analyzer.analyze_patterns()
            base_stats.update(patterns)

            return base_stats


class PersistentCacheBackend(CacheBackend):
    """æŒä¹…åŒ–ç·©å­˜å¾Œç«¯(L2)"""

    def __init__(self, db_path: str = "cache.db", max_size: int = 10000):
        self.db_path = db_path
        self.max_size = max_size
        self.stats = CacheStats()
        self._connection_pool: list[sqlite3.Connection] = []
        self._pool_lock = threading.Lock()
        self._lock = asyncio.Lock()
        self._initialize_db()

    def _initialize_db(self):
        """åˆå§‹åŒ–è³‡æ–™åº«"""
        try:
            conn = sqlite3.connect(self.db_path)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    value BLOB,
                    timestamp REAL,
                    access_count INTEGER,
                    last_access REAL,
                    ttl REAL,
                    size INTEGER,
                    metadata TEXT,
                    hit_count INTEGER DEFAULT 0,
                    load_time REAL DEFAULT 0.0
                )
            """)

            # æ·»åŠ ç´¢å¼•ä»¥æå‡æ€§èƒ½
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_last_access ON cache_entries(last_access)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_access_count ON cache_entries(access_count)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_timestamp ON cache_entries(timestamp)"
            )

            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")

    def _get_connection(self) -> sqlite3.Connection:
        """ç²å–è³‡æ–™åº«é€£æ¥"""
        with self._pool_lock:
            if self._connection_pool:
                return self._connection_pool.pop()
            else:
                return sqlite3.connect(self.db_path)

    def _return_connection(self, conn: sqlite3.Connection):
        """æ­¸é‚„è³‡æ–™åº«é€£æ¥"""
        with self._pool_lock:
            if len(self._connection_pool) < 10:  # é™åˆ¶é€£æ¥æ± å¤§å°
                self._connection_pool.append(conn)
            else:
                conn.close()

    async def get(self, key: str) -> CacheEntry | None:
        """ç²å–ç·©å­˜æ¢ç›®"""
        async with self._lock:
            conn = self._get_connection()
            try:
                cursor = conn.execute(
                    "SELECT * FROM cache_entries WHERE key = ?", (key,)
                )
                row = cursor.fetchone()

                if row is None:
                    self.stats.misses += 1
                    return None

                entry = self._row_to_entry(row)

                # æª¢æŸ¥éæœŸ
                if entry.is_expired():
                    await self._delete_entry(key, conn)
                    self.stats.misses += 1
                    self.stats.expired += 1
                    return None

                # æ›´æ–°è¨ªå•ä¿¡æ¯
                entry.touch()
                await self._update_entry(entry, conn)
                self.stats.hits += 1

                return entry

            finally:
                self._return_connection(conn)

    async def set(self, key: str, entry: CacheEntry) -> bool:
        """è¨­ç½®ç·©å­˜æ¢ç›®"""
        async with self._lock:
            conn = self._get_connection()
            try:
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°
                entry_count = await self._get_entry_count(conn)
                if entry_count >= self.max_size:
                    await self._evict_entries(conn)

                # åºåˆ—åŒ–æ•¸æ“š
                value_blob = pickle.dumps(entry.value)
                metadata_json = json.dumps(entry.metadata)

                # æ’å…¥æˆ–æ›´æ–°
                conn.execute(
                    """
                    INSERT OR REPLACE INTO cache_entries
                    (key, value, timestamp, access_count, last_access, ttl, size, metadata, hit_count, load_time)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        entry.key,
                        value_blob,
                        entry.timestamp,
                        entry.access_count,
                        entry.last_access,
                        entry.ttl,
                        entry.size,
                        metadata_json,
                        entry.hit_count,
                        entry.load_time,
                    ),
                )

                conn.commit()
                self.stats.sets += 1

                return True

            except Exception as e:
                logger.error(f"è¨­ç½®ç·©å­˜æ¢ç›®å¤±æ•—: {e}")
                return False
            finally:
                self._return_connection(conn)

    async def delete(self, key: str) -> bool:
        """åˆªé™¤ç·©å­˜æ¢ç›®"""
        async with self._lock:
            conn = self._get_connection()
            try:
                result = await self._delete_entry(key, conn)
                if result:
                    self.stats.deletes += 1
                return result
            finally:
                self._return_connection(conn)

    async def clear(self) -> bool:
        """æ¸…ç©ºç·©å­˜"""
        async with self._lock:
            conn = self._get_connection()
            try:
                conn.execute("DELETE FROM cache_entries")
                conn.commit()
                return True
            except Exception as e:
                logger.error(f"æ¸…ç©ºç·©å­˜å¤±æ•—: {e}")
                return False
            finally:
                self._return_connection(conn)

    async def keys(self) -> list[str]:
        """ç²å–æ‰€æœ‰éµ"""
        async with self._lock:
            conn = self._get_connection()
            try:
                cursor = conn.execute("SELECT key FROM cache_entries")
                return [row[0] for row in cursor.fetchall()]
            finally:
                self._return_connection(conn)

    async def size(self) -> int:
        """ç²å–ç·©å­˜å¤§å°"""
        async with self._lock:
            conn = self._get_connection()
            try:
                return await self._get_entry_count(conn)
            finally:
                self._return_connection(conn)

    def _row_to_entry(self, row: tuple) -> CacheEntry:
        """å°‡è³‡æ–™åº«è¡Œè½‰æ›ç‚ºç·©å­˜æ¢ç›®"""
        (
            key,
            value_blob,
            timestamp,
            access_count,
            last_access,
            ttl,
            size,
            metadata_json,
            hit_count,
            load_time,
        ) = row

        try:
            value = pickle.loads(value_blob)
        except (pickle.PickleError, EOFError, ValueError) as e:
            logger.error(f"Failed to deserialize cache value: {e}")
            raise
        metadata = json.loads(metadata_json) if metadata_json else {}

        entry = CacheEntry(
            key=key,
            value=value,
            timestamp=timestamp,
            access_count=access_count,
            last_access=last_access,
            ttl=ttl,
            size=size,
            metadata=metadata,
        )
        entry.hit_count = hit_count or 0
        entry.load_time = load_time or 0.0

        return entry

    async def _delete_entry(self, key: str, conn: sqlite3.Connection) -> bool:
        """åˆªé™¤æ¢ç›®"""
        cursor = conn.execute("DELETE FROM cache_entries WHERE key = ?", (key,))
        conn.commit()
        return cursor.rowcount > 0

    async def _update_entry(self, entry: CacheEntry, conn: sqlite3.Connection):
        """æ›´æ–°æ¢ç›®"""
        conn.execute(
            """
            UPDATE cache_entries
            SET access_count = ?, last_access = ?, hit_count = ?
            WHERE key = ?
        """,
            (entry.access_count, entry.last_access, entry.hit_count, entry.key),
        )
        conn.commit()

    async def _get_entry_count(self, conn: sqlite3.Connection) -> int:
        """ç²å–æ¢ç›®æ•¸é‡"""
        cursor = conn.execute("SELECT COUNT(*) FROM cache_entries")
        return cursor.fetchone()[0]

    async def _evict_entries(self, conn: sqlite3.Connection):
        """æ·˜æ±°æ¢ç›®"""
        # ç§»é™¤æœ€èˆŠçš„æ¢ç›®
        conn.execute(
            """
            DELETE FROM cache_entries
            WHERE key IN (
                SELECT key FROM cache_entries
                ORDER BY last_access ASC
                LIMIT ?
            )
        """,
            (max(1, self.max_size // 10),),
        )  # ä¸€æ¬¡æ·˜æ±°10%
        conn.commit()
        self.stats.evictions += conn.total_changes


class MultiLevelCache:
    """å¤šç´šç·©å­˜ç®¡ç†å™¨"""

    def __init__(
        self,
        l1_max_size: int = 1000,
        l2_max_size: int = 10000,
        l1_strategy: CacheStrategy = CacheStrategy.ADAPTIVE,
        default_ttl: float | None = None,
        db_path: str = "cache.db",
        enable_preloading: bool = True,
    ):
        self.l1_backend = MemoryCacheBackend(l1_max_size, l1_strategy)
        self.l2_backend = PersistentCacheBackend(db_path, l2_max_size)
        self.default_ttl = default_ttl
        self.enable_preloading = enable_preloading

        # å‰µå»ºé è¼‰å…¥å™¨
        if enable_preloading:
            self.preloader = CachePreloader(weakref.ref(self), self.l1_backend.analyzer)
        else:
            self.preloader = None

        # å•Ÿå‹•æ¸…ç†ä»»å‹™
        self._cleanup_task = None
        self._start_cleanup_task()

    def _start_cleanup_task(self):
        """å•Ÿå‹•æ¸…ç†ä»»å‹™"""
        if self._cleanup_task is None or self._cleanup_task.done():
            self._cleanup_task = asyncio.create_task(self._cleanup_loop())

    async def _cleanup_loop(self):
        """æ¸…ç†å¾ªç’°"""
        while True:
            try:
                await asyncio.sleep(300)  # æ¯5åˆ†é˜æ¸…ç†ä¸€æ¬¡
                await self._cleanup_expired()

                # è§¸ç™¼é è¼‰å…¥
                if self.preloader:
                    await self.preloader.start_preloading()

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"æ¸…ç†ä»»å‹™éŒ¯èª¤: {e}")
                await asyncio.sleep(60)

    async def _cleanup_expired(self):
        """æ¸…ç†éæœŸæ¢ç›®"""
        # æ¸…ç† L1 éæœŸæ¢ç›®
        l1_keys = await self.l1_backend.keys()
        for key in l1_keys:
            entry = await self.l1_backend.get(key)
            if entry and entry.is_expired():
                await self.l1_backend.delete(key)

        # æ¸…ç† L2 éæœŸæ¢ç›®(é€šéè³‡æ–™åº«æŸ¥è©¢)
        # é€™è£¡å¯ä»¥æ·»åŠ æ›´é«˜æ•ˆçš„æ‰¹é‡æ¸…ç†é‚è¼¯

    async def get(self, key: str) -> Any | None:
        """ç²å–ç·©å­˜å€¼"""
        # å…ˆå¾ L1 ç²å–
        entry = await self.l1_backend.get(key)
        if entry:
            return entry.value

        # å†å¾ L2 ç²å–
        entry = await self.l2_backend.get(key)
        if entry:
            # æå‡åˆ° L1
            await self.l1_backend.set(key, entry)
            return entry.value

        return None

    async def set(
        self,
        key: str,
        value: Any,
        ttl: float | None = None,
        level: CacheLevel = CacheLevel.BOTH,
    ) -> bool:
        """è¨­ç½®ç·©å­˜å€¼"""
        if ttl is None:
            ttl = self.default_ttl

        entry = CacheEntry(key=key, value=value, ttl=ttl, load_time=0.0)

        success = True

        # æ ¹æ“šç´šåˆ¥è¨­ç½®ç·©å­˜
        if level in [CacheLevel.L1, CacheLevel.BOTH]:
            success &= await self.l1_backend.set(key, entry)

        if level in [CacheLevel.L2, CacheLevel.BOTH]:
            success &= await self.l2_backend.set(key, entry)

        return success

    async def delete(self, key: str, level: CacheLevel = CacheLevel.BOTH) -> bool:
        """åˆªé™¤ç·©å­˜å€¼"""
        success = True

        if level in [CacheLevel.L1, CacheLevel.BOTH]:
            success &= await self.l1_backend.delete(key)

        if level in [CacheLevel.L2, CacheLevel.BOTH]:
            success &= await self.l2_backend.delete(key)

        return success

    async def clear(self, level: CacheLevel = CacheLevel.BOTH) -> bool:
        """æ¸…ç©ºç·©å­˜"""
        success = True

        if level in [CacheLevel.L1, CacheLevel.BOTH]:
            success &= await self.l1_backend.clear()

        if level in [CacheLevel.L2, CacheLevel.BOTH]:
            success &= await self.l2_backend.clear()

        return success

    async def exists(self, key: str) -> bool:
        """æª¢æŸ¥éµæ˜¯å¦å­˜åœ¨"""
        return await self.get(key) is not None

    async def expire(self, key: str, ttl: float) -> bool:
        """è¨­ç½®éæœŸæ™‚é–“"""
        # æ›´æ–° L1
        entry = await self.l1_backend.get(key)
        if entry:
            entry.ttl = ttl
            entry.timestamp = time.time()
            await self.l1_backend.set(key, entry)

        # æ›´æ–° L2
        entry = await self.l2_backend.get(key)
        if entry:
            entry.ttl = ttl
            entry.timestamp = time.time()
            await self.l2_backend.set(key, entry)

        return True

    def get_stats(self) -> dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        return {
            "l1": self.l1_backend.stats.__dict__,
            "l2": self.l2_backend.stats.__dict__,
        }

    async def get_detailed_stats(self) -> dict[str, Any]:
        """ç²å–è©³ç´°çµ±è¨ˆä¿¡æ¯"""
        l1_stats = await self.l1_backend.get_detailed_stats()
        l2_stats = self.l2_backend.stats.__dict__

        return {
            "l1": l1_stats,
            "l2": l2_stats,
            "combined": {
                "total_hits": l1_stats["hits"] + l2_stats["hits"],
                "total_misses": l1_stats["misses"] + l2_stats["misses"],
                "combined_hit_rate": (l1_stats["hits"] + l2_stats["hits"])
                / (
                    l1_stats["hits"]
                    + l1_stats["misses"]
                    + l2_stats["hits"]
                    + l2_stats["misses"]
                )
                if (
                    l1_stats["hits"]
                    + l1_stats["misses"]
                    + l2_stats["hits"]
                    + l2_stats["misses"]
                )
                > 0
                else 0,
                "total_entries": l1_stats["entry_count"] + l2_stats["entry_count"],
            },
        }

    def register_preload_function(self, pattern: str, func: Callable):
        """è¨»å†Šé è¼‰å…¥å‡½æ•¸"""
        if self.preloader:
            self.preloader.register_preload_function(pattern, func)

    @asynccontextmanager
    async def batch_operation(self):
        """æ‰¹é‡æ“ä½œä¸Šä¸‹æ–‡"""
        # é€™è£¡å¯ä»¥å¯¦ç¾æ‰¹é‡æ“ä½œå„ªåŒ–
        yield

    async def shutdown(self):
        """é—œé–‰ç·©å­˜ç®¡ç†å™¨"""
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            with suppress(asyncio.CancelledError):
                await self._cleanup_task

        # æ¸…ç†é€£æ¥æ± 
        with self.l2_backend._pool_lock:
            for conn in self.l2_backend._connection_pool:
                conn.close()
            self.l2_backend._connection_pool.clear()


# å…¨åŸŸç·©å­˜ç®¡ç†å™¨
_global_cache_manager: MultiLevelCache | None = None


async def get_global_cache_manager() -> MultiLevelCache:
    """ç²å–å…¨åŸŸç·©å­˜ç®¡ç†å™¨"""
    global _global_cache_manager
    if _global_cache_manager is None:
        _global_cache_manager = MultiLevelCache()
    return _global_cache_manager


async def dispose_global_cache_manager():
    """é‡‹æ”¾å…¨åŸŸç·©å­˜ç®¡ç†å™¨"""
    global _global_cache_manager
    if _global_cache_manager is not None:
        await _global_cache_manager.shutdown()
        _global_cache_manager = None


# ä¾¿æ·å‡½æ•¸
async def cache_get(key: str) -> Any | None:
    """ç²å–ç·©å­˜å€¼"""
    cache = await get_global_cache_manager()
    return await cache.get(key)


async def cache_set(key: str, value: Any, ttl: float | None = None) -> bool:
    """è¨­ç½®ç·©å­˜å€¼"""
    cache = await get_global_cache_manager()
    return await cache.set(key, value, ttl)


async def cache_delete(key: str) -> bool:
    """åˆªé™¤ç·©å­˜å€¼"""
    cache = await get_global_cache_manager()
    return await cache.delete(key)


def cache_key(*args, **kwargs) -> str:
    """ç”Ÿæˆç·©å­˜éµ"""
    # å‰µå»ºä¸€å€‹å”¯ä¸€çš„ç·©å­˜éµ
    key_parts = []

    # æ·»åŠ ä½ç½®åƒæ•¸
    for arg in args:
        if hasattr(arg, "__dict__"):
            key_parts.append(f"{type(arg).__name__}:{id(arg)}")
        else:
            key_parts.append(str(arg))

    # æ·»åŠ é—œéµå­—åƒæ•¸
    for k, v in sorted(kwargs.items()):
        key_parts.append(f"{k}:{v}")

    # ç”Ÿæˆå“ˆå¸Œ
    key_string = ":".join(key_parts)
    return hashlib.sha256(key_string.encode()).hexdigest()


def cached(ttl: float | None = None, key_func: Callable | None = None):
    """ç·©å­˜è£é£¾å™¨"""

    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç·©å­˜éµ
            if key_func:
                cache_key_str = key_func(*args, **kwargs)
            else:
                cache_key_str = f"{func.__name__}:{cache_key(*args, **kwargs)}"

            # å˜—è©¦å¾ç·©å­˜ç²å–
            cached_result = await cache_get(cache_key_str)
            if cached_result is not None:
                return cached_result

            # åŸ·è¡Œå‡½æ•¸ä¸¦ç·©å­˜çµæœ
            start_time = time.time()
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time

            # è¨­ç½®ç·©å­˜
            await cache_set(cache_key_str, result, ttl)

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            logger.debug(f"å‡½æ•¸ {func.__name__} åŸ·è¡Œè€—æ™‚: {execution_time:.3f}s")

            return result

        return wrapper

    return decorator
