"""
ğŸšŒ äº‹ä»¶ç¸½ç·šç³»çµ±
Discord ADR Bot v1.6 - çµ±ä¸€äº‹ä»¶ç®¡ç†æ¶æ§‹

æä¾›ä¼æ¥­ç´šçš„äº‹ä»¶é©…å‹•åŠŸèƒ½:
- äº‹ä»¶ç™¼å¸ƒ/è¨‚é–±æ©Ÿåˆ¶
- äº‹ä»¶éæ¿¾å’Œè·¯ç”±
- äº‹ä»¶æŒä¹…åŒ–å’Œé‡æ”¾
- äº‹ä»¶å„ªå…ˆç´šç®¡ç†
- ç•°æ­¥äº‹ä»¶è™•ç†
- äº‹ä»¶æ‰¹è™•ç†å„ªåŒ–
- æ™ºèƒ½äº‹ä»¶è·¯ç”±
- æ€§èƒ½ç›£æ§å’Œåˆ†æ

ä½œè€…:Discord ADR Bot æ¶æ§‹å¸«
ç‰ˆæœ¬:v1.6
"""

import asyncio
import json
import logging
import statistics
import time
from abc import ABC, abstractmethod
from collections import defaultdict, deque
from collections.abc import Awaitable, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from enum import Enum
from typing import (
    Any,
    TypeVar,
)

# è¨­ç½®æ—¥èªŒ
logger = logging.getLogger(__name__)

# é¡å‹è®Šé‡
T = TypeVar("T")
EventHandler = Callable[["Event"], Awaitable[None]]


class EventPriority(Enum):
    """äº‹ä»¶å„ªå…ˆç´šæšèˆ‰"""

    CRITICAL = 0  # é—œéµäº‹ä»¶,æœ€é«˜å„ªå…ˆç´š
    HIGH = 1  # é«˜å„ªå…ˆç´šäº‹ä»¶
    NORMAL = 2  # æ™®é€šå„ªå…ˆç´šäº‹ä»¶
    LOW = 3  # ä½å„ªå…ˆç´šäº‹ä»¶
    BACKGROUND = 4  # èƒŒæ™¯äº‹ä»¶,æœ€ä½å„ªå…ˆç´š


class EventStatus(Enum):
    """äº‹ä»¶ç‹€æ…‹æšèˆ‰"""

    PENDING = "pending"  # å¾…è™•ç†
    PROCESSING = "processing"  # è™•ç†ä¸­
    COMPLETED = "completed"  # å·²å®Œæˆ
    FAILED = "failed"  # è™•ç†å¤±æ•—
    CANCELLED = "cancelled"  # å·²å–æ¶ˆ
    BATCHED = "batched"  # å·²æ‰¹è™•ç†


class EventProcessingMode(Enum):
    """äº‹ä»¶è™•ç†æ¨¡å¼æšèˆ‰"""

    IMMEDIATE = "immediate"  # ç«‹å³è™•ç†
    BATCHED = "batched"  # æ‰¹è™•ç†
    SCHEDULED = "scheduled"  # å®šæ™‚è™•ç†
    ADAPTIVE = "adaptive"  # è‡ªé©æ‡‰è™•ç†


@dataclass
class Event:
    """äº‹ä»¶åŸºç¤é¡åˆ¥"""

    event_type: str
    data: dict[str, Any] = field(default_factory=dict)
    source: str | None = None
    target: str | None = None
    priority: EventPriority = EventPriority.NORMAL
    timestamp: float = field(default_factory=time.time)
    event_id: str | None = None
    correlation_id: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    processing_mode: EventProcessingMode = EventProcessingMode.IMMEDIATE
    batch_key: str | None = None  # æ‰¹è™•ç†éµ
    retry_count: int = 0
    max_retries: int = 3

    def __post_init__(self):
        if self.event_id is None:
            self.event_id = f"{self.event_type}_{int(self.timestamp * 1000000)}"

        # è‡ªå‹•ç”Ÿæˆæ‰¹è™•ç†éµ
        if (
            self.batch_key is None
            and self.processing_mode == EventProcessingMode.BATCHED
        ):
            self.batch_key = f"{self.event_type}_{self.source or 'default'}"

    def __lt__(self, other):
        """ç”¨æ–¼å„ªå…ˆç´šéšŠåˆ—æ’åº"""
        return self.priority.value < other.priority.value

    def to_dict(self) -> dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸æ ¼å¼"""
        return {
            "event_type": self.event_type,
            "data": self.data,
            "source": self.source,
            "target": self.target,
            "priority": self.priority.value,
            "timestamp": self.timestamp,
            "event_id": self.event_id,
            "correlation_id": self.correlation_id,
            "metadata": self.metadata,
            "processing_mode": self.processing_mode.value,
            "batch_key": self.batch_key,
            "retry_count": self.retry_count,
            "max_retries": self.max_retries,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "Event":
        """å¾å­—å…¸å‰µå»ºäº‹ä»¶"""
        return cls(
            event_type=data["event_type"],
            data=data.get("data", {}),
            source=data.get("source"),
            target=data.get("target"),
            priority=EventPriority(data.get("priority", EventPriority.NORMAL.value)),
            timestamp=data.get("timestamp", time.time()),
            event_id=data.get("event_id"),
            correlation_id=data.get("correlation_id"),
            metadata=data.get("metadata", {}),
            processing_mode=EventProcessingMode(
                data.get("processing_mode", EventProcessingMode.IMMEDIATE.value)
            ),
            batch_key=data.get("batch_key"),
            retry_count=data.get("retry_count", 0),
            max_retries=data.get("max_retries", 3),
        )

    def can_retry(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦å¯ä»¥é‡è©¦"""
        return self.retry_count < self.max_retries

    def increment_retry(self):
        """å¢åŠ é‡è©¦æ¬¡æ•¸"""
        self.retry_count += 1

    def get_size(self) -> int:
        """ä¼°ç®—äº‹ä»¶å¤§å°(å­—ç¯€)"""
        try:
            return len(json.dumps(self.to_dict()).encode("utf-8"))
        except:
            return len(str(self.data).encode("utf-8"))


@dataclass
class EventBatch:
    """äº‹ä»¶æ‰¹æ¬¡"""

    batch_key: str
    events: list[Event] = field(default_factory=list)
    created_at: float = field(default_factory=time.time)
    max_size: int = 100
    max_wait_time: float = 1.0  # æœ€å¤§ç­‰å¾…æ™‚é–“(ç§’)

    def add_event(self, event: Event) -> bool:
        """æ·»åŠ äº‹ä»¶åˆ°æ‰¹æ¬¡"""
        if len(self.events) >= self.max_size:
            return False

        event.processing_mode = EventProcessingMode.BATCHED
        self.events.append(event)
        return True

    def is_ready(self) -> bool:
        """æª¢æŸ¥æ‰¹æ¬¡æ˜¯å¦æº–å‚™å¥½è™•ç†"""
        return (
            len(self.events) >= self.max_size
            or time.time() - self.created_at >= self.max_wait_time
        )

    def get_total_size(self) -> int:
        """ç²å–æ‰¹æ¬¡ç¸½å¤§å°"""
        return sum(event.get_size() for event in self.events)


@dataclass
class EventSubscription:
    """äº‹ä»¶è¨‚é–±æè¿°ç¬¦"""

    handler: EventHandler
    event_types: set[str]
    filters: list[Callable[["Event"], bool]] = field(default_factory=list)
    priority: EventPriority = EventPriority.NORMAL
    max_retries: int = 3
    retry_delay: float = 1.0
    enabled: bool = True
    subscriber_id: str | None = None
    processing_mode: EventProcessingMode = EventProcessingMode.IMMEDIATE
    batch_size: int = 10
    max_batch_wait: float = 1.0

    def matches(self, event: Event) -> bool:
        """æª¢æŸ¥äº‹ä»¶æ˜¯å¦åŒ¹é…æ­¤è¨‚é–±"""
        # æª¢æŸ¥äº‹ä»¶é¡å‹
        if event.event_type not in self.event_types and "*" not in self.event_types:
            return False

        # æª¢æŸ¥éæ¿¾å™¨
        for filter_func in self.filters:
            try:
                if not filter_func(event):
                    return False
            except Exception as e:
                logger.warning(f"ã€äº‹ä»¶ç¸½ç·šã€‘éæ¿¾å™¨åŸ·è¡Œå¤±æ•—: {e}")
                return False

        return True


class EventMetrics:
    """äº‹ä»¶æŒ‡æ¨™æ”¶é›†å™¨"""

    def __init__(self):
        self.total_events_published = 0
        self.total_events_processed = 0
        self.total_events_failed = 0
        self.total_processing_time = 0.0
        self.events_by_type: dict[str, int] = defaultdict(int)
        self.events_by_priority: dict[str, int] = defaultdict(int)
        self.processing_times: deque[float] = deque(maxlen=1000)
        self.error_count = 0
        self.batch_count = 0
        self.batch_sizes: deque[int] = deque(maxlen=100)
        self.created_at = time.time()
        self._lock = asyncio.Lock()

    async def record_event_published(self, event: Event):
        """è¨˜éŒ„äº‹ä»¶ç™¼å¸ƒ"""
        async with self._lock:
            self.total_events_published += 1
            self.events_by_type[event.event_type] += 1
            self.events_by_priority[event.priority.name] += 1

    async def record_event_processed(
        self, event: Event, processing_time: float, success: bool = True
    ):
        """è¨˜éŒ„äº‹ä»¶è™•ç†"""
        async with self._lock:
            if success:
                self.total_events_processed += 1
                self.total_processing_time += processing_time
                self.processing_times.append(processing_time)
            else:
                self.total_events_failed += 1
                self.error_count += 1

    async def record_batch_processed(self, batch: EventBatch, processing_time: float):
        """è¨˜éŒ„æ‰¹æ¬¡è™•ç†"""
        async with self._lock:
            self.batch_count += 1
            self.batch_sizes.append(len(batch.events))

            # è¨˜éŒ„æ‰¹æ¬¡ä¸­æ¯å€‹äº‹ä»¶çš„è™•ç†
            for event in batch.events:
                await self.record_event_processed(
                    event, processing_time / len(batch.events)
                )

    async def get_metrics_summary(self) -> dict[str, Any]:
        """ç²å–æŒ‡æ¨™æ‘˜è¦"""
        async with self._lock:
            uptime = time.time() - self.created_at
            avg_processing_time = (
                statistics.mean(self.processing_times) if self.processing_times else 0.0
            )
            avg_batch_size = (
                statistics.mean(self.batch_sizes) if self.batch_sizes else 0.0
            )

            return {
                "uptime_seconds": uptime,
                "total_events_published": self.total_events_published,
                "total_events_processed": self.total_events_processed,
                "total_events_failed": self.total_events_failed,
                "success_rate": (
                    self.total_events_processed
                    / (self.total_events_processed + self.total_events_failed)
                    if (self.total_events_processed + self.total_events_failed) > 0
                    else 1.0
                ),
                "avg_processing_time_ms": avg_processing_time * 1000,
                "events_per_second": self.total_events_published / uptime
                if uptime > 0
                else 0.0,
                "events_by_type": dict(self.events_by_type),
                "events_by_priority": dict(self.events_by_priority),
                "batch_count": self.batch_count,
                "avg_batch_size": avg_batch_size,
                "error_rate": self.error_count / self.total_events_published
                if self.total_events_published > 0
                else 0.0,
            }


class EventRouter:
    """æ™ºèƒ½äº‹ä»¶è·¯ç”±å™¨"""

    def __init__(self):
        self.routing_rules: list[Callable[[Event], list[str]]] = []
        self.performance_cache: dict[str, float] = {}
        self._lock = asyncio.Lock()

    def add_routing_rule(self, rule: Callable[[Event], list[str]]):
        """æ·»åŠ è·¯ç”±è¦å‰‡"""
        self.routing_rules.append(rule)

    async def route_event(
        self, event: Event, subscriptions: dict[str, EventSubscription]
    ) -> list[str]:
        """è·¯ç”±äº‹ä»¶åˆ°åˆé©çš„è¨‚é–±è€…"""
        async with self._lock:
            matched_subscribers = []

            # æ‡‰ç”¨è·¯ç”±è¦å‰‡
            for rule in self.routing_rules:
                try:
                    rule_results = rule(event)
                    matched_subscribers.extend(rule_results)
                except Exception as e:
                    logger.warning(f"ã€äº‹ä»¶ç¸½ç·šã€‘è·¯ç”±è¦å‰‡åŸ·è¡Œå¤±æ•—: {e}")

            # å¦‚æœæ²’æœ‰è·¯ç”±è¦å‰‡åŒ¹é…,ä½¿ç”¨é»˜èªåŒ¹é…
            if not matched_subscribers:
                for subscriber_id, subscription in subscriptions.items():
                    if subscription.enabled and subscription.matches(event):
                        matched_subscribers.append(subscriber_id)

            # æ ¹æ“šæ€§èƒ½ç·©å­˜æ’åºè¨‚é–±è€…
            matched_subscribers.sort(
                key=lambda sub_id: self.performance_cache.get(sub_id, 0.0)
            )

            return matched_subscribers

    async def update_performance(self, subscriber_id: str, processing_time: float):
        """æ›´æ–°è¨‚é–±è€…æ€§èƒ½æŒ‡æ¨™"""
        async with self._lock:
            # ä½¿ç”¨æŒ‡æ•¸ç§»å‹•å¹³å‡æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            if subscriber_id in self.performance_cache:
                self.performance_cache[subscriber_id] = (
                    0.7 * self.performance_cache[subscriber_id] + 0.3 * processing_time
                )
            else:
                self.performance_cache[subscriber_id] = processing_time


class EventCompressor:
    """äº‹ä»¶å£“ç¸®å™¨"""

    def __init__(self):
        self.compression_rules: dict[str, Callable[[list[Event]], list[Event]]] = {}

    def register_compression_rule(
        self, event_type: str, rule: Callable[[list[Event]], list[Event]]
    ):
        """è¨»å†Šå£“ç¸®è¦å‰‡"""
        self.compression_rules[event_type] = rule

    async def compress_events(self, events: list[Event]) -> list[Event]:
        """å£“ç¸®äº‹ä»¶åˆ—è¡¨"""
        if not events:
            return events

        # æŒ‰äº‹ä»¶é¡å‹åˆ†çµ„
        events_by_type = defaultdict(list)
        for event in events:
            events_by_type[event.event_type].append(event)

        compressed_events = []

        # å°æ¯ç¨®äº‹ä»¶é¡å‹æ‡‰ç”¨å£“ç¸®è¦å‰‡
        for event_type, type_events in events_by_type.items():
            if event_type in self.compression_rules:
                try:
                    compressed = self.compression_rules[event_type](type_events)
                    compressed_events.extend(compressed)
                except Exception as e:
                    logger.warning(f"ã€äº‹ä»¶ç¸½ç·šã€‘äº‹ä»¶å£“ç¸®å¤±æ•— {event_type}: {e}")
                    compressed_events.extend(type_events)
            else:
                compressed_events.extend(type_events)

        return compressed_events


class EventFilter:
    """äº‹ä»¶éæ¿¾å™¨å·¥å…·é¡åˆ¥"""

    @staticmethod
    def by_source(source: str) -> Callable[["Event"], bool]:
        """æŒ‰ä¾†æºéæ¿¾"""
        return lambda event: event.source == source

    @staticmethod
    def by_target(target: str) -> Callable[["Event"], bool]:
        """æŒ‰ç›®æ¨™éæ¿¾"""
        return lambda event: event.target == target

    @staticmethod
    def by_priority(min_priority: EventPriority) -> Callable[["Event"], bool]:
        """æŒ‰å„ªå…ˆç´šéæ¿¾"""
        return lambda event: event.priority.value <= min_priority.value

    @staticmethod
    def by_data_key(key: str, value: Any = None) -> Callable[["Event"], bool]:
        """æŒ‰æ•¸æ“šéµéæ¿¾"""
        if value is None:
            return lambda event: key in event.data
        else:
            return lambda event: event.data.get(key) == value

    @staticmethod
    def by_time_range(start_time: float, end_time: float) -> Callable[["Event"], bool]:
        """æŒ‰æ™‚é–“ç¯„åœéæ¿¾"""
        return lambda event: start_time <= event.timestamp <= end_time

    @staticmethod
    def by_correlation_id(correlation_id: str) -> Callable[["Event"], bool]:
        """æŒ‰é—œè¯IDéæ¿¾"""
        return lambda event: event.correlation_id == correlation_id


class EventPersistence(ABC):
    """äº‹ä»¶æŒä¹…åŒ–æŠ½è±¡åŸºé¡"""

    @abstractmethod
    async def save_event(self, event: Event) -> bool:
        """ä¿å­˜äº‹ä»¶"""
        pass

    @abstractmethod
    async def load_events(
        self,
        event_type: str | None = None,
        start_time: float | None = None,
        end_time: float | None = None,
        limit: int = 100,
    ) -> list[Event]:
        """åŠ è¼‰äº‹ä»¶"""
        pass

    @abstractmethod
    async def delete_events(self, event_ids: list[str]) -> int:
        """åˆªé™¤äº‹ä»¶"""
        pass

    @abstractmethod
    async def save_batch(self, batch: EventBatch) -> bool:
        """ä¿å­˜äº‹ä»¶æ‰¹æ¬¡"""
        pass


class MemoryEventPersistence(EventPersistence):
    """å…§å­˜äº‹ä»¶æŒä¹…åŒ–å¯¦ç¾"""

    def __init__(self, max_events: int = 10000):
        self.events: list[Event] = []
        self.batches: list[EventBatch] = []
        self.max_events = max_events
        self._lock = asyncio.Lock()

    async def save_event(self, event: Event) -> bool:
        """ä¿å­˜äº‹ä»¶åˆ°å…§å­˜"""
        async with self._lock:
            self.events.append(event)

            # å¦‚æœè¶…éæœ€å¤§æ•¸é‡,ç§»é™¤æœ€èˆŠçš„äº‹ä»¶
            if len(self.events) > self.max_events:
                self.events = self.events[-self.max_events :]

            return True

    async def save_batch(self, batch: EventBatch) -> bool:
        """ä¿å­˜äº‹ä»¶æ‰¹æ¬¡"""
        async with self._lock:
            self.batches.append(batch)

            # ä¿å­˜æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰äº‹ä»¶
            for event in batch.events:
                await self.save_event(event)

            return True

    async def load_events(
        self,
        event_type: str | None = None,
        start_time: float | None = None,
        end_time: float | None = None,
        limit: int = 100,
    ) -> list[Event]:
        """å¾å…§å­˜åŠ è¼‰äº‹ä»¶"""
        async with self._lock:
            filtered_events = []

            for event in self.events:
                # æ‡‰ç”¨éæ¿¾æ¢ä»¶
                if event_type and event.event_type != event_type:
                    continue
                if start_time and event.timestamp < start_time:
                    continue
                if end_time and event.timestamp > end_time:
                    continue

                filtered_events.append(event)

                if len(filtered_events) >= limit:
                    break

            return filtered_events

    async def delete_events(self, event_ids: list[str]) -> int:
        """å¾å…§å­˜åˆªé™¤äº‹ä»¶"""
        async with self._lock:
            deleted_count = 0
            self.events = [
                event
                for event in self.events
                if event.event_id not in event_ids
                or (deleted_count := deleted_count + 1, False)[1]
            ]
            return deleted_count


class EventBus:
    """é«˜æ€§èƒ½äº‹ä»¶ç¸½ç·š"""

    def __init__(self, persistence: EventPersistence | None = None):
        self._subscriptions: dict[str, EventSubscription] = {}
        self._event_queue: asyncio.PriorityQueue = asyncio.PriorityQueue()
        self._batch_queues: dict[str, EventBatch] = {}
        self._processing_tasks: set[asyncio.Task] = set()
        self._batch_tasks: set[asyncio.Task] = set()
        self._shutdown_event = asyncio.Event()
        self._lock = asyncio.Lock()
        self._batch_lock = asyncio.Lock()

        # çµ„ä»¶åˆå§‹åŒ–
        self.persistence = persistence or MemoryEventPersistence()
        self.metrics = EventMetrics()
        self.router = EventRouter()
        self.compressor = EventCompressor()

        # é…ç½®åƒæ•¸
        self.max_workers = 10
        self.batch_processing_interval = 0.1  # 100ms
        self.enable_compression = True
        self.enable_batch_processing = True

        # æ€§èƒ½ç›£æ§
        self._processing_times: deque[float] = deque(maxlen=1000)
        self._error_counts: dict[str, int] = defaultdict(int)

        # è‡ªå‹•è¨­ç½®å£“ç¸®è¦å‰‡
        self._setup_default_compression_rules()

    def _setup_default_compression_rules(self):
        """è¨­ç½®é»˜èªå£“ç¸®è¦å‰‡"""

        # åˆä½µç›¸åŒé¡å‹çš„ç‹€æ…‹æ›´æ–°äº‹ä»¶
        def merge_status_updates(events: list[Event]) -> list[Event]:
            if len(events) <= 1:
                return events

            # ä¿ç•™æœ€æ–°çš„ç‹€æ…‹æ›´æ–°
            latest_event = max(events, key=lambda e: e.timestamp)
            return [latest_event]

        self.compressor.register_compression_rule("status_update", merge_status_updates)
        self.compressor.register_compression_rule("heartbeat", merge_status_updates)

        # åˆä½µè¨ˆæ•¸å™¨äº‹ä»¶
        def merge_counter_events(events: list[Event]) -> list[Event]:
            if len(events) <= 1:
                return events

            total_count = sum(event.data.get("count", 1) for event in events)
            merged_event = events[-1]  # ä½¿ç”¨æœ€æ–°äº‹ä»¶ä½œç‚ºåŸºç¤
            merged_event.data["count"] = total_count
            return [merged_event]

        self.compressor.register_compression_rule(
            "counter_increment", merge_counter_events
        )

    async def initialize(self):
        """åˆå§‹åŒ–äº‹ä»¶ç¸½ç·š"""
        # å•Ÿå‹•è™•ç†å·¥ä½œè€…
        for _i in range(self.max_workers):
            task = asyncio.create_task(self._process_events())
            self._processing_tasks.add(task)
            task.add_done_callback(self._processing_tasks.discard)

        # å•Ÿå‹•æ‰¹è™•ç†ä»»å‹™
        if self.enable_batch_processing:
            batch_task = asyncio.create_task(self._process_batches())
            self._batch_tasks.add(batch_task)
            batch_task.add_done_callback(self._batch_tasks.discard)

        logger.info("ã€äº‹ä»¶ç¸½ç·šã€‘äº‹ä»¶ç¸½ç·šå·²åˆå§‹åŒ–")

    async def shutdown(self):
        """é—œé–‰äº‹ä»¶ç¸½ç·š"""
        logger.info("ã€äº‹ä»¶ç¸½ç·šã€‘æ­£åœ¨é—œé–‰äº‹ä»¶ç¸½ç·š...")

        # è¨­ç½®é—œé–‰ä¿¡è™Ÿ
        self._shutdown_event.set()

        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        all_tasks = self._processing_tasks | self._batch_tasks
        if all_tasks:
            await asyncio.gather(*all_tasks, return_exceptions=True)

        logger.info("ã€äº‹ä»¶ç¸½ç·šã€‘äº‹ä»¶ç¸½ç·šå·²é—œé–‰")

    def subscribe(
        self,
        event_types: list[str],
        handler: EventHandler,
        filters: list[Callable[["Event | None"], bool]] | None = None,
        priority: EventPriority = EventPriority.NORMAL,
        max_retries: int = 3,
        subscriber_id: str | None = None,
        processing_mode: EventProcessingMode = EventProcessingMode.IMMEDIATE,
        batch_size: int = 10,
    ) -> str:
        """è¨‚é–±äº‹ä»¶"""
        if isinstance(event_types, str):
            event_types = [event_types]

        if subscriber_id is None:
            subscriber_id = f"sub_{int(time.time() * 1000000)}_{id(handler)}"

        subscription = EventSubscription(
            handler=handler,
            event_types=set(event_types),
            filters=filters or [],
            priority=priority,
            max_retries=max_retries,
            subscriber_id=subscriber_id,
            processing_mode=processing_mode,
            batch_size=batch_size,
        )

        self._subscriptions[subscriber_id] = subscription

        logger.debug(f"ã€äº‹ä»¶ç¸½ç·šã€‘æ–°è¨‚é–±: {subscriber_id} -> {event_types}")
        return subscriber_id

    def unsubscribe(self, subscriber_id: str) -> bool:
        """å–æ¶ˆè¨‚é–±"""
        if subscriber_id in self._subscriptions:
            del self._subscriptions[subscriber_id]
            logger.debug(f"ã€äº‹ä»¶ç¸½ç·šã€‘å–æ¶ˆè¨‚é–±: {subscriber_id}")
            return True
        return False

    async def publish(self, event: Event, persist: bool = True) -> bool:
        """ç™¼å¸ƒäº‹ä»¶"""
        try:
            # è¨˜éŒ„æŒ‡æ¨™
            await self.metrics.record_event_published(event)

            # æŒä¹…åŒ–äº‹ä»¶
            if persist:
                await self.persistence.save_event(event)

            # æ ¹æ“šè™•ç†æ¨¡å¼è·¯ç”±äº‹ä»¶
            if (
                event.processing_mode == EventProcessingMode.BATCHED
                and self.enable_batch_processing
            ):
                await self._add_to_batch(event)
            else:
                # ç«‹å³è™•ç†
                await self._event_queue.put((event.priority.value, time.time(), event))

            return True

        except Exception as e:
            logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘ç™¼å¸ƒäº‹ä»¶å¤±æ•—: {e}")
            return False

    async def publish_batch(self, events: list[Event], persist: bool = True) -> bool:
        """æ‰¹é‡ç™¼å¸ƒäº‹ä»¶"""
        try:
            # å£“ç¸®äº‹ä»¶(å¦‚æœå•Ÿç”¨)
            if self.enable_compression and len(events) > 1:
                events = await self.compressor.compress_events(events)

            # ç™¼å¸ƒæ¯å€‹äº‹ä»¶
            for event in events:
                await self.publish(event, persist)

            return True

        except Exception as e:
            logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘æ‰¹é‡ç™¼å¸ƒäº‹ä»¶å¤±æ•—: {e}")
            return False

    async def publish_sync(self, event: Event) -> list[Any]:
        """åŒæ­¥ç™¼å¸ƒäº‹ä»¶ä¸¦ç­‰å¾…æ‰€æœ‰è™•ç†å®Œæˆ"""
        results = []

        # ç²å–åŒ¹é…çš„è¨‚é–±è€…
        matched_subscribers = await self.router.route_event(event, self._subscriptions)

        # åŒæ­¥åŸ·è¡Œæ‰€æœ‰è™•ç†å™¨
        for subscriber_id in matched_subscribers:
            subscription = self._subscriptions.get(subscriber_id)
            if subscription and subscription.enabled:
                try:
                    start_time = time.time()
                    result = await subscription.handler(event)
                    processing_time = time.time() - start_time

                    results.append(result)

                    # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
                    await self.router.update_performance(subscriber_id, processing_time)
                    await self.metrics.record_event_processed(
                        event, processing_time, True
                    )

                except Exception as e:
                    logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘åŒæ­¥è™•ç†äº‹ä»¶å¤±æ•— {subscriber_id}: {e}")
                    results.append(e)
                    await self.metrics.record_event_processed(event, 0, False)

        return results

    async def _add_to_batch(self, event: Event):
        """æ·»åŠ äº‹ä»¶åˆ°æ‰¹è™•ç†éšŠåˆ—"""
        async with self._batch_lock:
            batch_key = event.batch_key or f"{event.event_type}_default"

            if batch_key not in self._batch_queues:
                self._batch_queues[batch_key] = EventBatch(
                    batch_key=batch_key,
                    max_size=10,  # å¯é…ç½®
                    max_wait_time=1.0,  # å¯é…ç½®
                )

            batch = self._batch_queues[batch_key]
            if not batch.add_event(event):
                # æ‰¹æ¬¡å·²æ»¿,è™•ç†ç•¶å‰æ‰¹æ¬¡ä¸¦å‰µå»ºæ–°æ‰¹æ¬¡
                await self._process_batch(batch)

                # å‰µå»ºæ–°æ‰¹æ¬¡
                new_batch = EventBatch(
                    batch_key=batch_key, max_size=10, max_wait_time=1.0
                )
                new_batch.add_event(event)
                self._batch_queues[batch_key] = new_batch

    async def _process_batches(self):
        """æ‰¹è™•ç†ä»»å‹™å¾ªç’°"""
        while not self._shutdown_event.is_set():
            try:
                await asyncio.sleep(self.batch_processing_interval)

                async with self._batch_lock:
                    ready_batches = []

                    for batch_key, batch in list(self._batch_queues.items()):
                        if batch.is_ready():
                            ready_batches.append(batch)
                            del self._batch_queues[batch_key]

                # è™•ç†æº–å‚™å¥½çš„æ‰¹æ¬¡
                for batch in ready_batches:
                    await self._process_batch(batch)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘æ‰¹è™•ç†å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(1)

    async def _process_batch(self, batch: EventBatch):
        """è™•ç†äº‹ä»¶æ‰¹æ¬¡"""
        try:
            start_time = time.time()

            # æŒä¹…åŒ–æ‰¹æ¬¡
            await self.persistence.save_batch(batch)

            # ç²å–æ‰¹æ¬¡ä¸­æ‰€æœ‰äº‹ä»¶çš„åŒ¹é…è¨‚é–±è€…
            all_subscribers = set()
            for event in batch.events:
                subscribers = await self.router.route_event(event, self._subscriptions)
                all_subscribers.update(subscribers)

            # ç‚ºæ¯å€‹è¨‚é–±è€…è™•ç†æ‰¹æ¬¡
            for subscriber_id in all_subscribers:
                subscription = self._subscriptions.get(subscriber_id)
                if not subscription or not subscription.enabled:
                    continue

                # éæ¿¾å‡ºè©²è¨‚é–±è€…é—œå¿ƒçš„äº‹ä»¶
                relevant_events = [
                    event for event in batch.events if subscription.matches(event)
                ]

                if relevant_events:
                    await self._execute_batch_handler(subscription, relevant_events)

            processing_time = time.time() - start_time
            await self.metrics.record_batch_processed(batch, processing_time)

            logger.debug(
                f"ã€äº‹ä»¶ç¸½ç·šã€‘æ‰¹æ¬¡è™•ç†å®Œæˆ: {batch.batch_key} ({len(batch.events)} äº‹ä»¶)"
            )

        except Exception as e:
            logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")

    async def _execute_batch_handler(
        self, subscription: EventSubscription, events: list[Event]
    ):
        """åŸ·è¡Œæ‰¹æ¬¡è™•ç†å™¨"""
        try:
            # å¦‚æœè™•ç†å™¨æ”¯æŒæ‰¹æ¬¡è™•ç†,å‚³éäº‹ä»¶åˆ—è¡¨
            if hasattr(subscription.handler, "__annotations__"):
                sig = subscription.handler.__annotations__
                if "events" in sig or len(sig) > 1:
                    # æ”¯æŒæ‰¹æ¬¡è™•ç† - ä½†ç›®å‰çš„è™•ç†å™¨åªæ¥å—å–®å€‹äº‹ä»¶,æ‰€ä»¥é€å€‹è™•ç†
                    for event in events:
                        await subscription.handler(event)
                    return

            # å¦å‰‡é€å€‹è™•ç†äº‹ä»¶
            for event in events:
                await subscription.handler(event)

        except Exception as e:
            logger.error(
                f"ã€äº‹ä»¶ç¸½ç·šã€‘æ‰¹æ¬¡è™•ç†å™¨åŸ·è¡Œå¤±æ•— {subscription.subscriber_id}: {e}"
            )

    async def _process_events(self):
        """äº‹ä»¶è™•ç†å·¥ä½œè€…å¾ªç’°"""
        while not self._shutdown_event.is_set():
            try:
                # å¾éšŠåˆ—ç²å–äº‹ä»¶(å¸¶è¶…æ™‚)
                try:
                    priority, enqueue_time, event = await asyncio.wait_for(
                        self._event_queue.get(), timeout=1.0
                    )
                except TimeoutError:
                    continue

                await self._handle_event(event)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘äº‹ä»¶è™•ç†å·¥ä½œè€…éŒ¯èª¤: {e}")
                await asyncio.sleep(0.1)

    async def _handle_event(self, event: Event):
        """è™•ç†å–®å€‹äº‹ä»¶"""
        try:
            # ç²å–åŒ¹é…çš„è¨‚é–±è€…
            matched_subscribers = await self.router.route_event(
                event, self._subscriptions
            )

            # ä¸¦ç™¼åŸ·è¡Œæ‰€æœ‰åŒ¹é…çš„è™•ç†å™¨
            tasks = []
            for subscriber_id in matched_subscribers:
                subscription = self._subscriptions.get(subscriber_id)
                if subscription and subscription.enabled:
                    task = asyncio.create_task(
                        self._execute_handler(event, subscription)
                    )
                    tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰è™•ç†å™¨å®Œæˆ
            if tasks:
                await asyncio.gather(*tasks, return_exceptions=True)

        except Exception as e:
            logger.error(f"ã€äº‹ä»¶ç¸½ç·šã€‘è™•ç†äº‹ä»¶å¤±æ•— {event.event_id}: {e}")

    async def _execute_handler(self, event: Event, subscription: EventSubscription):
        """åŸ·è¡Œäº‹ä»¶è™•ç†å™¨"""
        retry_count = 0

        while retry_count <= subscription.max_retries:
            try:
                start_time = time.time()

                # åŸ·è¡Œè™•ç†å™¨
                await subscription.handler(event)

                processing_time = time.time() - start_time

                # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
                if subscription.subscriber_id:
                    await self.router.update_performance(
                        subscription.subscriber_id, processing_time
                    )
                await self.metrics.record_event_processed(event, processing_time, True)

                return  # æˆåŠŸ,é€€å‡ºé‡è©¦å¾ªç’°

            except Exception as e:
                retry_count += 1
                processing_time = time.time() - start_time

                await self.metrics.record_event_processed(event, processing_time, False)

                if retry_count <= subscription.max_retries:
                    logger.warning(
                        f"ã€äº‹ä»¶ç¸½ç·šã€‘è™•ç†å™¨åŸ·è¡Œå¤±æ•—,é‡è©¦ {retry_count}/{subscription.max_retries}: "
                        f"{subscription.subscriber_id} - {e}"
                    )
                    await asyncio.sleep(subscription.retry_delay * retry_count)
                else:
                    logger.error(
                        f"ã€äº‹ä»¶ç¸½ç·šã€‘è™•ç†å™¨åŸ·è¡Œå¤±æ•—,å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸: "
                        f"{subscription.subscriber_id} - {e}"
                    )

    async def get_event_history(
        self, event_type: str | None = None, limit: int = 100
    ) -> list[Event]:
        """ç²å–äº‹ä»¶æ­·å²"""
        return await self.persistence.load_events(event_type=event_type, limit=limit)

    async def replay_events(
        self,
        event_type: str | None = None,
        start_time: float | None = None,
        end_time: float | None = None,
    ) -> int:
        """é‡æ”¾äº‹ä»¶"""
        events = await self.persistence.load_events(
            event_type=event_type, start_time=start_time, end_time=end_time, limit=1000
        )

        replayed_count = 0
        for event in events:
            # å‰µå»ºé‡æ”¾äº‹ä»¶å‰¯æœ¬
            replay_event = Event(
                event_type=f"replay_{event.event_type}",
                data=event.data,
                source=event.source,
                target=event.target,
                priority=event.priority,
                correlation_id=event.correlation_id,
                metadata={**event.metadata, "original_event_id": event.event_id},
            )

            await self.publish(replay_event, persist=False)
            replayed_count += 1

        return replayed_count

    def get_metrics(self) -> dict[str, Any]:
        """ç²å–äº‹ä»¶ç¸½ç·šæŒ‡æ¨™"""
        return {
            "subscriptions_count": len(self._subscriptions),
            "queue_size": self._event_queue.qsize(),
            "batch_queues_count": len(self._batch_queues),
            "processing_workers": len(self._processing_tasks),
            "batch_workers": len(self._batch_tasks),
        }

    async def get_detailed_metrics(self) -> dict[str, Any]:
        """ç²å–è©³ç´°æŒ‡æ¨™"""
        basic_metrics = self.get_metrics()
        performance_metrics = await self.metrics.get_metrics_summary()

        return {
            **basic_metrics,
            **performance_metrics,
            "router_performance": self.router.performance_cache.copy(),
            "active_batches": {
                batch_key: {
                    "events_count": len(batch.events),
                    "created_at": batch.created_at,
                    "total_size": batch.get_total_size(),
                }
                for batch_key, batch in self._batch_queues.items()
            },
        }

    @asynccontextmanager
    async def event_scope(self, scope_name: str):
        """äº‹ä»¶ä½œç”¨åŸŸä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        scope_events = []

        async def scope_handler(event: Event):
            scope_events.append(event)

        # è¨‚é–±æ‰€æœ‰äº‹ä»¶
        subscription_id = self.subscribe(
            event_types=["*"],
            handler=scope_handler,
            subscriber_id=f"scope_{scope_name}_{int(time.time())}",
        )

        try:
            yield scope_events
        finally:
            self.unsubscribe(subscription_id)


# å…¨åŸŸäº‹ä»¶ç¸½ç·šå¯¦ä¾‹
_global_event_bus: EventBus | None = None
_bus_lock = asyncio.Lock()


async def get_global_event_bus() -> EventBus:
    """ç²å–å…¨åŸŸäº‹ä»¶ç¸½ç·š"""
    global _global_event_bus

    async with _bus_lock:
        if _global_event_bus is None:
            _global_event_bus = EventBus()
            await _global_event_bus.initialize()

    return _global_event_bus


async def dispose_global_event_bus():
    """é‡‹æ”¾å…¨åŸŸäº‹ä»¶ç¸½ç·š"""
    global _global_event_bus

    async with _bus_lock:
        if _global_event_bus is not None:
            await _global_event_bus.shutdown()
            _global_event_bus = None


# ä¾¿æ·å‡½æ•¸
async def publish_event(
    event_type: str,
    data: dict[str, Any | None] | None = None,
    source: str | None = None,
    target: str | None = None,
    priority: EventPriority = EventPriority.NORMAL,
    processing_mode: EventProcessingMode = EventProcessingMode.IMMEDIATE,
) -> bool:
    """ç™¼å¸ƒäº‹ä»¶çš„ä¾¿æ·å‡½æ•¸"""
    event = Event(
        event_type=event_type,
        data=data or {},
        source=source,
        target=target,
        priority=priority,
        processing_mode=processing_mode,
    )

    bus = await get_global_event_bus()
    return await bus.publish(event)


async def publish_batch_events(events_data: list[dict[str, Any]]) -> bool:
    """æ‰¹é‡ç™¼å¸ƒäº‹ä»¶çš„ä¾¿æ·å‡½æ•¸"""
    events = [Event.from_dict(event_data) for event_data in events_data]

    bus = await get_global_event_bus()
    return await bus.publish_batch(events)


def event_handler(
    event_types: list[str],
    filters: list[Callable[["Event | None"], bool]] | None = None,
    priority: EventPriority = EventPriority.NORMAL,
    processing_mode: EventProcessingMode = EventProcessingMode.IMMEDIATE,
):
    """äº‹ä»¶è™•ç†å™¨è£é£¾å™¨"""

    def decorator(func: EventHandler):
        async def wrapper():
            bus = await get_global_event_bus()
            return bus.subscribe(
                event_types=event_types,
                handler=func,
                filters=filters,
                priority=priority,
                processing_mode=processing_mode,
            )

        # è‡ªå‹•è¨»å†Šè™•ç†å™¨
        asyncio.create_task(wrapper())
        return func

    return decorator
