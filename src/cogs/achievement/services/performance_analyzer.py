"""æˆå°±ç³»çµ±æ•ˆèƒ½åˆ†æå™¨.

æ­¤æ¨¡çµ„æä¾›æˆå°±ç³»çµ±çš„æ•ˆèƒ½ç“¶é ¸åˆ†æ,åŒ…å«:
- æŸ¥è©¢æ•ˆèƒ½åˆ†æå’ŒåŸºæº–æ¸¬è©¦
- è³‡æ–™åº«ç´¢å¼•å„ªåŒ–å»ºè­°
- è¨˜æ†¶é«”ä½¿ç”¨é‡åˆ†æ
- ç“¶é ¸è­˜åˆ¥å’Œæ•ˆèƒ½å ±å‘Š

æ ¹æ“š Story 5.1 Task 1.1 çš„è¦æ±‚,åˆ†æç¾æœ‰æˆå°±æŸ¥è©¢çš„æ•ˆèƒ½ç“¶é ¸.
"""

from __future__ import annotations

import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import TYPE_CHECKING, Any

from ..database.models import AchievementType

if TYPE_CHECKING:
    from ..database.repository import AchievementRepository

logger = logging.getLogger(__name__)

# å¸¸æ•¸å®šç¾©
SLOW_QUERY_THRESHOLD_MS = 200  # æ…¢æŸ¥è©¢é–¾å€¼(æ¯«ç§’)
CRITICAL_QUERY_THRESHOLD_MS = 500  # é—œéµæ…¢æŸ¥è©¢é–¾å€¼(æ¯«ç§’)
HIGH_QUERY_THRESHOLD_MS = 300  # é«˜æ…¢æŸ¥è©¢é–¾å€¼(æ¯«ç§’)
HIGH_MEMORY_THRESHOLD_MB = 10  # é«˜è¨˜æ†¶é«”ä½¿ç”¨é–¾å€¼(MB)
MEMORY_WARNING_THRESHOLD_MB = 50  # è¨˜æ†¶é«”è­¦å‘Šé–¾å€¼(MB)
EFFICIENCY_THRESHOLD = 0.1  # æ•ˆç‡é–¾å€¼
EXCELLENT_RESPONSE_TIME_MS = 100  # å„ªç§€å›æ‡‰æ™‚é–“(æ¯«ç§’)
GOOD_RESPONSE_TIME_MS = 200  # è‰¯å¥½å›æ‡‰æ™‚é–“(æ¯«ç§’)
FAIR_RESPONSE_TIME_MS = 300  # ä¸€èˆ¬å›æ‡‰æ™‚é–“(æ¯«ç§’)
POOR_RESPONSE_TIME_MS = 500  # å·®å‹å›æ‡‰æ™‚é–“(æ¯«ç§’)
EXCELLENT_SLOW_QUERY_RATIO = 0.1  # å„ªç§€æ…¢æŸ¥è©¢æ¯”ä¾‹
GOOD_SLOW_QUERY_RATIO = 0.2  # è‰¯å¥½æ…¢æŸ¥è©¢æ¯”ä¾‹
FAIR_SLOW_QUERY_RATIO = 0.3  # ä¸€èˆ¬æ…¢æŸ¥è©¢æ¯”ä¾‹
POOR_SLOW_QUERY_RATIO = 0.5  # å·®å‹æ…¢æŸ¥è©¢æ¯”ä¾‹


class QueryType(str, Enum):
    """æŸ¥è©¢é¡å‹åˆ—èˆ‰."""

    ACHIEVEMENT_LIST = "achievement_list"
    USER_ACHIEVEMENTS = "user_achievements"
    USER_PROGRESS = "user_progress"
    STATS_QUERY = "stats_query"
    LEADERBOARD = "leaderboard"


@dataclass
class QueryPerformanceMetric:
    """æŸ¥è©¢æ•ˆèƒ½æŒ‡æ¨™."""

    query_type: QueryType
    """æŸ¥è©¢é¡å‹"""

    execution_time_ms: float
    """åŸ·è¡Œæ™‚é–“(æ¯«ç§’)"""

    rows_examined: int
    """æª¢æŸ¥çš„è¡Œæ•¸"""

    rows_returned: int
    """è¿”å›çš„è¡Œæ•¸"""

    memory_usage_mb: float
    """è¨˜æ†¶é«”ä½¿ç”¨é‡(MB)"""

    query_sql: str
    """æŸ¥è©¢ SQL"""

    parameters: dict[str, Any] = field(default_factory=dict)
    """æŸ¥è©¢åƒæ•¸"""

    timestamp: datetime = field(default_factory=datetime.now)
    """æ¸¬è©¦æ™‚é–“"""


@dataclass
class PerformanceBottleneck:
    """æ•ˆèƒ½ç“¶é ¸."""

    type: str
    """ç“¶é ¸é¡å‹"""

    severity: str
    """åš´é‡ç¨‹åº¦ (low, medium, high, critical)"""

    description: str
    """æè¿°"""

    affected_queries: list[QueryType]
    """å½±éŸ¿çš„æŸ¥è©¢é¡å‹"""

    recommendations: list[str]
    """å„ªåŒ–å»ºè­°"""

    performance_impact: float
    """æ•ˆèƒ½å½±éŸ¿(ç™¾åˆ†æ¯”)"""


@dataclass
class PerformanceBenchmark:
    """æ•ˆèƒ½åŸºæº–."""

    query_type: QueryType
    baseline_time_ms: float
    target_time_ms: float
    current_time_ms: float
    improvement_percentage: float


class PerformanceAnalyzer:
    """æ•ˆèƒ½åˆ†æå™¨.

    æä¾›æˆå°±ç³»çµ±çš„å…¨é¢æ•ˆèƒ½åˆ†æåŠŸèƒ½.
    """

    def __init__(self, repository: AchievementRepository):
        """åˆå§‹åŒ–æ•ˆèƒ½åˆ†æå™¨.

        Args:
            repository: æˆå°±è³‡æ–™å­˜å–åº«
        """
        self._repository = repository
        self._metrics_history: list[QueryPerformanceMetric] = []
        self._benchmarks: dict[QueryType, PerformanceBenchmark] = {}

        # Performance targets in milliseconds
        self._performance_targets = {
            QueryType.ACHIEVEMENT_LIST: 200,
            QueryType.USER_ACHIEVEMENTS: 150,
            QueryType.USER_PROGRESS: 100,
            QueryType.STATS_QUERY: 300,
            QueryType.LEADERBOARD: 400,
        }

        logger.info("PerformanceAnalyzer åˆå§‹åŒ–å®Œæˆ")

    async def analyze_all_queries(self) -> dict[str, Any]:
        """åˆ†ææ‰€æœ‰æŸ¥è©¢é¡å‹çš„æ•ˆèƒ½.

        Returns:
            å®Œæ•´çš„æ•ˆèƒ½åˆ†æå ±å‘Š
        """
        logger.info("é–‹å§‹å…¨é¢æ•ˆèƒ½åˆ†æ")

        analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "query_performance": {},
            "bottlenecks": [],
            "recommendations": [],
            "benchmark_results": {},
            "overall_health": "unknown",
        }

        try:
            # 1. åˆ†ææˆå°±åˆ—è¡¨æŸ¥è©¢
            achievement_metrics = await self._analyze_achievement_list_queries()
            analysis_results["query_performance"]["achievement_list"] = (
                achievement_metrics
            )

            # 2. åˆ†æç”¨æˆ¶æˆå°±æŸ¥è©¢
            user_achievement_metrics = await self._analyze_user_achievement_queries()
            analysis_results["query_performance"]["user_achievements"] = (
                user_achievement_metrics
            )

            # 3. åˆ†æç”¨æˆ¶é€²åº¦æŸ¥è©¢
            progress_metrics = await self._analyze_progress_queries()
            analysis_results["query_performance"]["user_progress"] = progress_metrics

            # 4. åˆ†æçµ±è¨ˆæŸ¥è©¢
            stats_metrics = await self._analyze_stats_queries()
            analysis_results["query_performance"]["stats"] = stats_metrics

            # 5. è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸
            bottlenecks = await self._identify_bottlenecks()
            analysis_results["bottlenecks"] = [
                self._bottleneck_to_dict(b) for b in bottlenecks
            ]

            # 6. ç”Ÿæˆå„ªåŒ–å»ºè­°
            recommendations = await self._generate_recommendations(bottlenecks)
            analysis_results["recommendations"] = recommendations

            # 7. è¨ˆç®—æ•´é«”å¥åº·åº¦
            overall_health = self._calculate_overall_health()
            analysis_results["overall_health"] = overall_health

            logger.info(f"æ•ˆèƒ½åˆ†æå®Œæˆ,æ•´é«”å¥åº·åº¦: {overall_health}")

        except Exception as e:
            logger.error(f"æ•ˆèƒ½åˆ†æå¤±æ•—: {e}", exc_info=True)
            analysis_results["error"] = str(e)

        return analysis_results

    async def _analyze_achievement_list_queries(self) -> dict[str, Any]:
        """åˆ†ææˆå°±åˆ—è¡¨æŸ¥è©¢æ•ˆèƒ½."""
        logger.debug("åˆ†ææˆå°±åˆ—è¡¨æŸ¥è©¢æ•ˆèƒ½")

        metrics = []

        # æ¸¬è©¦å ´æ™¯ 1: åŸºæœ¬æˆå°±åˆ—è¡¨æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            achievements = await self._repository.list_achievements(
                active_only=True, limit=50
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.ACHIEVEMENT_LIST,
                execution_time_ms=execution_time,
                rows_examined=len(achievements) * 2,  # ä¼°ç®—
                rows_returned=len(achievements),
                memory_usage_mb=len(achievements) * 0.005,  # ä¼°ç®—æ¯å€‹æˆå°± 5KB
                query_sql="SELECT * FROM achievements WHERE is_active = 1 LIMIT 50",
                parameters={"active_only": True, "limit": 50},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"åŸºæœ¬æˆå°±åˆ—è¡¨æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 2: åˆ†é¡ç¯©é¸æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            achievements = await self._repository.list_achievements(
                category_id=1, active_only=True, limit=20
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.ACHIEVEMENT_LIST,
                execution_time_ms=execution_time,
                rows_examined=len(achievements) * 3,  # ä¼°ç®—,éœ€è¦JOIN
                rows_returned=len(achievements),
                memory_usage_mb=len(achievements) * 0.005,
                query_sql="SELECT * FROM achievements WHERE category_id = 1 AND is_active = 1 LIMIT 20",
                parameters={"category_id": 1, "active_only": True, "limit": 20},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"åˆ†é¡ç¯©é¸æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 3: é¡å‹ç¯©é¸æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            achievements = await self._repository.list_achievements(
                achievement_type=AchievementType.PROGRESSIVE, active_only=True
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.ACHIEVEMENT_LIST,
                execution_time_ms=execution_time,
                rows_examined=len(achievements) * 2,
                rows_returned=len(achievements),
                memory_usage_mb=len(achievements) * 0.005,
                query_sql="SELECT * FROM achievements WHERE type = 'progressive' AND is_active = 1",
                parameters={"achievement_type": "progressive", "active_only": True},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"é¡å‹ç¯©é¸æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # è¨ˆç®—çµ±è¨ˆ
        if metrics:
            avg_time = sum(m.execution_time_ms for m in metrics) / len(metrics)
            max_time = max(m.execution_time_ms for m in metrics)
            min_time = min(m.execution_time_ms for m in metrics)
            total_memory = sum(m.memory_usage_mb for m in metrics)
        else:
            avg_time = max_time = min_time = total_memory = 0

        return {
            "test_count": len(metrics),
            "average_time_ms": round(avg_time, 2),
            "max_time_ms": round(max_time, 2),
            "min_time_ms": round(min_time, 2),
            "total_memory_mb": round(total_memory, 2),
            "target_time_ms": self._performance_targets[QueryType.ACHIEVEMENT_LIST],
            "meets_target": avg_time
            <= self._performance_targets[QueryType.ACHIEVEMENT_LIST],
            "details": [self._metric_to_dict(m) for m in metrics],
        }

    async def _analyze_user_achievement_queries(self) -> dict[str, Any]:
        """åˆ†æç”¨æˆ¶æˆå°±æŸ¥è©¢æ•ˆèƒ½."""
        logger.debug("åˆ†æç”¨æˆ¶æˆå°±æŸ¥è©¢æ•ˆèƒ½")

        metrics = []
        test_user_id = 123456  # æ¸¬è©¦ç”¨æˆ¶ ID

        # æ¸¬è©¦å ´æ™¯ 1: åŸºæœ¬ç”¨æˆ¶æˆå°±æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            user_achievements = await self._repository.get_user_achievements(
                test_user_id
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.USER_ACHIEVEMENTS,
                execution_time_ms=execution_time,
                rows_examined=len(user_achievements) * 3,  # éœ€è¦JOIN achievementsè¡¨
                rows_returned=len(user_achievements),
                memory_usage_mb=len(user_achievements) * 0.008,  # åŒ…å«æˆå°±è©³æƒ…
                query_sql="""SELECT ua.*, a.* FROM user_achievements ua
                           JOIN achievements a ON a.id = ua.achievement_id
                           WHERE ua.user_id = ?""",
                parameters={"user_id": test_user_id},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"åŸºæœ¬ç”¨æˆ¶æˆå°±æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 2: åˆ†é¡ç”¨æˆ¶æˆå°±æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            user_achievements = await self._repository.get_user_achievements(
                test_user_id, category_id=1
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.USER_ACHIEVEMENTS,
                execution_time_ms=execution_time,
                rows_examined=len(user_achievements) * 4,  # è¤‡é›œJOIN
                rows_returned=len(user_achievements),
                memory_usage_mb=len(user_achievements) * 0.008,
                query_sql="""SELECT ua.*, a.* FROM user_achievements ua
                           JOIN achievements a ON a.id = ua.achievement_id
                           WHERE ua.user_id = ? AND a.category_id = ?""",
                parameters={"user_id": test_user_id, "category_id": 1},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"åˆ†é¡ç”¨æˆ¶æˆå°±æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 3: ç”¨æˆ¶æˆå°±çµ±è¨ˆæŸ¥è©¢
        start_time = time.perf_counter()
        try:
            stats = await self._repository.get_user_achievement_stats(test_user_id)
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.STATS_QUERY,
                execution_time_ms=execution_time,
                rows_examined=stats.get("total_achievements", 0) * 3,
                rows_returned=1,  # çµ±è¨ˆçµæœ
                memory_usage_mb=0.001,  # çµ±è¨ˆçµæœå¾ˆå°
                query_sql="""SELECT COUNT(*), SUM(a.points) FROM user_achievements ua
                           JOIN achievements a ON a.id = ua.achievement_id
                           WHERE ua.user_id = ?""",
                parameters={"user_id": test_user_id},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"ç”¨æˆ¶æˆå°±çµ±è¨ˆæŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # è¨ˆç®—çµ±è¨ˆ
        if metrics:
            avg_time = sum(m.execution_time_ms for m in metrics) / len(metrics)
            max_time = max(m.execution_time_ms for m in metrics)
            min_time = min(m.execution_time_ms for m in metrics)
            total_memory = sum(m.memory_usage_mb for m in metrics)
        else:
            avg_time = max_time = min_time = total_memory = 0

        return {
            "test_count": len(metrics),
            "average_time_ms": round(avg_time, 2),
            "max_time_ms": round(max_time, 2),
            "min_time_ms": round(min_time, 2),
            "total_memory_mb": round(total_memory, 2),
            "target_time_ms": self._performance_targets[QueryType.USER_ACHIEVEMENTS],
            "meets_target": avg_time
            <= self._performance_targets[QueryType.USER_ACHIEVEMENTS],
            "details": [self._metric_to_dict(m) for m in metrics],
        }

    async def _analyze_progress_queries(self) -> dict[str, Any]:
        """åˆ†æç”¨æˆ¶é€²åº¦æŸ¥è©¢æ•ˆèƒ½."""
        logger.debug("åˆ†æç”¨æˆ¶é€²åº¦æŸ¥è©¢æ•ˆèƒ½")

        metrics = []
        test_user_id = 123456
        test_achievement_id = 1

        # æ¸¬è©¦å ´æ™¯ 1: å–®å€‹é€²åº¦æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            progress = await self._repository.get_user_progress(
                test_user_id, test_achievement_id
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.USER_PROGRESS,
                execution_time_ms=execution_time,
                rows_examined=1,
                rows_returned=1 if progress else 0,
                memory_usage_mb=0.001,
                query_sql="""SELECT * FROM achievement_progress
                           WHERE user_id = ? AND achievement_id = ?""",
                parameters={
                    "user_id": test_user_id,
                    "achievement_id": test_achievement_id,
                },
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"å–®å€‹é€²åº¦æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 2: ç”¨æˆ¶æ‰€æœ‰é€²åº¦æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            progresses = await self._repository.get_user_progresses(test_user_id)
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.USER_PROGRESS,
                execution_time_ms=execution_time,
                rows_examined=len(progresses) * 2,
                rows_returned=len(progresses),
                memory_usage_mb=len(progresses) * 0.003,
                query_sql="""SELECT * FROM achievement_progress WHERE user_id = ?
                           ORDER BY last_updated DESC""",
                parameters={"user_id": test_user_id},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"ç”¨æˆ¶æ‰€æœ‰é€²åº¦æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # è¨ˆç®—çµ±è¨ˆ
        if metrics:
            avg_time = sum(m.execution_time_ms for m in metrics) / len(metrics)
            max_time = max(m.execution_time_ms for m in metrics)
            min_time = min(m.execution_time_ms for m in metrics)
            total_memory = sum(m.memory_usage_mb for m in metrics)
        else:
            avg_time = max_time = min_time = total_memory = 0

        return {
            "test_count": len(metrics),
            "average_time_ms": round(avg_time, 2),
            "max_time_ms": round(max_time, 2),
            "min_time_ms": round(min_time, 2),
            "total_memory_mb": round(total_memory, 2),
            "target_time_ms": self._performance_targets[QueryType.USER_PROGRESS],
            "meets_target": avg_time
            <= self._performance_targets[QueryType.USER_PROGRESS],
            "details": [self._metric_to_dict(m) for m in metrics],
        }

    async def _analyze_stats_queries(self) -> dict[str, Any]:
        """åˆ†æçµ±è¨ˆæŸ¥è©¢æ•ˆèƒ½."""
        logger.debug("åˆ†æçµ±è¨ˆæŸ¥è©¢æ•ˆèƒ½")

        metrics = []

        # æ¸¬è©¦å ´æ™¯ 1: å…¨åŸŸçµ±è¨ˆæŸ¥è©¢
        start_time = time.perf_counter()
        try:
            global_stats = await self._repository.get_global_achievement_stats()
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.STATS_QUERY,
                execution_time_ms=execution_time,
                rows_examined=global_stats.get("total_achievements", 0),
                rows_returned=4,  # çµ±è¨ˆçµæœé …ç›®æ•¸
                memory_usage_mb=0.001,
                query_sql="""SELECT COUNT(*) FROM achievements;
                           SELECT COUNT(*) FROM achievements WHERE is_active = 1;
                           SELECT COUNT(*) FROM user_achievements;
                           SELECT COUNT(DISTINCT user_id) FROM user_achievements""",
                parameters={},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"å…¨åŸŸçµ±è¨ˆæŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦å ´æ™¯ 2: ç†±é–€æˆå°±æŸ¥è©¢
        start_time = time.perf_counter()
        try:
            popular_achievements = await self._repository.get_popular_achievements(
                limit=10
            )
            execution_time = (time.perf_counter() - start_time) * 1000

            metric = QueryPerformanceMetric(
                query_type=QueryType.LEADERBOARD,
                execution_time_ms=execution_time,
                rows_examined=len(popular_achievements) * 10,  # è¤‡é›œèšåˆæŸ¥è©¢
                rows_returned=len(popular_achievements),
                memory_usage_mb=len(popular_achievements) * 0.005,
                query_sql="""SELECT a.*, COUNT(ua.id) as earned_count
                           FROM achievements a
                           LEFT JOIN user_achievements ua ON ua.achievement_id = a.id
                           WHERE a.is_active = 1
                           GROUP BY a.id
                           ORDER BY earned_count DESC LIMIT 10""",
                parameters={"limit": 10},
            )
            metrics.append(metric)
            self._metrics_history.append(metric)

        except Exception as e:
            logger.error(f"ç†±é–€æˆå°±æŸ¥è©¢æ¸¬è©¦å¤±æ•—: {e}")

        # è¨ˆç®—çµ±è¨ˆ
        if metrics:
            avg_time = sum(m.execution_time_ms for m in metrics) / len(metrics)
            max_time = max(m.execution_time_ms for m in metrics)
            min_time = min(m.execution_time_ms for m in metrics)
            total_memory = sum(m.memory_usage_mb for m in metrics)
        else:
            avg_time = max_time = min_time = total_memory = 0

        return {
            "test_count": len(metrics),
            "average_time_ms": round(avg_time, 2),
            "max_time_ms": round(max_time, 2),
            "min_time_ms": round(min_time, 2),
            "total_memory_mb": round(total_memory, 2),
            "target_time_ms": self._performance_targets[QueryType.STATS_QUERY],
            "meets_target": avg_time
            <= self._performance_targets[QueryType.STATS_QUERY],
            "details": [self._metric_to_dict(m) for m in metrics],
        }

    async def _identify_bottlenecks(self) -> list[PerformanceBottleneck]:
        """è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸."""
        logger.debug("è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸")

        bottlenecks = []

        if not self._metrics_history:
            return bottlenecks

        # åˆ†ææŸ¥è©¢æ™‚é–“ç“¶é ¸
        slow_queries = [
            m
            for m in self._metrics_history
            if m.execution_time_ms > SLOW_QUERY_THRESHOLD_MS
        ]
        if slow_queries:
            affected_types = list({q.query_type for q in slow_queries})
            avg_slow_time = sum(q.execution_time_ms for q in slow_queries) / len(
                slow_queries
            )

            severity = (
                "critical"
                if avg_slow_time > CRITICAL_QUERY_THRESHOLD_MS
                else "high"
                if avg_slow_time > HIGH_QUERY_THRESHOLD_MS
                else "medium"
            )

            bottleneck = PerformanceBottleneck(
                type="slow_queries",
                severity=severity,
                description=f"ç™¼ç¾ {len(slow_queries)} å€‹æ…¢æŸ¥è©¢,å¹³å‡åŸ·è¡Œæ™‚é–“ {avg_slow_time:.1f}ms",
                affected_queries=affected_types,
                recommendations=[
                    "å„ªåŒ–è³‡æ–™åº«ç´¢å¼•",
                    "å¯¦æ–½æŸ¥è©¢å¿«å–",
                    "è€ƒæ…®åˆ†é æŸ¥è©¢",
                    "å„ªåŒ–JOINæ“ä½œ",
                ],
                performance_impact=min(avg_slow_time / 100, 90),  # æœ€é«˜90%å½±éŸ¿
            )
            bottlenecks.append(bottleneck)

        # åˆ†æè¨˜æ†¶é«”ä½¿ç”¨ç“¶é ¸
        high_memory_queries = [
            m
            for m in self._metrics_history
            if m.memory_usage_mb > HIGH_MEMORY_THRESHOLD_MB
        ]
        if high_memory_queries:
            total_memory = sum(q.memory_usage_mb for q in high_memory_queries)

            bottleneck = PerformanceBottleneck(
                type="high_memory_usage",
                severity="medium"
                if total_memory < MEMORY_WARNING_THRESHOLD_MB
                else "high",
                description=f"é«˜è¨˜æ†¶é«”ä½¿ç”¨æŸ¥è©¢ç¸½è¨ˆ {total_memory:.1f}MB",
                affected_queries=list({q.query_type for q in high_memory_queries}),
                recommendations=[
                    "å¯¦æ–½çµæœé›†é™åˆ¶",
                    "ä½¿ç”¨åˆ†é æŸ¥è©¢",
                    "å„ªåŒ–è³‡æ–™çµæ§‹",
                    "å¢åŠ è¨˜æ†¶é«”å¿«å–",
                ],
                performance_impact=min(total_memory * 2, 70),
            )
            bottlenecks.append(bottleneck)

        # åˆ†æè³‡æ–™åº«æƒæç“¶é ¸
        high_scan_queries = [
            m for m in self._metrics_history if m.rows_examined > m.rows_returned * 10
        ]
        if high_scan_queries:
            avg_efficiency = sum(
                q.rows_returned / max(q.rows_examined, 1) for q in high_scan_queries
            ) / len(high_scan_queries)

            bottleneck = PerformanceBottleneck(
                type="inefficient_scans",
                severity="high" if avg_efficiency < EFFICIENCY_THRESHOLD else "medium",
                description=f"ä½æ•ˆæƒææŸ¥è©¢,å¹³å‡æ•ˆç‡ {avg_efficiency:.1%}",
                affected_queries=list({q.query_type for q in high_scan_queries}),
                recommendations=[
                    "æ·»åŠ é©ç•¶çš„ç´¢å¼•",
                    "å„ªåŒ–WHEREæ¢ä»¶",
                    "é¿å…SELECT *",
                    "ä½¿ç”¨è¦†è“‹ç´¢å¼•",
                ],
                performance_impact=(1 - avg_efficiency) * 80,
            )
            bottlenecks.append(bottleneck)

        return bottlenecks

    async def _generate_recommendations(
        self, bottlenecks: list[PerformanceBottleneck]
    ) -> list[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°."""
        recommendations = []

        if not bottlenecks:
            recommendations.append("ç³»çµ±æ•ˆèƒ½è‰¯å¥½,ç„¡æ˜é¡¯ç“¶é ¸")
            return recommendations

        # æŒ‰åš´é‡ç¨‹åº¦æ’åºç“¶é ¸
        critical_bottlenecks = [b for b in bottlenecks if b.severity == "critical"]
        high_bottlenecks = [b for b in bottlenecks if b.severity == "high"]
        medium_bottlenecks = [b for b in bottlenecks if b.severity == "medium"]

        if critical_bottlenecks:
            recommendations.append("ğŸš¨ ç·Šæ€¥å„ªåŒ–å»ºè­°:")
            for bottleneck in critical_bottlenecks:
                recommendations.extend([
                    f"  - {rec}" for rec in bottleneck.recommendations
                ])

        if high_bottlenecks:
            recommendations.append("âš ï¸ é«˜å„ªå…ˆç´šå„ªåŒ–å»ºè­°:")
            for bottleneck in high_bottlenecks:
                recommendations.extend([
                    f"  - {rec}" for rec in bottleneck.recommendations
                ])

        if medium_bottlenecks:
            recommendations.append("ä¸­å„ªå…ˆç´šå„ªåŒ–å»ºè­°:")
            for bottleneck in medium_bottlenecks:
                recommendations.extend([
                    f"  - {rec}" for rec in bottleneck.recommendations
                ])

        # æ·»åŠ é€šç”¨å»ºè­°
        recommendations.extend([
            "",
            "ğŸ”§ é€šç”¨å„ªåŒ–å»ºè­°:",
            "  - å®šæœŸæ›´æ–°è³‡æ–™åº«çµ±è¨ˆè³‡è¨Š",
            "  - ç›£æ§æŸ¥è©¢åŸ·è¡Œè¨ˆç•«",
            "  - å¯¦æ–½é©ç•¶çš„å¿«å–ç­–ç•¥",
            "  - è€ƒæ…®è®€å¯«åˆ†é›¢æ¶æ§‹",
            "  - å®šæœŸæ¸…ç†æ­·å²è³‡æ–™",
        ])

        return recommendations

    def _calculate_overall_health(self) -> str:
        """è¨ˆç®—æ•´é«”å¥åº·åº¦."""
        if not self._metrics_history:
            return "unknown"

        # è¨ˆç®—å„é …æŒ‡æ¨™
        avg_time = sum(m.execution_time_ms for m in self._metrics_history) / len(
            self._metrics_history
        )
        slow_query_ratio = len([
            m
            for m in self._metrics_history
            if m.execution_time_ms > SLOW_QUERY_THRESHOLD_MS
        ]) / len(self._metrics_history)

        # æ ¹æ“šæŒ‡æ¨™è©•ä¼°å¥åº·åº¦
        if (
            avg_time <= EXCELLENT_RESPONSE_TIME_MS
            and slow_query_ratio <= EXCELLENT_SLOW_QUERY_RATIO
        ):
            return "excellent"
        elif (
            avg_time <= GOOD_RESPONSE_TIME_MS
            and slow_query_ratio <= GOOD_SLOW_QUERY_RATIO
        ):
            return "good"
        elif (
            avg_time <= FAIR_RESPONSE_TIME_MS
            and slow_query_ratio <= FAIR_SLOW_QUERY_RATIO
        ):
            return "fair"
        elif (
            avg_time <= POOR_RESPONSE_TIME_MS
            and slow_query_ratio <= POOR_SLOW_QUERY_RATIO
        ):
            return "poor"
        else:
            return "critical"

    def _metric_to_dict(self, metric: QueryPerformanceMetric) -> dict[str, Any]:
        """å°‡æ•ˆèƒ½æŒ‡æ¨™è½‰æ›ç‚ºå­—å…¸."""
        return {
            "query_type": metric.query_type.value,
            "execution_time_ms": round(metric.execution_time_ms, 2),
            "rows_examined": metric.rows_examined,
            "rows_returned": metric.rows_returned,
            "memory_usage_mb": round(metric.memory_usage_mb, 3),
            "efficiency": round(metric.rows_returned / max(metric.rows_examined, 1), 3),
            "query_sql": metric.query_sql,
            "parameters": metric.parameters,
            "timestamp": metric.timestamp.isoformat(),
        }

    def _bottleneck_to_dict(self, bottleneck: PerformanceBottleneck) -> dict[str, Any]:
        """å°‡æ•ˆèƒ½ç“¶é ¸è½‰æ›ç‚ºå­—å…¸."""
        return {
            "type": bottleneck.type,
            "severity": bottleneck.severity,
            "description": bottleneck.description,
            "affected_queries": [q.value for q in bottleneck.affected_queries],
            "recommendations": bottleneck.recommendations,
            "performance_impact": round(bottleneck.performance_impact, 1),
        }

    def get_metrics_history(self) -> list[dict[str, Any]]:
        """å–å¾—æ•ˆèƒ½æŒ‡æ¨™æ­·å²."""
        return [self._metric_to_dict(m) for m in self._metrics_history]

    def clear_metrics_history(self) -> None:
        """æ¸…ç©ºæ•ˆèƒ½æŒ‡æ¨™æ­·å²."""
        self._metrics_history.clear()
        logger.info("æ•ˆèƒ½æŒ‡æ¨™æ­·å²å·²æ¸…ç©º")


__all__ = [
    "PerformanceAnalyzer",
    "PerformanceBenchmark",
    "PerformanceBottleneck",
    "QueryPerformanceMetric",
    "QueryType",
]
