# ROAS Bot v2.4.3 è³‡æ–™æµæ¶æ§‹è¨­è¨ˆ
**Task ID: 1** - Dockerå•Ÿå‹•ç³»çµ±ä¿®å¾©

## ğŸ—ï¸ æ•´é«”è³‡æ–™æµæ¶æ§‹

### åˆ†å±¤è³‡æ–™æµè¨­è¨ˆ

```mermaid
graph TD
    A[ä½¿ç”¨è€…æ“ä½œ] --> B[éƒ¨ç½²ç®¡ç†å™¨]
    B --> C[ç’°å¢ƒæª¢æŸ¥å™¨]
    B --> D[Docker Compose ç·¨æ’]
    D --> E[å®¹å™¨å¯¦ä¾‹]
    E --> F[ç›£æ§æ”¶é›†å™¨]
    F --> G[ç›£æ§è³‡æ–™åº«]
    B --> H[éŒ¯èª¤è™•ç†å™¨]
    H --> I[éŒ¯èª¤è³‡æ–™åº«]
    
    subgraph "è³‡æ–™å­˜å„²å±¤"
        G[monitoring.db]
        I[errors.db]
        J[deployment_logs.db]
        K[service_health.db]
    end
    
    subgraph "å¤–éƒ¨æ•´åˆ"
        L[Redis Cache]
        M[Prometheus]
        N[Grafana]
    end
    
    F --> L
    F --> M
    M --> N
```

## ğŸ“Š è³‡æ–™åº«æ¶æ§‹è¨­è¨ˆ

### 1. éƒ¨ç½²æ—¥èªŒè³‡æ–™åº« (deployment_logs.db)

```sql
-- éƒ¨ç½²æ“ä½œè¨˜éŒ„
CREATE TABLE deployment_operations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    deployment_id TEXT NOT NULL UNIQUE,
    operation_type TEXT NOT NULL, -- start, stop, restart, health_check
    environment TEXT NOT NULL,    -- dev, prod
    status TEXT NOT NULL,         -- pending, running, success, failed
    start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    end_time DATETIME,
    duration_ms INTEGER,
    user_agent TEXT,
    metadata TEXT, -- JSON format
    INDEX idx_deployment_id (deployment_id),
    INDEX idx_operation_type (operation_type),
    INDEX idx_status (status),
    INDEX idx_start_time (start_time)
);

-- æœå‹™ç‹€æ…‹è®Šæ›´è¨˜éŒ„
CREATE TABLE service_state_changes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    deployment_id TEXT NOT NULL,
    service_name TEXT NOT NULL,
    previous_state TEXT,
    new_state TEXT NOT NULL,
    change_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    change_reason TEXT,
    metadata TEXT, -- JSON format
    FOREIGN KEY (deployment_id) REFERENCES deployment_operations(deployment_id),
    INDEX idx_service_name (service_name),
    INDEX idx_change_time (change_time)
);

-- ç’°å¢ƒé©—è­‰è¨˜éŒ„
CREATE TABLE environment_validations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    deployment_id TEXT NOT NULL,
    validation_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    overall_result BOOLEAN NOT NULL,
    failed_checks TEXT, -- JSON array
    validation_details TEXT, -- JSON format
    recommendations TEXT, -- JSON array
    FOREIGN KEY (deployment_id) REFERENCES deployment_operations(deployment_id)
);
```

### 2. æ“´å±•ç¾æœ‰ç›£æ§è³‡æ–™åº« (monitoring.db)

```sql
-- æ–°å¢ï¼šéƒ¨ç½²å¥åº·ç‹€æ…‹æ­·å²
CREATE TABLE deployment_health_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    deployment_id TEXT,
    overall_health TEXT NOT NULL, -- healthy, degraded, unhealthy
    services_total INTEGER,
    services_healthy INTEGER,
    services_degraded INTEGER,
    services_unhealthy INTEGER,
    system_load_avg REAL,
    critical_alerts_count INTEGER,
    INDEX idx_timestamp (timestamp),
    INDEX idx_deployment_id (deployment_id)
);

-- æ–°å¢ï¼šæœå‹™ä¾è³´é—œä¿‚
CREATE TABLE service_dependencies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    service_name TEXT NOT NULL,
    depends_on_service TEXT NOT NULL,
    dependency_type TEXT NOT NULL, -- required, optional, performance
    health_impact TEXT NOT NULL,   -- critical, moderate, low
    last_check DATETIME DEFAULT CURRENT_TIMESTAMP,
    status TEXT DEFAULT 'active',  -- active, inactive, failed
    UNIQUE(service_name, depends_on_service)
);

-- æ–°å¢ï¼šæ•ˆèƒ½åŸºæº–ç·š
CREATE TABLE performance_baselines (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    service_name TEXT NOT NULL,
    metric_name TEXT NOT NULL,     -- response_time, cpu_usage, memory_usage
    baseline_value REAL NOT NULL,
    threshold_warning REAL,
    threshold_critical REAL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(service_name, metric_name)
);
```

### 3. æ“´å±•éŒ¯èª¤è™•ç†è³‡æ–™åº« (errors.db)

```sql
-- æ–°å¢ï¼šéŒ¯èª¤æ¨¡å¼åˆ†æ
CREATE TABLE error_patterns (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    pattern_name TEXT NOT NULL UNIQUE,
    pattern_regex TEXT NOT NULL,
    category TEXT NOT NULL,
    severity TEXT NOT NULL,
    auto_recovery_possible BOOLEAN DEFAULT FALSE,
    recovery_script_path TEXT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    last_matched DATETIME,
    match_count INTEGER DEFAULT 0
);

-- æ–°å¢ï¼šéŒ¯èª¤å½±éŸ¿åˆ†æ
CREATE TABLE error_impact_analysis (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    error_id TEXT NOT NULL,
    impact_type TEXT NOT NULL,      -- service_down, performance_degraded, data_loss
    affected_services TEXT,         -- JSON array
    estimated_recovery_time INTEGER, -- seconds
    business_impact TEXT,           -- low, medium, high, critical
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (error_id) REFERENCES deployment_errors(error_id)
);

-- æ–°å¢ï¼šè‡ªå‹•æ¢å¾©åŸ·è¡Œè¨˜éŒ„
CREATE TABLE auto_recovery_executions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    error_id TEXT NOT NULL,
    recovery_strategy TEXT NOT NULL,
    execution_start DATETIME DEFAULT CURRENT_TIMESTAMP,
    execution_end DATETIME,
    success BOOLEAN,
    steps_executed TEXT,            -- JSON array
    final_status TEXT,
    manual_intervention_required BOOLEAN DEFAULT FALSE,
    FOREIGN KEY (error_id) REFERENCES deployment_errors(error_id)
);
```

## ğŸ”„ æ ¸å¿ƒè³‡æ–™æµç¨‹

### 1. éƒ¨ç½²å•Ÿå‹•æµç¨‹

```python
async def deployment_startup_flow(environment: str) -> DeploymentResult:
    """éƒ¨ç½²å•Ÿå‹•è³‡æ–™æµ"""
    deployment_id = generate_deployment_id()
    
    try:
        # 1. è¨˜éŒ„éƒ¨ç½²é–‹å§‹
        await log_deployment_start(deployment_id, environment)
        
        # 2. ç’°å¢ƒæª¢æŸ¥
        env_result = await environment_validator.validate_environment()
        await log_environment_validation(deployment_id, env_result)
        
        if not env_result.passed:
            await log_deployment_failure(deployment_id, "Environment validation failed")
            return DeploymentResult(success=False, errors=env_result.errors)
        
        # 3. å•Ÿå‹•æœå‹™
        services_result = await deployment_manager.start_services()
        await log_service_state_changes(deployment_id, services_result.state_changes)
        
        # 4. å¥åº·æª¢æŸ¥
        health_result = await monitoring_collector.comprehensive_health_check()
        await log_deployment_health(deployment_id, health_result)
        
        # 5. è¨˜éŒ„éƒ¨ç½²æˆåŠŸ
        await log_deployment_success(deployment_id, services_result.duration)
        
        return DeploymentResult(
            success=True,
            deployment_id=deployment_id,
            services=services_result.services,
            health_status=health_result.overall_status
        )
        
    except Exception as e:
        # éŒ¯èª¤è™•ç†æµç¨‹
        await error_handler.handle_error(e, {"deployment_id": deployment_id})
        await log_deployment_failure(deployment_id, str(e))
        raise
```

### 2. ç›£æ§æ•¸æ“šæ”¶é›†æµç¨‹

```python
async def monitoring_collection_flow() -> MonitoringReport:
    """ç›£æ§æ•¸æ“šæ”¶é›†æµç¨‹"""
    
    # ä¸¦è¡Œæ”¶é›†å„é¡æŒ‡æ¨™
    tasks = [
        collect_system_metrics(),
        collect_service_metrics(),
        collect_dependency_status(),
        collect_performance_baselines()
    ]
    
    system_metrics, service_metrics, dependencies, baselines = await asyncio.gather(*tasks)
    
    # åˆ†æå¥åº·ç‹€æ…‹
    health_analysis = analyze_overall_health(system_metrics, service_metrics, dependencies)
    
    # ç”Ÿæˆå‘Šè­¦
    alerts = generate_smart_alerts(service_metrics, baselines)
    
    # æ›´æ–°æ•ˆèƒ½åŸºæº–ç·š
    await update_performance_baselines(service_metrics)
    
    # å­˜å„²ç›£æ§æ•¸æ“š
    await store_monitoring_data(system_metrics, service_metrics, health_analysis)
    
    return MonitoringReport(
        timestamp=datetime.now(),
        overall_status=health_analysis.status,
        system_metrics=system_metrics,
        service_metrics=service_metrics,
        alerts=alerts,
        recommendations=generate_recommendations(health_analysis)
    )
```

### 3. éŒ¯èª¤è™•ç†èˆ‡æ¢å¾©æµç¨‹

```python
async def error_handling_flow(error: Exception, context: Dict[str, Any]) -> RecoveryResult:
    """éŒ¯èª¤è™•ç†èˆ‡æ¢å¾©æµç¨‹"""
    
    # 1. éŒ¯èª¤åˆ†é¡å’Œæ¨¡å¼åŒ¹é…
    error_pattern = await match_error_pattern(error)
    classification = classify_error(error, context, error_pattern)
    
    # 2. å½±éŸ¿åˆ†æ
    impact_analysis = await analyze_error_impact(error, context)
    
    # 3. è‡ªå‹•æ¢å¾©è©•ä¼°
    recovery_possible = assess_auto_recovery_possibility(classification, impact_analysis)
    
    if recovery_possible:
        # 4. åŸ·è¡Œè‡ªå‹•æ¢å¾©
        recovery_result = await execute_auto_recovery(error, classification)
        await log_auto_recovery_execution(error.error_id, recovery_result)
        
        if recovery_result.success:
            await mark_error_resolved(error.error_id)
            return RecoveryResult(success=True, method="automatic")
    
    # 5. æ‰‹å‹•å¹²é å»ºè­°
    manual_steps = generate_manual_recovery_steps(classification, impact_analysis)
    await create_manual_intervention_ticket(error.error_id, manual_steps)
    
    return RecoveryResult(
        success=False, 
        method="manual_required",
        steps=manual_steps
    )
```

## ğŸ”— æ¨¡çµ„é–“è³‡æ–™äº¤äº’

### 1. EnvironmentValidator â†” DeploymentManager

```python
@dataclass
class EnvironmentValidationResult:
    passed: bool
    failed_checks: List[ValidationCheck]
    system_resources: SystemResourceInfo
    docker_status: DockerEnvironmentInfo
    recommendations: List[str]
    
# DeploymentManagerä½¿ç”¨é©—è­‰çµæœ
async def pre_deployment_validation(self) -> bool:
    validation_result = await self.env_validator.validate_environment()
    
    # è¨˜éŒ„é©—è­‰çµæœåˆ°è³‡æ–™åº«
    await self._log_validation_result(self.deployment_id, validation_result)
    
    # æ ¹æ“šé©—è­‰çµæœæ±ºå®šæ˜¯å¦ç¹¼çºŒéƒ¨ç½²
    if not validation_result.passed:
        critical_failures = [
            check for check in validation_result.failed_checks 
            if check.severity == CheckSeverity.CRITICAL
        ]
        if critical_failures:
            return False
    
    return True
```

### 2. MonitoringCollector â†” ErrorHandler

```python
async def proactive_error_detection(self):
    """ä¸»å‹•éŒ¯èª¤æª¢æ¸¬æ©Ÿåˆ¶"""
    
    # MonitoringCollectoræª¢æ¸¬åˆ°ç•°å¸¸æŒ‡æ¨™
    monitoring_data = await self.monitoring_collector.collect_metrics()
    
    for service_metric in monitoring_data.service_metrics:
        # æª¢æŸ¥æ˜¯å¦è¶…éé–¾å€¼
        anomalies = detect_anomalies(service_metric)
        
        for anomaly in anomalies:
            # å‰µå»ºé è­¦éŒ¯èª¤
            predictive_error = create_predictive_error(anomaly)
            
            # ä½¿ç”¨ErrorHandlerè™•ç†é è­¦
            recovery_action = await self.error_handler.handle_error(
                predictive_error, 
                {"source": "monitoring", "service": service_metric.service_name}
            )
            
            # å¦‚æœå¯ä»¥è‡ªå‹•æ¢å¾©ï¼Œç«‹å³åŸ·è¡Œ
            if recovery_action.auto_executable:
                await self.error_handler.execute_recovery_action(
                    predictive_error.error_id, 
                    recovery_action
                )
```

### 3. è³‡æ–™åº«é€£æ¥æ± ç®¡ç†

```python
class DatabaseConnectionManager:
    """çµ±ä¸€çš„è³‡æ–™åº«é€£æ¥ç®¡ç†"""
    
    def __init__(self):
        self.connections = {
            'monitoring': AsyncSQLiteConnection('data/monitoring.db'),
            'errors': AsyncSQLiteConnection('data/errors.db'),
            'deployment': AsyncSQLiteConnection('data/deployment_logs.db')
        }
    
    @asynccontextmanager
    async def get_connection(self, db_name: str):
        """ç²å–è³‡æ–™åº«é€£æ¥"""
        conn = self.connections[db_name]
        try:
            await conn.connect()
            yield conn
        finally:
            await conn.close()
    
    async def execute_cross_db_transaction(self, operations: List[DatabaseOperation]):
        """è·¨è³‡æ–™åº«äº‹å‹™åŸ·è¡Œ"""
        # å¯¦ç¾è·¨å¤šå€‹SQLiteè³‡æ–™åº«çš„äº‹å‹™ä¸€è‡´æ€§
        pass
```

## ğŸ“ˆ è³‡æ–™åˆ†æèˆ‡å ±å‘Š

### 1. éƒ¨ç½²æˆåŠŸç‡åˆ†æ

```python
async def analyze_deployment_success_rate(days: int = 30) -> DeploymentAnalysis:
    """åˆ†æéƒ¨ç½²æˆåŠŸç‡è¶¨å‹¢"""
    
    query = """
    SELECT 
        DATE(start_time) as deployment_date,
        COUNT(*) as total_deployments,
        SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful_deployments,
        AVG(duration_ms) as avg_duration_ms
    FROM deployment_operations 
    WHERE start_time >= datetime('now', '-{} days')
    GROUP BY DATE(start_time)
    ORDER BY deployment_date
    """.format(days)
    
    results = await execute_query('deployment', query)
    
    return DeploymentAnalysis(
        success_rate_trend=calculate_trend(results),
        performance_trend=calculate_performance_trend(results),
        recommendations=generate_deployment_recommendations(results)
    )
```

### 2. æœå‹™å¥åº·åº¦è©•åˆ†

```python
async def calculate_service_health_score(service_name: str) -> ServiceHealthScore:
    """è¨ˆç®—æœå‹™å¥åº·åº¦è©•åˆ†"""
    
    # æ”¶é›†å¤šç¶­åº¦æ•¸æ“š
    uptime_score = await calculate_uptime_score(service_name)
    performance_score = await calculate_performance_score(service_name)
    error_rate_score = await calculate_error_rate_score(service_name)
    dependency_score = await calculate_dependency_score(service_name)
    
    # åŠ æ¬Šè¨ˆç®—ç¸½åˆ†
    total_score = (
        uptime_score * 0.3 +
        performance_score * 0.25 +
        error_rate_score * 0.25 +
        dependency_score * 0.2
    )
    
    return ServiceHealthScore(
        service_name=service_name,
        total_score=total_score,
        uptime_score=uptime_score,
        performance_score=performance_score,
        error_rate_score=error_rate_score,
        dependency_score=dependency_score,
        grade=calculate_grade(total_score),
        improvement_suggestions=generate_improvement_suggestions(
            service_name, uptime_score, performance_score, error_rate_score, dependency_score
        )
    )
```

## ğŸš€ è³‡æ–™æµå„ªåŒ–ç­–ç•¥

### 1. æ‰¹é‡è™•ç†å„ªåŒ–

```python
class BatchDataProcessor:
    """æ‰¹é‡æ•¸æ“šè™•ç†å™¨"""
    
    def __init__(self):
        self.batch_size = 100
        self.flush_interval = 30  # ç§’
        self.pending_operations = []
    
    async def add_operation(self, operation: DatabaseOperation):
        """æ·»åŠ æ“ä½œåˆ°æ‰¹è™•ç†éšŠåˆ—"""
        self.pending_operations.append(operation)
        
        if len(self.pending_operations) >= self.batch_size:
            await self.flush_batch()
    
    async def flush_batch(self):
        """æ‰¹é‡æäº¤æ“ä½œ"""
        if not self.pending_operations:
            return
        
        try:
            async with database_transaction() as tx:
                for operation in self.pending_operations:
                    await tx.execute(operation.query, operation.params)
                await tx.commit()
            
            logger.info(f"æ‰¹é‡æäº¤ {len(self.pending_operations)} å€‹æ“ä½œ")
            self.pending_operations.clear()
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ“ä½œå¤±æ•—: {str(e)}")
            # å¯¦ç¾é‡è©¦é‚è¼¯
            await self.retry_failed_operations()
```

### 2. è³‡æ–™å£“ç¸®èˆ‡æ¸…ç†

```python
async def data_lifecycle_management():
    """è³‡æ–™ç”Ÿå‘½é€±æœŸç®¡ç†"""
    
    # å£“ç¸®æ­·å²æ•¸æ“š
    await compress_old_monitoring_data(days_old=90)
    
    # æ¸…ç†éæœŸéŒ¯èª¤è¨˜éŒ„
    await cleanup_resolved_errors(days_old=30)
    
    # æ­¸æª”éƒ¨ç½²æ—¥èªŒ
    await archive_deployment_logs(days_old=180)
    
    # å„ªåŒ–è³‡æ–™åº«
    await optimize_database_indexes()
    await vacuum_databases()
```

é€™å€‹è³‡æ–™æµæ¶æ§‹è¨­è¨ˆç¢ºä¿äº†ï¼š

1. **å®Œæ•´çš„è³‡æ–™è¿½è¹¤**ï¼šå¾éƒ¨ç½²é–‹å§‹åˆ°éŒ¯èª¤æ¢å¾©çš„å®Œæ•´éˆè·¯
2. **é«˜æ•ˆçš„è³‡æ–™è™•ç†**ï¼šæ‰¹é‡æ“ä½œã€é€£æ¥æ± ç®¡ç†ã€ç•°æ­¥è™•ç†
3. **æ™ºèƒ½çš„éŒ¯èª¤è™•ç†**ï¼šæ¨¡å¼è­˜åˆ¥ã€è‡ªå‹•æ¢å¾©ã€å½±éŸ¿åˆ†æ  
4. **å…¨é¢çš„ç›£æ§åˆ†æ**ï¼šå¥åº·åº¦è©•åˆ†ã€è¶¨å‹¢åˆ†æã€æ•ˆèƒ½åŸºæº–
5. **å„ªç§€çš„è³‡æ–™ç®¡ç†**ï¼šç”Ÿå‘½é€±æœŸç®¡ç†ã€å£“ç¸®æ­¸æª”ã€ç´¢å¼•å„ªåŒ–

é€™ç‚ºROAS Bot v2.4.3æä¾›äº†å …å¯¦çš„è³‡æ–™æ¶æ§‹åŸºç¤ï¼