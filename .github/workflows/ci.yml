name: Discord Bot CI/CD Pipeline
# Task ID: T7 - ç’°å¢ƒèˆ‡ä¾è³´ç®¡ç†ç³»çµ±ï¼šCIæµç¨‹é·ç§»è‡³uv

on:
  push:
    branches: [ main, restore/specs-2.4.1 ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.10'
  TESTING: true

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 25  # ç¸½æ¸¬è©¦æ™‚é–“é™åˆ¶ 25 åˆ†é˜
    
    strategy:
      fail-fast: false  # ä¸è¦å› ç‚ºä¸€å€‹æ¸¬è©¦å¤±æ•—å°±å–æ¶ˆå…¶ä»–
      matrix:
        test-type: [unit, integration, dpytest, docker]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Cache uv dependencies  
      uses: actions/cache@v3
      with:
        path: ~/.cache/uv
        key: ${{ runner.os }}-uv-${{ hashFiles('**/uv.lock') }}
        restore-keys: |
          ${{ runner.os }}-uv-
          
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Set up test environment
      run: |
        mkdir -p logs test_reports tests/dpytest
        export LOG_LEVEL=WARNING
        
    - name: Run unit tests
      if: matrix.test-type == 'unit'
      run: |
        uv run python -m pytest tests/unit/ -v \
          --cov=services --cov=panels --cov=core \
          --cov-report=xml --cov-report=html \
          --junit-xml=test_reports/unit-results.xml \
          -m "unit and not slow"
          
    - name: Run integration tests  
      if: matrix.test-type == 'integration'
      run: |
        uv run python -m pytest tests/integration/ -v \
          --junit-xml=test_reports/integration-results.xml \
          -m "integration and not slow" \
          --timeout=300
          
    - name: Run dpytest Discord tests
      if: matrix.test-type == 'dpytest'
      run: |
        # dpytest Discordæ¸¬è©¦ç¾åœ¨å·²ä¿®å¾©è¼”åŠ©å‡½æ•¸å•é¡Œ
        uv run python -m pytest tests/dpytest/ -v \
          --junit-xml=test_reports/dpytest-results.xml \
          -m "dpytest" \
          --timeout=180 \
          --tb=short \
          --disable-warnings
      # ç§»é™¤continue-on-errorï¼Œå› ç‚ºå•é¡Œå·²ä¿®å¾©
      
    - name: Run Docker tests
      if: matrix.test-type == 'docker'
      run: |
        # è¨­ç½® Docker æ¸¬è©¦ç’°å¢ƒ
        export DOCKER_AVAILABLE=true
        export TESTING=true
        export CI_DOCKER_TEST=true
        export DOCKER_BUILDKIT=1
        
        # æª¢æŸ¥ Docker å¯ç”¨æ€§
        docker version
        docker system info
        
        # é å…ˆæ§‹å»ºæ¸¬è©¦é¡åƒä»¥ç¯€çœæ™‚é–“
        docker build -t roas-bot:test .
        
        # é‹è¡Œå¢å¼·çš„ Docker æ¸¬è©¦å¥—ä»¶
        uv run python -m pytest tests/docker/ -v \
          --cov=tests.docker --cov=services --cov=panels --cov=core \
          --cov-report=xml:test_reports/docker-coverage.xml \
          --cov-report=json:test_reports/docker-coverage.json \
          --cov-report=html:test_reports/docker-htmlcov \
          --junit-xml=test_reports/docker-results.xml \
          -m "docker" \
          --timeout=600 \
          --tb=short \
          --maxfail=5 \
          --durations=10 \
          --strict-markers \
          -x
        
        # é‹è¡Œå®¹å™¨åŸºç¤åŠŸèƒ½é©—è­‰
        uv run python -m pytest tests/docker/test_container_basics.py -v \
          --junit-xml=test_reports/docker-basics-results.xml \
          --timeout=300
        
        # é‹è¡ŒåŸºç¤è¨­æ–½é©—è­‰æ¸¬è©¦
        uv run python tests/docker/validate_infrastructure.py
      
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.test-type }}
        path: |
          test_reports/
          htmlcov/
          coverage.xml
          
    - name: Upload coverage to Codecov (unit tests)
      if: matrix.test-type == 'unit'
      uses: codecov/codecov-action@v3
      with:
        file: coverage.xml
        flags: unittests
        name: codecov-umbrella
        
    - name: Upload Docker test coverage to Codecov
      if: matrix.test-type == 'docker'
      uses: codecov/codecov-action@v3
      with:
        files: test_reports/docker-coverage.xml
        flags: dockertests
        name: codecov-docker
        fail_ci_if_error: false
        verbose: true
        
    - name: Generate enhanced Docker coverage report with mandatory quality gates
      if: matrix.test-type == 'docker'
      run: |
        # ç”Ÿæˆè©³ç´°çš„ Docker æ¸¬è©¦è¦†è“‹ç‡å ±å‘Šä¸¦å¯¦æ–½å¼·åˆ¶å“è³ªé–€æª»æª¢æŸ¥
        uv run python -c "
        import json
        import xml.etree.ElementTree as ET
        import sys
        from datetime import datetime
        from pathlib import Path
        
        def parse_coverage_xml():
            xml_path = Path('test_reports/docker-coverage.xml')
            if not xml_path.exists():
                return {'error': 'Coverage XML file not found'}
                
            tree = ET.parse(xml_path)
            root = tree.getroot()
            
            line_rate = float(root.get('line-rate', 0)) * 100
            branch_rate = float(root.get('branch-rate', 0)) * 100
            
            # è¨ˆç®—ç¸½è¡Œæ•¸
            total_lines = 0
            covered_lines = 0
            
            for package in root.findall('.//package'):
                for class_elem in package.findall('classes/class'):
                    for line in class_elem.findall('lines/line'):
                        total_lines += 1
                        if int(line.get('hits', '0')) > 0:
                            covered_lines += 1
            
            return {
                'line_coverage': round(line_rate, 2),
                'branch_coverage': round(branch_rate, 2),
                'total_lines': total_lines,
                'covered_lines': covered_lines,
                'overall_coverage': round((line_rate + branch_rate) / 2, 2)
            }
        
        # å¼·åˆ¶æ€§å“è³ªé–€æª»è¨­å®š
        MANDATORY_COVERAGE_THRESHOLD = 90.0
        MANDATORY_LINE_COVERAGE_THRESHOLD = 85.0
        MANDATORY_BRANCH_COVERAGE_THRESHOLD = 80.0
        
        coverage_data = parse_coverage_xml()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
        if 'error' in coverage_data:
            print(f'::error title=Coverage Analysis Failed::{coverage_data[\"error\"]}')
            sys.exit(1)
        
        overall_cov = coverage_data.get('overall_coverage', 0)
        line_cov = coverage_data.get('line_coverage', 0)
        branch_cov = coverage_data.get('branch_coverage', 0)
        
        # ç”Ÿæˆå¢å¼·å ±å‘Š
        enhanced_report = {
            'task_id': 'T1',
            'test_type': 'docker',
            'timestamp': datetime.now().isoformat(),
            'coverage_metrics': coverage_data,
            'quality_gates': {
                'minimum_coverage': MANDATORY_COVERAGE_THRESHOLD,
                'minimum_line_coverage': MANDATORY_LINE_COVERAGE_THRESHOLD,
                'minimum_branch_coverage': MANDATORY_BRANCH_COVERAGE_THRESHOLD,
                'actual_coverage': overall_cov,
                'actual_line_coverage': line_cov,
                'actual_branch_coverage': branch_cov,
                'overall_passed': overall_cov >= MANDATORY_COVERAGE_THRESHOLD,
                'line_passed': line_cov >= MANDATORY_LINE_COVERAGE_THRESHOLD,
                'branch_passed': branch_cov >= MANDATORY_BRANCH_COVERAGE_THRESHOLD
            }
        }
        
        # å¼·åˆ¶å“è³ªé–€æª»æª¢æŸ¥
        quality_failures = []
        
        if overall_cov < MANDATORY_COVERAGE_THRESHOLD:
            quality_failures.append(f'æ•´é«”è¦†è“‹ç‡ {overall_cov}% < {MANDATORY_COVERAGE_THRESHOLD}%')
            
        if line_cov < MANDATORY_LINE_COVERAGE_THRESHOLD:
            quality_failures.append(f'è¡Œè¦†è“‹ç‡ {line_cov}% < {MANDATORY_LINE_COVERAGE_THRESHOLD}%')
            
        if branch_cov < MANDATORY_BRANCH_COVERAGE_THRESHOLD:
            quality_failures.append(f'åˆ†æ”¯è¦†è“‹ç‡ {branch_cov}% < {MANDATORY_BRANCH_COVERAGE_THRESHOLD}%')
        
        # ä¿å­˜å¢å¼·å ±å‘Š
        with open('test_reports/docker-coverage-enhanced.json', 'w') as f:
            json.dump(enhanced_report, f, indent=2)
        
        # è¼¸å‡ºè©³ç´°è¦†è“‹ç‡è³‡è¨Š
        print(f'ğŸ“Š è¦†è“‹ç‡è©³ç´°å ±å‘Š:')
        print(f'   æ•´é«”è¦†è“‹ç‡: {overall_cov}% (é–€æª»: {MANDATORY_COVERAGE_THRESHOLD}%)')
        print(f'   è¡Œè¦†è“‹ç‡: {line_cov}% (é–€æª»: {MANDATORY_LINE_COVERAGE_THRESHOLD}%)')
        print(f'   åˆ†æ”¯è¦†è“‹ç‡: {branch_cov}% (é–€æª»: {MANDATORY_BRANCH_COVERAGE_THRESHOLD}%)')
        print(f'   ç¸½è¡Œæ•¸: {coverage_data.get(\"total_lines\", 0)}')
        print(f'   å·²è¦†è“‹è¡Œæ•¸: {coverage_data.get(\"covered_lines\", 0)}')
        print(f'   æœªè¦†è“‹è¡Œæ•¸: {coverage_data.get(\"total_lines\", 0) - coverage_data.get(\"covered_lines\", 0)}')
        
        # å¼·åˆ¶å“è³ªé–€æª»åŸ·è¡Œ
        if quality_failures:
            print(f'::error title=MANDATORY QUALITY GATE FAILED::âŒ Dockeræ¸¬è©¦è¦†è“‹ç‡æœªé”å¼·åˆ¶è¦æ±‚ï¼')
            for failure in quality_failures:
                print(f'::error title=Quality Gate Failure::{failure}')
            print(f'::error title=Action Required::å¿…é ˆæå‡æ¸¬è©¦è¦†è“‹ç‡æ‰èƒ½ç¹¼çºŒCIæµç¨‹')
            
            # ç”Ÿæˆä¿®å¾©å»ºè­°
            print(f'ğŸ”§ ä¿®å¾©å»ºè­°:')
            if overall_cov < MANDATORY_COVERAGE_THRESHOLD:
                needed_coverage = MANDATORY_COVERAGE_THRESHOLD - overall_cov
                print(f'   - éœ€è¦å¢åŠ  {needed_coverage:.2f}% çš„è¦†è“‹ç‡')
            
            # è¨˜éŒ„å“è³ªé–€æª»å¤±æ•—
            enhanced_report['quality_gates']['failure_timestamp'] = datetime.now().isoformat()
            enhanced_report['quality_gates']['failure_details'] = quality_failures
            
            with open('test_reports/docker-coverage-enhanced.json', 'w') as f:
                json.dump(enhanced_report, f, indent=2)
            
            # å¼·åˆ¶å¤±æ•—CI
            sys.exit(1)
        else:
            print(f'::notice title=Quality Gates PASSED::âœ… æ‰€æœ‰Dockeræ¸¬è©¦è¦†è“‹ç‡é–€æª»æª¢æŸ¥é€šéï¼')
            print(f'::notice title=Coverage Excellent::Docker æ¸¬è©¦è¦†è“‹ç‡é” {overall_cov}% (â‰¥{MANDATORY_COVERAGE_THRESHOLD}%)')
        "

  random-interaction-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 12  # éš¨æ©Ÿæ¸¬è©¦æ™‚é–“é™åˆ¶ 12 åˆ†é˜
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Run random interaction tests
      run: |
        # è¨­ç½®å›ºå®šç¨®å­é€²è¡Œå¯é‡ç¾çš„éš¨æ©Ÿæ¸¬è©¦
        uv run python -m pytest tests/random/ -v \
          --junit-xml=test_reports/random-results.xml \
          -m "random_interaction" \
          --timeout=600 \
          --seed=12345 \
          --max-steps=50
      continue-on-error: true
      
    - name: Generate reproduction report
      if: failure()
      run: |
        # ç”Ÿæˆå¤±æ•—é‡ç¾å ±å‘Š
        uv run python scripts/generate_failure_report.py \
          --test-results test_reports/random-results.xml \
          --output test_reports/reproduction-report.json
        
    - name: Upload random test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: random-test-results
        path: test_reports/

  stability-check:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # ç©©å®šæ€§æª¢æŸ¥æ™‚é–“é™åˆ¶ 15 åˆ†é˜
    needs: test
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Run stability tests (3x repeat)
      run: |
        # é‹è¡Œ3æ¬¡ä»¥æª¢æ¸¬flakyæ¸¬è©¦
        for i in {1..3}; do
          echo "Stability run $i/3"
          uv run python -m pytest tests/dpytest/ tests/random/ \
            --junit-xml=test_reports/stability-run-$i.xml \
            -m "stability or random_interaction" \
            --timeout=300 || true
        done
        
    - name: Analyze stability
      run: |
        uv run python scripts/analyze_stability.py \
          --input-dir test_reports/ \
          --output test_reports/stability-analysis.json
      continue-on-error: true
        
    - name: Upload stability results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: stability-results
        path: test_reports/

  lint-and-format:
    runs-on: ubuntu-latest
    timeout-minutes: 8  # ç¨‹å¼ç¢¼æª¢æŸ¥æ™‚é–“é™åˆ¶ 8 åˆ†é˜
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Run Black formatter check
      run: |
        uv run black --check --diff .
        
    - name: Run isort import sorting check
      run: |
        uv run isort --check-only --diff .
        
    - name: Run Flake8 linting
      run: |
        uv run flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        
    - name: Run MyPy type checking
      run: |
        uv run mypy services/ panels/ core/ --ignore-missing-imports
      continue-on-error: true

  security-scan:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # å¢åŠ å®‰å…¨æƒææ™‚é–“é™åˆ¶ä»¥æ”¯æ´ Docker æƒæ
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        uv add safety bandit --no-sync
        
    - name: Run dependency security check
      run: |
        uv run safety check
        
    - name: Run code security analysis
      run: |
        uv run bandit -r services/ panels/ core/ -f json -o security-report.json || true

    - name: Install Trivy for container security scanning
      run: |
        # å®‰è£ Trivy å®¹å™¨å®‰å…¨æƒæå™¨
        sudo apt-get update
        sudo apt-get install wget apt-transport-https gnupg lsb-release -y
        wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | sudo apt-key add -
        echo "deb https://aquasecurity.github.io/trivy-repo/deb $(lsb_release -sc) main" | sudo tee -a /etc/apt/sources.list.d/trivy.list
        sudo apt-get update
        sudo apt-get install trivy -y
        
    - name: Build Docker image for security scanning
      run: |
        # æ§‹å»º Docker é¡åƒä»¥é€²è¡Œå®‰å…¨æƒæ
        docker build -t roas-bot:security-scan . || echo "Docker build failed, will use existing image"
        
    - name: Run comprehensive Docker security scan with Trivy
      run: |
        # åŸ·è¡Œå…¨é¢çš„ Docker å®‰å…¨æƒæ
        echo "::group::Docker Security Scanning with Trivy"
        
        mkdir -p security-reports
        
        # æª¢æŸ¥é¡åƒæ˜¯å¦å­˜åœ¨
        if docker image inspect roas-bot:security-scan >/dev/null 2>&1; then
          echo "ä½¿ç”¨æ§‹å»ºçš„é¡åƒé€²è¡Œæƒæ: roas-bot:security-scan"
          IMAGE_NAME="roas-bot:security-scan"
        else
          echo "ä½¿ç”¨å‚™ç”¨é¡åƒé€²è¡Œæƒæ: python:3.10"
          IMAGE_NAME="python:3.10"
          docker pull python:3.10 || true
        fi
        
        # Trivy æ¼æ´æƒæ (é«˜åš´é‡æ€§å’Œé—œéµæ¼æ´)
        echo "ğŸ” åŸ·è¡Œæ¼æ´æƒæ..."
        trivy image \
          --format json \
          --output security-reports/trivy-vulnerabilities.json \
          --severity HIGH,CRITICAL \
          --no-progress \
          "$IMAGE_NAME" || echo "æ¼æ´æƒæå®Œæˆï¼Œå¯èƒ½åŒ…å«è­¦å‘Š"
          
        # Trivy è¨­å®šéŒ¯èª¤æƒæ
        echo "âš™ï¸ åŸ·è¡Œé…ç½®éŒ¯èª¤æƒæ..."
        trivy image \
          --format json \
          --output security-reports/trivy-misconfigurations.json \
          --scanners misconfig \
          --severity HIGH,CRITICAL \
          --no-progress \
          "$IMAGE_NAME" || echo "é…ç½®æƒæå®Œæˆï¼Œå¯èƒ½åŒ…å«è­¦å‘Š"
        
        # Trivy ç§˜å¯†æƒæ
        echo "ğŸ” åŸ·è¡Œç§˜å¯†æƒæ..."
        trivy image \
          --format json \
          --output security-reports/trivy-secrets.json \
          --scanners secret \
          --no-progress \
          "$IMAGE_NAME" || echo "ç§˜å¯†æƒæå®Œæˆï¼Œå¯èƒ½åŒ…å«è­¦å‘Š"
          
        echo "::endgroup::"
        
    - name: Analyze Docker security scan results
      run: |
        # åˆ†æ Docker å®‰å…¨æƒæçµæœä¸¦ç”Ÿæˆå ±å‘Š
        uv run python -c "
        import json
        import sys
        from pathlib import Path
        from datetime import datetime
        
        security_reports_dir = Path('security-reports')
        
        def analyze_trivy_results():
            results = {
                'vulnerabilities': {'count': 0, 'critical': 0, 'high': 0, 'details': []},
                'misconfigurations': {'count': 0, 'critical': 0, 'high': 0, 'details': []},
                'secrets': {'count': 0, 'details': []},
                'overall_risk_level': 'LOW'
            }
            
            # åˆ†ææ¼æ´å ±å‘Š
            vuln_file = security_reports_dir / 'trivy-vulnerabilities.json'
            if vuln_file.exists():
                try:
                    with open(vuln_file) as f:
                        data = json.load(f)
                    
                    for result in data.get('Results', []):
                        vulnerabilities = result.get('Vulnerabilities', [])
                        for vuln in vulnerabilities:
                            severity = vuln.get('Severity', '').upper()
                            results['vulnerabilities']['count'] += 1
                            
                            if severity == 'CRITICAL':
                                results['vulnerabilities']['critical'] += 1
                            elif severity == 'HIGH':
                                results['vulnerabilities']['high'] += 1
                            
                            results['vulnerabilities']['details'].append({
                                'id': vuln.get('VulnerabilityID'),
                                'severity': severity,
                                'package': vuln.get('PkgName'),
                                'version': vuln.get('InstalledVersion'),
                                'title': vuln.get('Title', '')[:100]
                            })
                except Exception as e:
                    print(f'åˆ†ææ¼æ´å ±å‘Šæ™‚å‡ºéŒ¯: {e}')
            
            # åˆ†æé…ç½®éŒ¯èª¤å ±å‘Š
            misconfig_file = security_reports_dir / 'trivy-misconfigurations.json'
            if misconfig_file.exists():
                try:
                    with open(misconfig_file) as f:
                        data = json.load(f)
                    
                    for result in data.get('Results', []):
                        misconfigs = result.get('Misconfigurations', [])
                        for misconfig in misconfigs:
                            severity = misconfig.get('Severity', '').upper()
                            results['misconfigurations']['count'] += 1
                            
                            if severity == 'CRITICAL':
                                results['misconfigurations']['critical'] += 1
                            elif severity == 'HIGH':
                                results['misconfigurations']['high'] += 1
                            
                            results['misconfigurations']['details'].append({
                                'id': misconfig.get('ID'),
                                'severity': severity,
                                'title': misconfig.get('Title', '')[:100],
                                'message': misconfig.get('Message', '')[:200]
                            })
                except Exception as e:
                    print(f'åˆ†æé…ç½®éŒ¯èª¤å ±å‘Šæ™‚å‡ºéŒ¯: {e}')
            
            # åˆ†æç§˜å¯†æƒæå ±å‘Š
            secrets_file = security_reports_dir / 'trivy-secrets.json'
            if secrets_file.exists():
                try:
                    with open(secrets_file) as f:
                        data = json.load(f)
                    
                    for result in data.get('Results', []):
                        secrets = result.get('Secrets', [])
                        for secret in secrets:
                            results['secrets']['count'] += 1
                            results['secrets']['details'].append({
                                'rule_id': secret.get('RuleID'),
                                'category': secret.get('Category'),
                                'severity': secret.get('Severity', '').upper(),
                                'title': secret.get('Title', '')[:100]
                            })
                except Exception as e:
                    print(f'åˆ†æç§˜å¯†æƒæå ±å‘Šæ™‚å‡ºéŒ¯: {e}')
            
            # è¨ˆç®—æ•´é«”é¢¨éšªç­‰ç´š
            total_critical = (results['vulnerabilities']['critical'] + 
                            results['misconfigurations']['critical'])
            total_high = (results['vulnerabilities']['high'] + 
                        results['misconfigurations']['high'])
            secret_count = results['secrets']['count']
            
            if total_critical > 0 or secret_count > 0:
                results['overall_risk_level'] = 'CRITICAL'
            elif total_high > 5:
                results['overall_risk_level'] = 'HIGH'
            elif total_high > 0:
                results['overall_risk_level'] = 'MEDIUM'
            
            return results
        
        # åŸ·è¡Œåˆ†æ
        analysis_results = analyze_trivy_results()
        
        # ç”Ÿæˆç¶œåˆå®‰å…¨å ±å‘Š
        security_report = {
            'timestamp': datetime.now().isoformat(),
            'scan_type': 'docker_comprehensive',
            'task_id': 'T1',
            'trivy_analysis': analysis_results,
            'risk_assessment': {
                'overall_risk_level': analysis_results['overall_risk_level'],
                'total_vulnerabilities': analysis_results['vulnerabilities']['count'],
                'critical_vulnerabilities': analysis_results['vulnerabilities']['critical'],
                'high_vulnerabilities': analysis_results['vulnerabilities']['high'],
                'total_misconfigurations': analysis_results['misconfigurations']['count'],
                'total_secrets': analysis_results['secrets']['count'],
                'security_score': max(0, 100 - (analysis_results['vulnerabilities']['critical'] * 10) - (analysis_results['vulnerabilities']['high'] * 5) - (analysis_results['secrets']['count'] * 20))
            }
        }
        
        # ä¿å­˜å ±å‘Š
        with open('security-reports/comprehensive-security-analysis.json', 'w') as f:
            json.dump(security_report, f, indent=2)
        
        # è¼¸å‡ºçµæœåˆ° GitHub Actions
        risk_level = analysis_results['overall_risk_level']
        total_critical = analysis_results['vulnerabilities']['critical'] + analysis_results['misconfigurations']['critical']
        total_high = analysis_results['vulnerabilities']['high'] + analysis_results['misconfigurations']['high']
        total_secrets = analysis_results['secrets']['count']
        security_score = security_report['risk_assessment']['security_score']
        
        print(f'ğŸ›¡ï¸ Docker å®‰å…¨æƒæå®Œæˆ')
        print(f'   é¢¨éšªç­‰ç´š: {risk_level}')
        print(f'   å®‰å…¨åˆ†æ•¸: {security_score}/100')
        print(f'   é—œéµæ¼æ´: {total_critical}')
        print(f'   é«˜é¢¨éšªå•é¡Œ: {total_high}')
        print(f'   ç™¼ç¾ç§˜å¯†: {total_secrets}')
        
        if risk_level == 'CRITICAL':
            print(f'::error title=CRITICAL Security Issues::ç™¼ç¾ {total_critical} å€‹é—œéµå®‰å…¨å•é¡Œå’Œ {total_secrets} å€‹ç§˜å¯†æ´©éœ²')
            print(f'::error title=Action Required::å¿…é ˆä¿®å¾©é—œéµå®‰å…¨å•é¡Œæ‰èƒ½éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ')
            # æ³¨æ„ï¼šé€™è£¡ä¸å¼·åˆ¶å¤±æ•— CIï¼Œä½†æœƒç”¢ç”Ÿè­¦å‘Š
        elif risk_level == 'HIGH':
            print(f'::warning title=HIGH Security Risk::ç™¼ç¾ {total_high} å€‹é«˜é¢¨éšªå®‰å…¨å•é¡Œ')
            print(f'::warning title=Recommendation::å»ºè­°åœ¨éƒ¨ç½²å‰ä¿®å¾©é«˜é¢¨éšªå•é¡Œ')
        elif risk_level == 'MEDIUM':
            print(f'::notice title=MEDIUM Security Risk::ç™¼ç¾å°‘é‡å®‰å…¨å•é¡Œï¼Œå»ºè­°å®šæœŸæª¢æŸ¥')
        else:
            print(f'::notice title=Security Scan PASSED::âœ… Docker å®‰å…¨æƒæé€šéï¼Œé¢¨éšªç­‰ç´šè¼ƒä½')
        "
        
    - name: Upload security results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: comprehensive-security-results
        path: |
          security-report.json
          security-reports/

  docker-cross-platform:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 15  # è·¨å¹³å°æ¸¬è©¦æ™‚é–“é™åˆ¶ 15 åˆ†é˜
    needs: test
    if: github.event_name == 'pull_request'
    
    strategy:
      fail-fast: false  # ä¸è¦å› ç‚ºä¸€å€‹å¹³å°å¤±æ•—å°±å–æ¶ˆå…¶ä»–
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        include:
          - os: ubuntu-latest
            platform: linux
            docker_available: true
          - os: windows-latest
            platform: windows  
            docker_available: true
          - os: macos-latest
            platform: darwin
            docker_available: true
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
    
    - name: Check Docker availability (Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        docker version || echo "Docker not available"
        docker system info || echo "Docker system info failed"
        
    - name: Check Docker availability (Windows)
      if: runner.os == 'Windows'
      run: |
        docker version
        docker system info
        
    - name: Run cross-platform Docker tests
      run: |
        # è¨­ç½®è·¨å¹³å°æ¸¬è©¦ç’°å¢ƒ
        export DOCKER_AVAILABLE=${{ matrix.docker_available }}
        export TESTING=true
        export CI_PLATFORM="${{ matrix.platform }}"
        export CURRENT_PLATFORM="${{ matrix.platform }}"
        
        # å‰µå»ºæ¸¬è©¦å ±å‘Šç›®éŒ„
        mkdir -p test_reports
        
        # é‹è¡Œè·¨å¹³å° Docker æ¸¬è©¦
        uv run python -m pytest tests/docker/test_cross_platform.py -v \
          --junit-xml=test_reports/cross-platform-${{ matrix.platform }}-results.xml \
          -m "cross_platform" \
          --timeout=600 \
          --tb=short \
          --maxfail=3 \
          --durations=5
      continue-on-error: false
      
    - name: Generate cross-platform compatibility report
      if: always()
      run: |
        # ç”Ÿæˆè·¨å¹³å°ç›¸å®¹æ€§å ±å‘Š
        uv run python -c "
        import json
        from pathlib import Path
        from datetime import datetime
        
        report = {
            'platform': '${{ matrix.platform }}',
            'os': '${{ matrix.os }}',
            'timestamp': datetime.now().isoformat(),
            'docker_available': '${{ matrix.docker_available }}',
            'test_status': 'completed'
        }
        
        Path('test_reports').mkdir(exist_ok=True)
        with open('test_reports/platform-compatibility-${{ matrix.platform }}.json', 'w') as f:
            json.dump(report, f, indent=2)
        "
        
    - name: Upload cross-platform test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cross-platform-results-${{ matrix.platform }}
        path: test_reports/

  test-failure-notification:
    runs-on: ubuntu-latest
    timeout-minutes: 8
    needs: [test, docker-cross-platform]
    if: always() && (needs.test.result == 'failure' || needs.docker-cross-platform.result == 'failure')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv (fast mode)
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install minimal dependencies for analysis
      run: |
        # åªå®‰è£åˆ†æéœ€è¦çš„æœ€å°ä¾è³´
        uv sync --extra dev --no-progress --no-cache --frozen
        
    - name: Download all test artifacts for failure analysis
      uses: actions/download-artifact@v3
      with:
        path: failure-analysis-artifacts
      continue-on-error: true
        
    - name: Comprehensive test failure analysis and notifications
      run: |
        # åŸ·è¡Œå…¨é¢çš„æ¸¬è©¦å¤±æ•—åˆ†æå’Œé€šçŸ¥
        uv run python -c "
        import sys
        import json
        import xml.etree.ElementTree as ET
        from pathlib import Path
        from datetime import datetime
        from collections import defaultdict
        
        def parse_junit_results(xml_path):
            '''è§£æ JUnit XML çµæœ'''
            if not xml_path.exists():
                return None
                
            try:
                tree = ET.parse(xml_path)
                root = tree.getroot()
                
                results = {
                    'total': int(root.get('tests', 0)),
                    'failures': int(root.get('failures', 0)),
                    'errors': int(root.get('errors', 0)),
                    'skipped': int(root.get('skipped', 0)),
                    'time': float(root.get('time', 0)),
                    'failed_cases': []
                }
                
                # æ”¶é›†å¤±æ•—çš„æ¸¬è©¦æ¡ˆä¾‹è©³æƒ…
                for testcase in root.findall('.//testcase'):
                    failure = testcase.find('failure')
                    error = testcase.find('error')
                    
                    if failure is not None or error is not None:
                        case_info = {
                            'name': testcase.get('name', 'unknown'),
                            'classname': testcase.get('classname', 'unknown'),
                            'time': float(testcase.get('time', 0)),
                            'type': 'failure' if failure is not None else 'error'
                        }
                        
                        if failure is not None:
                            case_info['message'] = failure.get('message', '')
                            case_info['details'] = (failure.text or '')[:500]
                        else:
                            case_info['message'] = error.get('message', '')
                            case_info['details'] = (error.text or '')[:500]
                            
                        results['failed_cases'].append(case_info)
                        
                return results
            except Exception as e:
                print(f'è§£æ {xml_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}')
                return None
        
        # åˆ†ææ‰€æœ‰æ¸¬è©¦çµæœ
        artifact_dir = Path('failure-analysis-artifacts')
        all_results = {}
        failure_summary = defaultdict(list)
        
        # æœå°‹æ‰€æœ‰ JUnit XML æª”æ¡ˆ
        for xml_file in artifact_dir.rglob('*-results.xml'):
            test_type = xml_file.stem.replace('-results', '')
            print(f'åˆ†æ {test_type} æ¸¬è©¦çµæœ: {xml_file}')
            
            results = parse_junit_results(xml_file)
            if results:
                all_results[test_type] = results
                
                if results['failures'] > 0 or results['errors'] > 0:
                    failure_summary[test_type] = results['failed_cases']
        
        # ç”Ÿæˆå…¨é¢çš„å¤±æ•—å ±å‘Š
        total_failures = sum(len(cases) for cases in failure_summary.values())
        
        if total_failures > 0:
            print(f'::error title=Critical: {total_failures} Test Failures::åœ¨å¤šå€‹æ¸¬è©¦é¡å‹ä¸­æª¢æ¸¬åˆ° {total_failures} å€‹æ¸¬è©¦å¤±æ•—')
            
            # æŒ‰æ¸¬è©¦é¡å‹åˆ†é¡é€šçŸ¥
            for test_type, failed_cases in failure_summary.items():
                print(f'::error title={test_type.upper()} Failures::{len(failed_cases)} å€‹ {test_type} æ¸¬è©¦å¤±æ•—')
                
                # é¡¯ç¤ºå‰ 3 å€‹æœ€é‡è¦çš„å¤±æ•—
                for i, case in enumerate(failed_cases[:3]):
                    short_msg = case['message'][:100] + '...' if len(case['message']) > 100 else case['message']
                    print(f'::error title=Failure {i+1} in {test_type}::{case[\"name\"]} - {short_msg}')
            
            # ç”Ÿæˆè©³ç´°å¤±æ•—åˆ†æå ±å‘Š
            failure_report = {
                'analysis_timestamp': datetime.now().isoformat(),
                'total_test_failures': total_failures,
                'failure_breakdown': dict(failure_summary),
                'test_statistics': all_results,
                'ci_context': {
                    'workflow_run': '${{ github.run_id }}',
                    'commit_sha': '${{ github.sha }}',
                    'branch': '${{ github.ref }}',
                    'actor': '${{ github.actor }}'
                },
                'recommendations': []
            }
            
            # æ·»åŠ ä¿®å¾©å»ºè­°
            if 'docker' in failure_summary:
                failure_report['recommendations'].append('æª¢æŸ¥ Docker ç’°å¢ƒå’Œå®¹å™¨é…ç½®')
            if 'unit' in failure_summary:
                failure_report['recommendations'].append('æª¢æŸ¥å–®å…ƒæ¸¬è©¦é‚è¼¯å’Œæ¨¡æ“¬è¨­ç½®')
            if 'integration' in failure_summary:
                failure_report['recommendations'].append('æª¢æŸ¥æœå‹™æ•´åˆå’Œä¾è³´é …')
            if 'cross-platform' in failure_summary:
                failure_report['recommendations'].append('æª¢æŸ¥è·¨å¹³å°ç›¸å®¹æ€§å•é¡Œ')
            
            # ä¿å­˜è©³ç´°å ±å‘Š
            with open('comprehensive-failure-report.json', 'w') as f:
                json.dump(failure_report, f, indent=2, ensure_ascii=False)
            
            # ç™¼é€æ‘˜è¦é€šçŸ¥
            print(f'::warning title=Failure Analysis Complete::å·²ç”Ÿæˆè©³ç´°å¤±æ•—åˆ†æå ±å‘Šï¼Œå…± {total_failures} å€‹å¤±æ•—')
            
        else:
            print('::notice title=No Test Failures Detected::æ‰€æœ‰æ¸¬è©¦åŸ·è¡ŒæˆåŠŸæˆ–æœªæª¢æ¸¬åˆ°å¤±æ•—')
        "
        
    - name: Upload comprehensive failure analysis
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: comprehensive-failure-analysis
        path: |
          comprehensive-failure-report.json
          failure-analysis-artifacts/
          
    - name: Notify development team on critical failures
      if: failure()
      run: |
        echo "::error title=Development Team Alert::æ¸¬è©¦å¤±æ•—éœ€è¦ç«‹å³é—œæ³¨ - è«‹æŸ¥çœ‹è©³ç´°åˆ†æå ±å‘Š"
        echo "ğŸ“‹ å¤±æ•—åˆ†æå ±å‘Šå·²ä¸Šå‚³è‡³ artifacts"
        echo "ğŸ”§ å»ºè­°æª¢æŸ¥æœ€è¿‘çš„ç¨‹å¼ç¢¼è®Šæ›´å’Œç’°å¢ƒé…ç½®"
        echo "âš ï¸ åœ¨ä¿®å¾©å‰è«‹å‹¿åˆä½µæ­¤ PR"

  docker-test-notification:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: test
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Download test results
      uses: actions/download-artifact@v3
      with:
        name: test-results-docker
        path: test_reports/
      continue-on-error: true
        
    - name: Process Docker test results and send notifications
      run: |
        # æª¢æŸ¥æ¸¬è©¦çµæœæª”æ¡ˆ
        if [ ! -f test_reports/docker-results.xml ]; then
          echo "::warning title=Missing Docker Test Results::Dockeræ¸¬è©¦çµæœæª”æ¡ˆæœªæ‰¾åˆ°ï¼Œå¯èƒ½æ¸¬è©¦æœªåŸ·è¡Œ"
          exit 0
        fi
        
        # é‹è¡Œå¢å¼·çš„ CI æ•´åˆæ¨¡çµ„é€²è¡Œæ¸¬è©¦çµæœè™•ç†å’Œé€šçŸ¥
        uv run python -c "
        import sys
        import xml.etree.ElementTree as ET
        from pathlib import Path
        sys.path.append('tests/docker')
        from ci_integration import CIIntegration
        
        ci = CIIntegration()
        
        # è§£ææ¸¬è©¦çµæœçµ±è¨ˆ
        try:
            tree = ET.parse('test_reports/docker-results.xml')
            root = tree.getroot()
            tests = int(root.get('tests', 0))
            failures = int(root.get('failures', 0))
            errors = int(root.get('errors', 0))
            skipped = int(root.get('skipped', 0))
            
            success_rate = ((tests - failures - errors) / tests * 100) if tests > 0 else 0
            
            print(f'::notice title=Docker Test Statistics::åŸ·è¡Œ {tests} å€‹æ¸¬è©¦ï¼ŒæˆåŠŸç‡ {success_rate:.1f}%')
            
            if failures > 0 or errors > 0:
                print(f'::error title=Docker Test Failures::{failures} å€‹å¤±æ•—ï¼Œ{errors} å€‹éŒ¯èª¤')
            
            if skipped > 5:  # è¶…é 5 å€‹æ¸¬è©¦è·³éæ™‚è­¦å‘Š
                print(f'::warning title=Many Tests Skipped::{skipped} å€‹æ¸¬è©¦è¢«è·³éï¼Œè«‹æª¢æŸ¥æ¸¬è©¦ç’°å¢ƒ')
                
        except Exception as e:
            print(f'::warning title=Cannot Parse Test Results::ç„¡æ³•è§£ææ¸¬è©¦çµæœ: {e}')
        
        # åŸ·è¡Œå®Œæ•´ç®¡é“
        try:
            success = ci.run_full_test_pipeline()
            
            if not success:
                print('::error title=Docker Test Pipeline Failed::Docker æ¸¬è©¦ç®¡é“åŸ·è¡Œå¤±æ•—ï¼Œè«‹æª¢æŸ¥æ¸¬è©¦çµæœå’Œè¦†è“‹ç‡')
                sys.exit(1)
            else:
                print('::notice title=Docker Test Pipeline Successful::Docker æ¸¬è©¦ç®¡é“åŸ·è¡ŒæˆåŠŸ')
        except Exception as e:
            print(f'::error title=Pipeline Execution Failed::ç®¡é“åŸ·è¡Œç•°å¸¸: {e}')
            sys.exit(1)
        "
        
    - name: Upload notification results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: docker-test-notifications
        path: test_reports/

  documentation-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 5  # æ–‡æª”é©—è­‰æ™‚é–“é™åˆ¶ 5 åˆ†é˜
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Validate documentation link integrity
      run: |
        # T3ä»»å‹™: æ–‡æª”é€£çµæœ‰æ•ˆæ€§æª¢æŸ¥
        echo "::group::Documentation Link Validation"
        echo "ğŸ” é–‹å§‹æ–‡æª”é€£çµæœ‰æ•ˆæ€§æª¢æŸ¥..."
        
        # è¨­ç½®æª¢æŸ¥ç’°å¢ƒ
        export TESTING=true
        export CI_LINK_CHECK=true
        
        # åŸ·è¡Œé€£çµæª¢æŸ¥ (ä½¿ç”¨å…§éƒ¨é€£çµæª¢æŸ¥ï¼Œè·³éå¤–éƒ¨é€£çµä»¥æå‡é€Ÿåº¦)
        uv run python scripts/link_checker.py check docs/ \
          --format json \
          --project-root . \
          2>&1 | tee docs_link_check.log || {
            echo "::error title=Link Check Failed::æ–‡æª”é€£çµæª¢æŸ¥å¤±æ•—"
            echo "::group::Error Details"
            cat docs_link_check.log
            echo "::endgroup::"
            exit 1
          }
        
        # æª¢æŸ¥é€£çµæª¢æŸ¥çµæœä¸¦æä¾›è©³ç´°éŒ¯èª¤å ±å‘Š
        if grep -q "âŒ" docs_link_check.log; then
          echo "::error title=Broken Links Found::ç™¼ç¾ç„¡æ•ˆé€£çµï¼Œè«‹ä¿®å¾©å¾Œé‡æ–°æäº¤"
          echo "::group::Broken Links Details"
          
          # æå–ä¸¦é¡¯ç¤ºè©³ç´°çš„ç„¡æ•ˆé€£çµä¿¡æ¯
          if grep -q "ç„¡æ•ˆé€£çµè©³æƒ…" docs_link_check.log; then
            sed -n '/ç„¡æ•ˆé€£çµè©³æƒ…/,/^$/p' docs_link_check.log | head -20
          else
            grep "âŒ" docs_link_check.log | head -10
          fi
          
          echo "::endgroup::"
          
          # æä¾›ä¿®å¾©å»ºè­°
          echo "::group::Link Fix Recommendations"
          echo "ğŸ“‹ ä¿®å¾©å»ºè­°ï¼š"
          echo "   1. æª¢æŸ¥æª”æ¡ˆè·¯å¾‘æ˜¯å¦æ­£ç¢ºå­˜åœ¨"
          echo "   2. ç¢ºèªéŒ¨é»é€£çµçš„æ¨™é¡Œæ˜¯å¦æº–ç¢º"
          echo "   3. æª¢æŸ¥ç›¸å°è·¯å¾‘æ˜¯å¦å¾æ­£ç¢ºç›®éŒ„é–‹å§‹"
          echo "   4. è€ƒæ…®å°‡æš«æ™‚ç„¡æ•ˆçš„é€£çµæ·»åŠ åˆ° .linkcheckignore æ–‡ä»¶"
          echo "   5. å¦‚éœ€æª¢æŸ¥å¤–éƒ¨é€£çµï¼Œè«‹åœ¨æœ¬åœ°åŸ·è¡Œï¼špython scripts/link_checker.py check docs/ --external"
          echo "::endgroup::"
          
          # ç”ŸæˆéŒ¯èª¤æ‘˜è¦
          broken_count=$(grep -c "âŒ" docs_link_check.log || echo "0")
          echo "::error title=Link Check Summary::å…±ç™¼ç¾ $broken_count å€‹ç„¡æ•ˆé€£çµéœ€è¦ä¿®å¾©"
          
          exit 1
        fi
        
        # é¡¯ç¤ºæª¢æŸ¥æ‘˜è¦
        if grep -q "âœ… æ‰€æœ‰é€£çµæª¢æŸ¥é€šé" docs_link_check.log; then
          echo "::notice title=Link Validation PASSED::âœ… æ‰€æœ‰æ–‡æª”é€£çµé©—è­‰é€šé"
          
          # æå–æª¢æŸ¥çµ±è¨ˆ
          if grep -q "ğŸ“Š æª¢æŸ¥çµæœæ‘˜è¦:" docs_link_check.log; then
            echo "::group::Link Check Summary"
            sed -n '/ğŸ“Š æª¢æŸ¥çµæœæ‘˜è¦:/,/^$/p' docs_link_check.log
            echo "::endgroup::"
          fi
        else
          echo "::warning title=Link Check Incomplete::é€£çµæª¢æŸ¥å¯èƒ½æœªå®Œå…¨å®Œæˆ"
        fi
        
        echo "::endgroup::"
        
    - name: Upload documentation validation results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: documentation-validation-results
        path: |
          docs_link_check.log
          docs/reports/

  docker-infrastructure-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: test
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v4
      with:
        version: "latest"
        
    - name: Install dependencies with uv
      run: |
        uv sync --extra dev --no-progress
        
    - name: Validate Docker infrastructure setup
      run: |
        # é©—è­‰ Docker åŸºç¤è¨­æ–½é…ç½®
        echo "::group::Docker Infrastructure Validation"
        
        # æª¢æŸ¥ Docker æ¸¬è©¦æª”æ¡ˆçµæ§‹
        echo "æª¢æŸ¥ Docker æ¸¬è©¦ç›®éŒ„çµæ§‹..."
        test -d tests/docker || { echo "::error::tests/docker ç›®éŒ„ä¸å­˜åœ¨"; exit 1; }
        test -f tests/docker/conftest.py || { echo "::error::Docker conftest.py ä¸å­˜åœ¨"; exit 1; }
        test -f tests/docker/ci_integration.py || { echo "::error::CI æ•´åˆæ¨¡çµ„ä¸å­˜åœ¨"; exit 1; }
        
        # æª¢æŸ¥ Dockerfile
        test -f Dockerfile || { echo "::error::Dockerfile ä¸å­˜åœ¨"; exit 1; }
        
        # é©—è­‰æ¸¬è©¦æ¡†æ¶é…ç½®
        uv run python -c "
        import sys
        sys.path.append('tests/docker')
        try:
            from conftest import DOCKER_TEST_CONFIG, DockerTestFixture
            from ci_integration import CIIntegration, CoverageReporter
            print('âœ… Docker æ¸¬è©¦æ¨¡çµ„å°å…¥æˆåŠŸ')
            print(f'âœ… æ¸¬è©¦é…ç½®é©—è­‰: {DOCKER_TEST_CONFIG[\"image_name\"]}')
        except ImportError as e:
            print(f'::error::Docker æ¸¬è©¦æ¨¡çµ„å°å…¥å¤±æ•—: {e}')
            sys.exit(1)
        "
        
        echo "::endgroup::"
        echo "::notice title=Infrastructure Validation::Docker åŸºç¤è¨­æ–½é©—è­‰é€šé"

  build-status:
    runs-on: ubuntu-latest
    needs: [test, lint-and-format, security-scan, docker-test-notification, docker-infrastructure-validation, documentation-validation]
    if: always()
    
    steps:
    - name: Check build status with enhanced quality gates
      run: |
        echo "::group::Build Status Summary with Quality Gates"
        echo "Test: ${{ needs.test.result }}"
        echo "Lint: ${{ needs.lint-and-format.result }}"
        echo "Security (Enhanced with Trivy): ${{ needs.security-scan.result }}"
        echo "Docker Notification: ${{ needs.docker-test-notification.result }}"
        echo "Docker Infrastructure: ${{ needs.docker-infrastructure-validation.result }}"
        echo "Documentation Validation: ${{ needs.documentation-validation.result }}"
        echo "::endgroup::"
        
        # æª¢æŸ¥æ ¸å¿ƒæ§‹å»ºéšæ®µ
        CORE_SUCCESS=true
        if [[ "${{ needs.test.result }}" != "success" ]]; then
          echo "::error title=Test Stage Failed::æ¸¬è©¦éšæ®µå¤±æ•—"
          CORE_SUCCESS=false
        fi
        
        if [[ "${{ needs.lint-and-format.result }}" != "success" ]]; then
          echo "::error title=Lint Stage Failed::ç¨‹å¼ç¢¼æª¢æŸ¥éšæ®µå¤±æ•—"
          CORE_SUCCESS=false
        fi
        
        if [[ "${{ needs.security-scan.result }}" != "success" ]]; then
          echo "::warning title=Security Scan Issues::å®‰å…¨æƒæéšæ®µå‡ºç¾å•é¡Œï¼Œè«‹æª¢æŸ¥å®‰å…¨å ±å‘Š"
          # ä¸è¨­ç‚ºå¤±æ•—ï¼Œä½†è¨˜éŒ„è­¦å‘Š
        fi
        
        if [[ "${{ needs.docker-test-notification.result }}" != "success" ]]; then
          echo "::error title=Docker Test Notification Failed::Dockeræ¸¬è©¦é€šçŸ¥éšæ®µå¤±æ•—"
          CORE_SUCCESS=false
        fi
        
        if [[ "${{ needs.docker-infrastructure-validation.result }}" != "success" ]]; then
          echo "::error title=Docker Infrastructure Validation Failed::DockeråŸºç¤è¨­æ–½é©—è­‰å¤±æ•—"
          CORE_SUCCESS=false
        fi
        
        if [[ "${{ needs.documentation-validation.result }}" != "success" ]]; then
          echo "::error title=Documentation Validation Failed::æ–‡æª”é€£çµé©—è­‰å¤±æ•—"
          CORE_SUCCESS=false
        fi
        
        # è¼¸å‡ºæœ€çµ‚çµæœ
        if [[ "$CORE_SUCCESS" == "true" ]]; then
          echo "::notice title=Build Successful with Enhanced Quality Gates::âœ… æ‰€æœ‰é—œéµæ§‹å»ºéšæ®µæˆåŠŸå®Œæˆ"
          echo "âœ… Build passed - All critical stages completed successfully"
          
          # ç”Ÿæˆå¢å¼·çš„æ§‹å»ºæ‘˜è¦
          echo "::group::Enhanced Build Summary"
          echo "ğŸ”¬ æ¸¬è©¦åŸ·è¡Œ: âœ… æˆåŠŸ (åŒ…å«å¼·åˆ¶è¦†è“‹ç‡æª¢æŸ¥)"
          echo "ğŸ” ç¨‹å¼ç¢¼æª¢æŸ¥: âœ… æˆåŠŸ" 
          echo "ğŸ›¡ï¸ å¢å¼·å®‰å…¨æƒæ: ğŸ”„ å®Œæˆ (åŒ…å«Trivyå®¹å™¨æƒæ)"
          echo "ğŸ³ Docker æ¸¬è©¦: âœ… æˆåŠŸ (åŒ…å«å“è³ªé–€æª»é©—è­‰)"
          echo "ğŸ—ï¸ åŸºç¤è¨­æ–½é©—è­‰: âœ… æˆåŠŸ"
          echo "ğŸ“š æ–‡æª”é€£çµé©—è­‰: âœ… æˆåŠŸ (T3ä»»å‹™æ•´åˆ)"
          echo "ğŸ“Š å“è³ªé–€æª»: ğŸš€ å·²å¯¦æ–½å¼·åˆ¶æ€§æª¢æŸ¥æ©Ÿåˆ¶"
          echo "ğŸ” å®‰å…¨é˜²è­·: ğŸ”’ å·²æ•´åˆå¤šå±¤å®¹å™¨å®‰å…¨æƒæ"
          echo "::endgroup::"
          
          # è¼¸å‡ºåŸºç¤è¨­æ–½ä¿®å¾©æ‘˜è¦
          echo "::group::Infrastructure Enhancement Summary (T1 & T3 Tasks)"
          echo "âœ… å¼·åˆ¶æ€§å“è³ªé–€æª»æª¢æŸ¥ - å·²å¯¦æ–½90%è¦†è“‹ç‡é–€æª»"
          echo "âœ… Trivyå®¹å™¨å®‰å…¨æƒæ - å·²æ•´åˆæ¼æ´/é…ç½®/ç§˜å¯†æƒæ"
          echo "âœ… CI/CDç®¡é“å„ªåŒ– - å·²å¯¦æ–½è‡ªå‹•å“è³ªé©—è­‰"
          echo "âœ… æ¸¬è©¦åŸºç¤è¨­æ–½å¼·åŒ– - Dockeræ¸¬è©¦æ¡†æ¶æŒçºŒå„ªåŒ–"
          echo "âœ… æ–‡æª”å“è³ªä¿è­‰ - å·²æ•´åˆé€£çµæœ‰æ•ˆæ€§è‡ªå‹•æª¢æŸ¥"
          echo "âœ… æ–‡æª”CIæ•´åˆ - T3ä»»å‹™åŸºç¤è¨­æ–½å±¤å®Œæˆ"
          echo "::endgroup::"
          
          exit 0
        else
          echo "::error title=Build Failed with Quality Issues::âŒ é—œéµæ§‹å»ºéšæ®µå¤±æ•— - è«‹æª¢æŸ¥å¤±æ•—çš„éšæ®µ"
          echo "âŒ Build failed - Check failed stages above"
          echo "ğŸš¨ æ³¨æ„ï¼šå¼·åˆ¶å“è³ªé–€æª»ç¾å·²ç”Ÿæ•ˆï¼Œå¿…é ˆä¿®å¾©æ‰€æœ‰é—œéµå•é¡Œ"
          exit 1
        fi