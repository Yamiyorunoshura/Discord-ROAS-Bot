#!/usr/bin/env python3
"""
æœå‹™æ•´åˆæ¸¬è©¦å¥—ä»¶
Task ID: 1 - ROAS Bot v2.4.3 Dockerå•Ÿå‹•ç³»çµ±ä¿®å¾©

é€™å€‹æ¸¬è©¦å¥—ä»¶é©—è­‰æ‰€æœ‰æœå‹™æ•´åˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬APIå¥‘ç´„ã€æœå‹™å•Ÿå‹•ç·¨æ’ã€
å¥åº·æª¢æŸ¥ã€æ—¥èªŒæ•´åˆå’ŒéŒ¯èª¤è™•ç†æ©Ÿåˆ¶ã€‚
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import sys
import os

# æ·»åŠ æ ¸å¿ƒæ¨¡çµ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent))

from service_integration_coordinator import ServiceIntegrationCoordinator
from api_contracts import IntegrationContractValidator
from service_startup_orchestrator import ServiceStartupOrchestrator
from unified_health_checker import UnifiedHealthChecker
from unified_logging_integration import UnifiedLogHandler, create_logger
from deployment_manager import DeploymentManager
from environment_validator import EnvironmentValidator

logger = logging.getLogger(__name__)


class IntegrationTestSuite:
    """æœå‹™æ•´åˆæ¸¬è©¦å¥—ä»¶"""
    
    def __init__(self, project_root: Path, environment: str = 'dev'):
        self.project_root = project_root
        self.environment = environment
        self.logger = create_logger("integration-test", "test-suite")
        
        # æ¸¬è©¦çµæœ
        self.test_results: Dict[str, Dict[str, Any]] = {}
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.coordinator = ServiceIntegrationCoordinator(project_root, environment)
        self.contract_validator = IntegrationContractValidator()
        self.startup_orchestrator = ServiceStartupOrchestrator(project_root, environment)
        self.health_checker = UnifiedHealthChecker(project_root)
        self.log_handler = UnifiedLogHandler.get_instance()
        self.deployment_manager = DeploymentManager(project_root, f'docker-compose.{environment}.yml')
        self.environment_validator = EnvironmentValidator(project_root)
    
    async def run_full_integration_test(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ•´åˆæ¸¬è©¦"""
        self.logger.info("ğŸš€ é–‹å§‹å®Œæ•´æœå‹™æ•´åˆæ¸¬è©¦")
        start_time = time.time()
        
        test_summary = {
            'start_time': datetime.now().isoformat(),
            'test_results': {},
            'overall_success': False,
            'duration_seconds': 0,
            'recommendations': []
        }
        
        # æ¸¬è©¦éšæ®µ
        test_phases = [
            ("ç’°å¢ƒé©—è­‰æ¸¬è©¦", self._test_environment_validation),
            ("APIå¥‘ç´„æ¸¬è©¦", self._test_api_contracts),
            ("æœå‹™å•Ÿå‹•ç·¨æ’æ¸¬è©¦", self._test_startup_orchestration),
            ("å¥åº·æª¢æŸ¥æ•´åˆæ¸¬è©¦", self._test_health_check_integration),
            ("æ—¥èªŒæ•´åˆæ¸¬è©¦", self._test_logging_integration),
            ("ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦", self._test_end_to_end_integration),
            ("éŒ¯èª¤æ¢å¾©æ¸¬è©¦", self._test_error_recovery),
            ("æ€§èƒ½åŸºæº–æ¸¬è©¦", self._test_performance_benchmarks)
        ]
        
        passed_tests = 0
        total_tests = len(test_phases)
        
        for phase_name, test_function in test_phases:
            self.logger.info(f"åŸ·è¡Œæ¸¬è©¦éšæ®µ: {phase_name}")
            
            try:
                phase_result = await test_function()
                test_summary['test_results'][phase_name] = phase_result
                
                if phase_result.get('success', False):
                    passed_tests += 1
                    self.logger.info(f"âœ… {phase_name} é€šé")
                else:
                    self.logger.error(f"âŒ {phase_name} å¤±æ•—: {phase_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                
            except Exception as e:
                self.logger.error(f"âŒ {phase_name} ç•°å¸¸: {str(e)}", exception=e)
                test_summary['test_results'][phase_name] = {
                    'success': False,
                    'error': str(e),
                    'phase': phase_name
                }
        
        # è¨ˆç®—ç¸½é«”çµæœ
        total_duration = time.time() - start_time
        test_summary['duration_seconds'] = total_duration
        test_summary['overall_success'] = passed_tests == total_tests
        test_summary['passed_tests'] = passed_tests
        test_summary['total_tests'] = total_tests
        test_summary['success_rate'] = (passed_tests / total_tests) * 100
        
        # ç”Ÿæˆå»ºè­°
        test_summary['recommendations'] = self._generate_test_recommendations(test_summary)
        
        self.logger.info(f"æ•´åˆæ¸¬è©¦å®Œæˆ: {passed_tests}/{total_tests} é€šé ({test_summary['success_rate']:.1f}%)")
        return test_summary
    
    async def _test_environment_validation(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç’°å¢ƒé©—è­‰"""
        try:
            # åŸ·è¡Œç’°å¢ƒæª¢æŸ¥
            validation_success, validation_errors = await self.environment_validator.validate_environment()
            
            # ç”Ÿæˆç’°å¢ƒå ±å‘Š
            report = self.environment_validator.generate_report()
            
            return {
                'success': validation_success,
                'validation_errors': validation_errors,
                'system_info': report.system_info,
                'total_validations': len(report.validation_results),
                'passed_validations': len([r for r in report.validation_results if r.passed]),
                'recommendations': report.recommendations
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'environment_validation'
            }
    
    async def _test_api_contracts(self) -> Dict[str, Any]:
        """æ¸¬è©¦APIå¥‘ç´„"""
        try:
            # é©—è­‰æ‰€æœ‰å¥‘ç´„
            contract_results = self.contract_validator.validate_all_contracts()
            
            # åˆ†æçµæœ
            all_valid = True
            total_checks = 0
            passed_checks = 0
            
            for service, result in contract_results.items():
                if isinstance(result, dict) and 'endpoints_valid' in result:
                    total_checks += 1
                    if result['endpoints_valid']:
                        passed_checks += 1
                    else:
                        all_valid = False
            
            return {
                'success': all_valid,
                'contract_results': contract_results,
                'total_checks': total_checks,
                'passed_checks': passed_checks,
                'validation_summary': {
                    service: result.get('issues', []) if isinstance(result, dict) else str(result)
                    for service, result in contract_results.items()
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'api_contracts'
            }
    
    async def _test_startup_orchestration(self) -> Dict[str, Any]:
        """æ¸¬è©¦æœå‹™å•Ÿå‹•ç·¨æ’"""
        try:
            # åŸ·è¡Œå•Ÿå‹•ç·¨æ’ï¼ˆæ¨¡æ“¬æ¨¡å¼ï¼‰
            result = await self.startup_orchestrator.orchestrate_startup(['redis', 'discord-bot'])
            
            return {
                'success': result.success,
                'startup_order': result.startup_order,
                'total_duration': result.total_duration,
                'failed_services': result.failed_services,
                'service_results': {
                    name: {
                        'success': sr.success,
                        'duration': sr.duration_seconds,
                        'attempts': sr.attempts,
                        'phase': sr.phase.value
                    }
                    for name, sr in result.service_results.items()
                },
                'recommendations': result.recommendations
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'startup_orchestration'
            }
    
    async def _test_health_check_integration(self) -> Dict[str, Any]:
        """æ¸¬è©¦å¥åº·æª¢æŸ¥æ•´åˆ"""
        try:
            # åŸ·è¡Œå¥åº·æª¢æŸ¥
            health_report = await self.health_checker.check_all_services()
            
            return {
                'success': health_report.overall_status.value in ['healthy', 'degraded'],
                'overall_status': health_report.overall_status.value,
                'health_score': health_report.health_score,
                'service_count': len(health_report.service_results),
                'healthy_services': len([
                    s for s in health_report.service_results.values() 
                    if s.status.value == 'healthy'
                ]),
                'critical_issues': health_report.critical_issues,
                'warnings': health_report.warnings,
                'recommendations': health_report.recommendations,
                'response_time_stats': health_report.response_time_stats
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'health_check_integration'
            }
    
    async def _test_logging_integration(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ—¥èªŒæ•´åˆ"""
        try:
            # æ¸¬è©¦æ—¥èªŒè¨˜éŒ„
            test_logger = create_logger("integration-test", "logging-test")
            
            # è¨˜éŒ„æ¸¬è©¦æ—¥èªŒ
            with test_logger.context("logging-test") as ctx:
                test_logger.info("æ¸¬è©¦è³‡è¨Šæ—¥èªŒ", tags={'test': 'logging'})
                test_logger.warning("æ¸¬è©¦è­¦å‘Šæ—¥èªŒ", tags={'test': 'logging'})
                test_logger.error("æ¸¬è©¦éŒ¯èª¤æ—¥èªŒ", error_code="TEST-001", tags={'test': 'logging'})
            
            # ç­‰å¾…æ—¥èªŒè™•ç†
            await asyncio.sleep(2)
            
            # ç²å–æ—¥èªŒçµ±è¨ˆ
            log_stats = self.log_handler.get_stats()
            
            # åˆ†ææ¸¬è©¦æ—¥èªŒ
            analysis_report = self.log_handler.analyze_logs(hours=1)
            
            return {
                'success': True,
                'log_stats': log_stats,
                'analysis_report': {
                    'total_logs': analysis_report.total_logs,
                    'log_level_distribution': analysis_report.log_level_distribution,
                    'recommendations': analysis_report.recommendations
                },
                'test_logs_recorded': 3
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'logging_integration'
            }
    
    async def _test_end_to_end_integration(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç«¯åˆ°ç«¯æ•´åˆ"""
        try:
            # åŸ·è¡Œå®Œæ•´çš„æ•´åˆå”èª¿
            integration_result = await self.coordinator.orchestrate_integration()
            
            # ç²å–æ•´åˆå ±å‘Š
            integration_report = await self.coordinator.get_integration_report()
            
            return {
                'success': integration_result.success,
                'integration_phase': integration_result.phase.value,
                'duration': integration_result.duration_seconds,
                'service_status': integration_result.service_status,
                'integration_health_score': integration_report['integration_health_score'],
                'contract_validation': integration_report['contract_validation'],
                'dependency_status': integration_report['dependency_status'],
                'recommendations': integration_report['recommendations'],
                'errors': integration_result.errors,
                'warnings': integration_result.warnings
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'end_to_end_integration'
            }
    
    async def _test_error_recovery(self) -> Dict[str, Any]:
        """æ¸¬è©¦éŒ¯èª¤æ¢å¾©"""
        try:
            # æ¨¡æ“¬éŒ¯èª¤æƒ…æ³
            test_error = Exception("æ¨¡æ“¬æ•´åˆæ¸¬è©¦éŒ¯èª¤")
            error_context = {
                'operation': 'integration_test',
                'component': 'error_recovery_test',
                'test_scenario': 'simulated_error'
            }
            
            # åŸ·è¡ŒéŒ¯èª¤è™•ç†
            recovery_action = await self.coordinator.error_handler.handle_error(test_error, error_context)
            
            # ç”ŸæˆéŒ¯èª¤å ±å‘Š
            error_report = await self.coordinator.error_handler.generate_error_report(hours=1)
            
            return {
                'success': True,
                'recovery_action_type': recovery_action.action_type,
                'recovery_description': recovery_action.description,
                'error_report_summary': {
                    'total_errors': error_report.total_errors,
                    'resolution_success_rate': error_report.resolution_success_rate,
                    'recommendations': error_report.recommendations
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'error_recovery'
            }
    
    async def _test_performance_benchmarks(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ€§èƒ½åŸºæº–"""
        try:
            # è¨˜éŒ„åŸºæº–æ¸¬è©¦é–‹å§‹æ™‚é–“
            benchmark_start = time.time()
            
            # ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹æ“ä½œä¾†æ¸¬è©¦æ€§èƒ½
            tasks = [
                self._benchmark_health_check(),
                self._benchmark_logging(),
                self._benchmark_contract_validation(),
            ]
            
            benchmark_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            total_benchmark_time = time.time() - benchmark_start
            
            # åˆ†æçµæœ
            successful_benchmarks = sum(1 for result in benchmark_results if not isinstance(result, Exception))
            
            return {
                'success': successful_benchmarks >= len(tasks) * 0.8,  # è‡³å°‘80%æˆåŠŸ
                'total_benchmark_time': total_benchmark_time,
                'benchmark_results': [
                    result if not isinstance(result, Exception) else {'error': str(result)}
                    for result in benchmark_results
                ],
                'successful_benchmarks': successful_benchmarks,
                'total_benchmarks': len(tasks),
                'performance_score': (successful_benchmarks / len(tasks)) * 100
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'phase': 'performance_benchmarks'
            }
    
    async def _benchmark_health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥æ€§èƒ½åŸºæº–"""
        start_time = time.time()
        
        # åŸ·è¡Œå¤šæ¬¡å¥åº·æª¢æŸ¥
        iterations = 5
        total_checks = 0
        successful_checks = 0
        
        for i in range(iterations):
            try:
                report = await self.health_checker.check_all_services()
                total_checks += len(report.service_results)
                successful_checks += len([s for s in report.service_results.values() if s.status.value != 'unknown'])
            except Exception:
                pass
        
        duration = time.time() - start_time
        
        return {
            'benchmark_type': 'health_check',
            'iterations': iterations,
            'total_duration': duration,
            'avg_duration_per_iteration': duration / iterations,
            'total_checks': total_checks,
            'successful_checks': successful_checks,
            'success_rate': (successful_checks / total_checks) * 100 if total_checks > 0 else 0
        }
    
    async def _benchmark_logging(self) -> Dict[str, Any]:
        """æ—¥èªŒè¨˜éŒ„æ€§èƒ½åŸºæº–"""
        start_time = time.time()
        
        # è¨˜éŒ„å¤§é‡æ—¥èªŒ
        test_logger = create_logger("benchmark", "logging")
        log_count = 100
        
        for i in range(log_count):
            test_logger.info(f"åŸºæº–æ¸¬è©¦æ—¥èªŒ #{i}", tags={'benchmark': 'logging'})
        
        # ç­‰å¾…æ—¥èªŒè™•ç†
        await asyncio.sleep(1)
        
        duration = time.time() - start_time
        
        return {
            'benchmark_type': 'logging',
            'log_count': log_count,
            'total_duration': duration,
            'logs_per_second': log_count / duration,
            'avg_time_per_log': (duration / log_count) * 1000  # æ¯«ç§’
        }
    
    async def _benchmark_contract_validation(self) -> Dict[str, Any]:
        """å¥‘ç´„é©—è­‰æ€§èƒ½åŸºæº–"""
        start_time = time.time()
        
        # åŸ·è¡Œå¤šæ¬¡å¥‘ç´„é©—è­‰
        iterations = 10
        
        for i in range(iterations):
            self.contract_validator.validate_all_contracts()
        
        duration = time.time() - start_time
        
        return {
            'benchmark_type': 'contract_validation',
            'iterations': iterations,
            'total_duration': duration,
            'avg_duration_per_iteration': duration / iterations
        }
    
    def _generate_test_recommendations(self, test_summary: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ¸¬è©¦å»ºè­°"""
        recommendations = []
        
        success_rate = test_summary.get('success_rate', 0)
        
        if success_rate < 50:
            recommendations.append("æ•´åˆæ¸¬è©¦æˆåŠŸç‡éä½ï¼Œéœ€è¦å…¨é¢æª¢æŸ¥ç³»çµ±é…ç½®")
        elif success_rate < 80:
            recommendations.append("éƒ¨åˆ†æ•´åˆæ¸¬è©¦å¤±æ•—ï¼Œå»ºè­°æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦é …ç›®")
        else:
            recommendations.append("æ•´åˆæ¸¬è©¦è¡¨ç¾è‰¯å¥½")
        
        # æª¢æŸ¥ç‰¹å®šæ¸¬è©¦å¤±æ•—
        test_results = test_summary.get('test_results', {})
        
        failed_tests = [
            test_name for test_name, result in test_results.items()
            if not result.get('success', False)
        ]
        
        if failed_tests:
            recommendations.append(f"é‡é»æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦: {', '.join(failed_tests)}")
        
        # æ€§èƒ½ç›¸é—œå»ºè­°
        performance_result = test_results.get('æ€§èƒ½åŸºæº–æ¸¬è©¦', {})
        if performance_result.get('success') and performance_result.get('performance_score', 0) < 80:
            recommendations.append("ç³»çµ±æ€§èƒ½æœ‰å¾…æå‡ï¼Œè€ƒæ…®å„ªåŒ–æ…¢é€Ÿçµ„ä»¶")
        
        return recommendations


async def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ROAS Bot æœå‹™æ•´åˆæ¸¬è©¦å¥—ä»¶')
    parser.add_argument('--environment', '-e', default='dev', choices=['dev', 'prod'],
                       help='æ¸¬è©¦ç’°å¢ƒ')
    parser.add_argument('--output', '-o', help='æ¸¬è©¦å ±å‘Šè¼¸å‡ºè·¯å¾‘')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
    parser.add_argument('--quick', '-q', action='store_true', help='å¿«é€Ÿæ¸¬è©¦ï¼ˆè·³éæ€§èƒ½åŸºæº–ï¼‰')
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, 
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ç¢ºå®šå°ˆæ¡ˆæ ¹ç›®éŒ„
    project_root = Path.cwd()
    
    print("ğŸ§ª ROAS Bot v2.4.3 æœå‹™æ•´åˆæ¸¬è©¦å¥—ä»¶")
    print(f"å°ˆæ¡ˆæ ¹ç›®éŒ„: {project_root}")
    print(f"æ¸¬è©¦ç’°å¢ƒ: {args.environment}")
    print(f"é–‹å§‹æ™‚é–“: {datetime.now()}")
    print("="*60)
    
    try:
        # å‰µå»ºæ¸¬è©¦å¥—ä»¶
        test_suite = IntegrationTestSuite(project_root, args.environment)
        
        # åŸ·è¡Œå®Œæ•´æ¸¬è©¦
        test_summary = await test_suite.run_full_integration_test()
        
        # è¼¸å‡ºçµæœ
        success_rate = test_summary['success_rate']
        print(f"\n{'='*60}")
        print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
        print(f"{'='*60}")
        print(f"æ•´é«”ç‹€æ…‹: {'âœ… é€šé' if test_summary['overall_success'] else 'âŒ å¤±æ•—'}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}% ({test_summary['passed_tests']}/{test_summary['total_tests']})")
        print(f"ç¸½è€—æ™‚: {test_summary['duration_seconds']:.1f} ç§’")
        
        # æ¸¬è©¦è©³æƒ…
        print(f"\næ¸¬è©¦è©³æƒ…:")
        for test_name, result in test_summary['test_results'].items():
            status = "âœ…" if result.get('success', False) else "âŒ"
            print(f"  {status} {test_name}")
            if not result.get('success', False) and result.get('error'):
                print(f"    éŒ¯èª¤: {result['error']}")
        
        # å»ºè­°
        if test_summary['recommendations']:
            print(f"\nğŸ’¡ å»ºè­°:")
            for rec in test_summary['recommendations']:
                print(f"  â€¢ {rec}")
        
        # ä¿å­˜å ±å‘Š
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(test_summary, f, indent=2, ensure_ascii=False, default=str)
            print(f"\nğŸ“„ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
        return 0 if test_summary['overall_success'] else 1
        
    except KeyboardInterrupt:
        print("\næ“ä½œå·²å–æ¶ˆ")
        return 130
    except Exception as e:
        print(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(asyncio.run(main()))