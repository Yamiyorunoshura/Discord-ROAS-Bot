#!/usr/bin/env python3
"""
çµ±ä¸€æ—¥èªŒå’ŒéŒ¯èª¤è™•ç†æ•´åˆç³»çµ±
Task ID: 1 - ROAS Bot v2.4.3 Dockerå•Ÿå‹•ç³»çµ±ä¿®å¾©

é€™å€‹æ¨¡çµ„æ•´åˆæ‰€æœ‰æœå‹™çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„ï¼Œæä¾›çµ±ä¸€çš„æ—¥èªŒæ ¼å¼ã€
åˆ†æ•£å¼è¿½è¹¤ã€éŒ¯èª¤èšåˆå’Œæ™ºèƒ½åˆ†æåŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
import logging.handlers
import time
import uuid
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Callable, TextIO
from contextlib import contextmanager, asynccontextmanager
import threading
import queue
import traceback

from .error_handler import ErrorHandler, DeploymentError, ErrorCategory, ErrorSeverity

logger = logging.getLogger(__name__)


class LogLevel(Enum):
    """æ—¥èªŒç­‰ç´š"""
    TRACE = "trace"
    DEBUG = "debug" 
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class LogSource(Enum):
    """æ—¥èªŒä¾†æº"""
    DISCORD_BOT = "discord-bot"
    REDIS = "redis"
    PROMETHEUS = "prometheus"
    GRAFANA = "grafana"
    DEPLOYMENT = "deployment"
    MONITORING = "monitoring"
    INTEGRATION = "integration"
    SYSTEM = "system"


@dataclass
class LogContext:
    """æ—¥èªŒä¸Šä¸‹æ–‡"""
    trace_id: str
    span_id: str
    service_name: str
    component: str
    operation: str
    user_id: Optional[str] = None
    request_id: Optional[str] = None
    session_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class StructuredLogEntry:
    """çµæ§‹åŒ–æ—¥èªŒæ¢ç›®"""
    timestamp: datetime
    level: LogLevel
    source: LogSource
    message: str
    context: LogContext
    exception: Optional[str] = None
    stack_trace: Optional[str] = None
    duration_ms: Optional[float] = None
    error_code: Optional[str] = None
    tags: Dict[str, str] = field(default_factory=dict)
    metrics: Dict[str, Union[int, float]] = field(default_factory=dict)


@dataclass
class ErrorAggregation:
    """éŒ¯èª¤èšåˆçµ±è¨ˆ"""
    error_type: str
    count: int
    first_occurrence: datetime
    last_occurrence: datetime
    affected_services: List[str]
    error_rate: float
    sample_messages: List[str]


@dataclass
class LogAnalysisReport:
    """æ—¥èªŒåˆ†æå ±å‘Š"""
    timestamp: datetime
    time_range_hours: int
    total_logs: int
    log_level_distribution: Dict[str, int]
    error_aggregations: List[ErrorAggregation]
    top_errors: List[str]
    service_error_rates: Dict[str, float]
    performance_issues: List[str]
    recommendations: List[str]


class StructuredLogger:
    """çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨"""
    
    def __init__(self, service_name: str, component: str):
        self.service_name = service_name
        self.component = component
        self.context_stack: List[LogContext] = []
        
    def create_context(self, operation: str, trace_id: Optional[str] = None,
                      span_id: Optional[str] = None, **kwargs) -> LogContext:
        """å‰µå»ºæ—¥èªŒä¸Šä¸‹æ–‡"""
        return LogContext(
            trace_id=trace_id or str(uuid.uuid4()),
            span_id=span_id or str(uuid.uuid4())[:8],
            service_name=self.service_name,
            component=self.component,
            operation=operation,
            **kwargs
        )
    
    @contextmanager
    def context(self, operation: str, **kwargs):
        """æ—¥èªŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        ctx = self.create_context(operation, **kwargs)
        self.context_stack.append(ctx)
        start_time = time.time()
        
        try:
            yield ctx
        finally:
            # è¨˜éŒ„æ“ä½œå®Œæˆ
            duration_ms = (time.time() - start_time) * 1000
            if self.context_stack:
                self.context_stack.pop()
    
    def get_current_context(self) -> Optional[LogContext]:
        """ç²å–ç•¶å‰ä¸Šä¸‹æ–‡"""
        return self.context_stack[-1] if self.context_stack else None
    
    def log(self, level: LogLevel, message: str, **kwargs) -> None:
        """è¨˜éŒ„çµæ§‹åŒ–æ—¥èªŒ"""
        context = self.get_current_context()
        if not context:
            # å‰µå»ºé»˜èªä¸Šä¸‹æ–‡
            context = self.create_context("default")
        
        entry = StructuredLogEntry(
            timestamp=datetime.now(),
            level=level,
            source=LogSource(self.service_name),
            message=message,
            context=context,
            **kwargs
        )
        
        # ç™¼é€åˆ°çµ±ä¸€æ—¥èªŒè™•ç†å™¨
        UnifiedLogHandler.get_instance().handle_log(entry)
    
    def trace(self, message: str, **kwargs):
        """è¨˜éŒ„è¿½è¹¤æ—¥èªŒ"""
        self.log(LogLevel.TRACE, message, **kwargs)
    
    def debug(self, message: str, **kwargs):
        """è¨˜éŒ„é™¤éŒ¯æ—¥èªŒ"""
        self.log(LogLevel.DEBUG, message, **kwargs)
    
    def info(self, message: str, **kwargs):
        """è¨˜éŒ„è³‡è¨Šæ—¥èªŒ"""
        self.log(LogLevel.INFO, message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        """è¨˜éŒ„è­¦å‘Šæ—¥èªŒ"""
        self.log(LogLevel.WARNING, message, **kwargs)
    
    def error(self, message: str, exception: Optional[Exception] = None, **kwargs):
        """è¨˜éŒ„éŒ¯èª¤æ—¥èªŒ"""
        kwargs.update({
            'exception': str(exception) if exception else None,
            'stack_trace': traceback.format_exc() if exception else None
        })
        self.log(LogLevel.ERROR, message, **kwargs)
    
    def critical(self, message: str, exception: Optional[Exception] = None, **kwargs):
        """è¨˜éŒ„é—œéµéŒ¯èª¤æ—¥èªŒ"""
        kwargs.update({
            'exception': str(exception) if exception else None,
            'stack_trace': traceback.format_exc() if exception else None
        })
        self.log(LogLevel.CRITICAL, message, **kwargs)


class UnifiedLogHandler:
    """çµ±ä¸€æ—¥èªŒè™•ç†å™¨ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self._initialized = True
        self.project_root = Path.cwd()
        self.log_queue = queue.Queue(maxsize=10000)
        self.error_handler = ErrorHandler(self.project_root)
        
        # æ—¥èªŒç·©è¡å€
        self.log_buffer: List[StructuredLogEntry] = []
        self.buffer_lock = threading.Lock()
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_logs': 0,
            'error_count': 0,
            'last_flush': datetime.now()
        }
        
        # å•Ÿå‹•æ—¥èªŒè™•ç†ç·šç¨‹
        self.processing_thread = threading.Thread(target=self._process_logs, daemon=True)
        self.processing_thread.start()
        
        # è¨­ç½®æ¨™æº–æ—¥èªŒè™•ç†å™¨
        self._setup_standard_loggers()
    
    @classmethod
    def get_instance(cls):
        """ç²å–å–®ä¾‹å¯¦ä¾‹"""
        if cls._instance is None:
            cls()
        return cls._instance
    
    def handle_log(self, entry: StructuredLogEntry) -> None:
        """è™•ç†æ—¥èªŒæ¢ç›®"""
        try:
            self.log_queue.put_nowait(entry)
        except queue.Full:
            # éšŠåˆ—æ»¿äº†ï¼Œè¨˜éŒ„è­¦å‘Šä¸¦ä¸Ÿæ£„æœ€èˆŠçš„æ—¥èªŒ
            try:
                self.log_queue.get_nowait()
                self.log_queue.put_nowait(entry)
            except queue.Empty:
                pass
    
    def _process_logs(self) -> None:
        """æ—¥èªŒè™•ç†ç·šç¨‹"""
        while True:
            try:
                # æ‰¹é‡è™•ç†æ—¥èªŒ
                batch = []
                deadline = time.time() + 1.0  # 1ç§’æ‰¹è™•ç†è¶…æ™‚
                
                while len(batch) < 100 and time.time() < deadline:
                    try:
                        entry = self.log_queue.get(timeout=0.1)
                        batch.append(entry)
                    except queue.Empty:
                        break
                
                if batch:
                    self._process_log_batch(batch)
                
            except Exception as e:
                # æ—¥èªŒè™•ç†å‡ºéŒ¯ï¼Œé¿å…ç„¡é™å¾ªç’°
                print(f"æ—¥èªŒè™•ç†ç•°å¸¸: {e}", file=__import__('sys').stderr)
                time.sleep(1)
    
    def _process_log_batch(self, batch: List[StructuredLogEntry]) -> None:
        """è™•ç†æ—¥èªŒæ‰¹æ¬¡"""
        with self.buffer_lock:
            self.log_buffer.extend(batch)
            
            # æ›´æ–°çµ±è¨ˆ
            self.stats['total_logs'] += len(batch)
            error_count = sum(1 for entry in batch 
                            if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL])
            self.stats['error_count'] += error_count
            
            # å¯«å…¥æ–‡ä»¶
            self._write_to_files(batch)
            
            # éŒ¯èª¤è™•ç†
            self._handle_errors(batch)
            
            # æ¸…ç†èˆŠæ—¥èªŒ
            if len(self.log_buffer) > 50000:
                self.log_buffer = self.log_buffer[-25000:]
    
    def _write_to_files(self, batch: List[StructuredLogEntry]) -> None:
        """å¯«å…¥æ—¥èªŒæ–‡ä»¶"""
        # æŒ‰æœå‹™åˆ†çµ„
        service_logs = {}
        for entry in batch:
            service = entry.source.value
            if service not in service_logs:
                service_logs[service] = []
            service_logs[service].append(entry)
        
        # å¯«å…¥å„æœå‹™æ—¥èªŒæ–‡ä»¶
        for service, entries in service_logs.items():
            log_file = self.project_root / 'logs' / f'{service}-integration.log'
            log_file.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                with open(log_file, 'a', encoding='utf-8') as f:
                    for entry in entries:
                        log_line = self._format_log_entry(entry)
                        f.write(log_line + '\n')
            except Exception as e:
                print(f"å¯«å…¥æ—¥èªŒæ–‡ä»¶å¤±æ•— {log_file}: {e}", file=__import__('sys').stderr)
        
        # å¯«å…¥çµ±ä¸€æ—¥èªŒæ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼‰
        unified_log_file = self.project_root / 'logs' / 'unified-integration.jsonl'
        try:
            with open(unified_log_file, 'a', encoding='utf-8') as f:
                for entry in batch:
                    json_line = json.dumps(self._serialize_log_entry(entry), 
                                         ensure_ascii=False, default=str)
                    f.write(json_line + '\n')
        except Exception as e:
            print(f"å¯«å…¥çµ±ä¸€æ—¥èªŒæ–‡ä»¶å¤±æ•—: {e}", file=__import__('sys').stderr)
    
    def _handle_errors(self, batch: List[StructuredLogEntry]) -> None:
        """è™•ç†éŒ¯èª¤æ—¥èªŒ"""
        for entry in batch:
            if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]:
                try:
                    # å‰µå»ºéƒ¨ç½²éŒ¯èª¤
                    deployment_error = DeploymentError(
                        error_id=f"LOG-{entry.context.trace_id}",
                        timestamp=entry.timestamp,
                        category=self._map_to_error_category(entry),
                        severity=self._map_to_error_severity(entry.level),
                        title=f"{entry.source.value}: {entry.message[:100]}",
                        message=entry.message,
                        context={
                            'service': entry.source.value,
                            'component': entry.context.component,
                            'operation': entry.context.operation,
                            'trace_id': entry.context.trace_id
                        },
                        stack_trace=entry.stack_trace
                    )
                    
                    # ç•°æ­¥è™•ç†éŒ¯èª¤
                    asyncio.create_task(
                        self.error_handler.log_structured_error(deployment_error)
                    )
                    
                except Exception as e:
                    print(f"éŒ¯èª¤è™•ç†å¤±æ•—: {e}", file=__import__('sys').stderr)
    
    def _format_log_entry(self, entry: StructuredLogEntry) -> str:
        """æ ¼å¼åŒ–æ—¥èªŒæ¢ç›®ç‚ºå¯è®€æ ¼å¼"""
        timestamp = entry.timestamp.strftime('%Y-%m-%d %H:%M:%S')
        level = entry.level.value.upper()
        service = entry.source.value
        component = entry.context.component
        operation = entry.context.operation
        trace_id = entry.context.trace_id[:8]
        
        # åŸºç¤æ ¼å¼
        base_format = f"{timestamp} [{level}] {service}/{component} [{trace_id}] {operation}: {entry.message}"
        
        # æ·»åŠ é¡å¤–ä¿¡æ¯
        extras = []
        if entry.duration_ms:
            extras.append(f"duration={entry.duration_ms:.1f}ms")
        if entry.error_code:
            extras.append(f"code={entry.error_code}")
        if entry.metrics:
            metrics_str = ' '.join([f"{k}={v}" for k, v in entry.metrics.items()])
            extras.append(f"metrics=[{metrics_str}]")
        
        if extras:
            base_format += f" ({', '.join(extras)})"
        
        # æ·»åŠ ç•°å¸¸ä¿¡æ¯
        if entry.exception:
            base_format += f"\n  Exception: {entry.exception}"
        
        return base_format
    
    def _serialize_log_entry(self, entry: StructuredLogEntry) -> Dict[str, Any]:
        """åºåˆ—åŒ–æ—¥èªŒæ¢ç›®ç‚ºå­—å…¸"""
        return {
            'timestamp': entry.timestamp.isoformat(),
            'level': entry.level.value,
            'source': entry.source.value,
            'message': entry.message,
            'context': {
                'trace_id': entry.context.trace_id,
                'span_id': entry.context.span_id,
                'service_name': entry.context.service_name,
                'component': entry.context.component,
                'operation': entry.context.operation,
                'user_id': entry.context.user_id,
                'request_id': entry.context.request_id,
                'session_id': entry.context.session_id,
                'metadata': entry.context.metadata
            },
            'exception': entry.exception,
            'stack_trace': entry.stack_trace,
            'duration_ms': entry.duration_ms,
            'error_code': entry.error_code,
            'tags': entry.tags,
            'metrics': entry.metrics
        }
    
    def _map_to_error_category(self, entry: StructuredLogEntry) -> ErrorCategory:
        """æ˜ å°„æ—¥èªŒæ¢ç›®åˆ°éŒ¯èª¤åˆ†é¡"""
        message_lower = entry.message.lower()
        
        if 'docker' in message_lower or 'container' in message_lower:
            return ErrorCategory.DOCKER
        elif 'network' in message_lower or 'connection' in message_lower:
            return ErrorCategory.NETWORK
        elif 'permission' in message_lower or 'access' in message_lower:
            return ErrorCategory.PERMISSION
        elif 'config' in message_lower:
            return ErrorCategory.CONFIGURATION
        elif 'memory' in message_lower or 'disk' in message_lower:
            return ErrorCategory.RESOURCE
        elif entry.source == LogSource.REDIS:
            return ErrorCategory.SERVICE
        elif entry.source == LogSource.DISCORD_BOT:
            return ErrorCategory.SERVICE
        else:
            return ErrorCategory.UNKNOWN
    
    def _map_to_error_severity(self, level: LogLevel) -> ErrorSeverity:
        """æ˜ å°„æ—¥èªŒç­‰ç´šåˆ°éŒ¯èª¤åš´é‡æ€§"""
        if level == LogLevel.CRITICAL:
            return ErrorSeverity.CRITICAL
        elif level == LogLevel.ERROR:
            return ErrorSeverity.HIGH
        elif level == LogLevel.WARNING:
            return ErrorSeverity.MEDIUM
        else:
            return ErrorSeverity.LOW
    
    def _setup_standard_loggers(self) -> None:
        """è¨­ç½®æ¨™æº–æ—¥èªŒè¨˜éŒ„å™¨"""
        # ç‚ºPythonæ¨™æº–loggingè¨­ç½®è™•ç†å™¨
        class StructuredLogAdapter(logging.Handler):
            def __init__(self, log_handler, source: LogSource):
                super().__init__()
                self.log_handler = log_handler
                self.source = source
            
            def emit(self, record):
                try:
                    # è½‰æ›ç‚ºçµæ§‹åŒ–æ—¥èªŒ
                    level_map = {
                        logging.DEBUG: LogLevel.DEBUG,
                        logging.INFO: LogLevel.INFO,
                        logging.WARNING: LogLevel.WARNING,
                        logging.ERROR: LogLevel.ERROR,
                        logging.CRITICAL: LogLevel.CRITICAL
                    }
                    
                    level = level_map.get(record.levelno, LogLevel.INFO)
                    message = record.getMessage()
                    
                    # å‰µå»ºä¸Šä¸‹æ–‡
                    context = LogContext(
                        trace_id=str(uuid.uuid4()),
                        span_id=str(uuid.uuid4())[:8],
                        service_name=self.source.value,
                        component=record.name,
                        operation="log"
                    )
                    
                    # å‰µå»ºçµæ§‹åŒ–æ—¥èªŒæ¢ç›®
                    entry = StructuredLogEntry(
                        timestamp=datetime.fromtimestamp(record.created),
                        level=level,
                        source=self.source,
                        message=message,
                        context=context,
                        exception=str(record.exc_info[1]) if record.exc_info else None,
                        stack_trace=self.format(record) if record.exc_info else None
                    )
                    
                    self.log_handler.handle_log(entry)
                    
                except Exception:
                    pass  # é¿å…æ—¥èªŒè™•ç†ä¸­çš„éŒ¯èª¤å°è‡´ç¨‹åºå´©æ½°
        
        # è¨­ç½®å„æœå‹™çš„æ—¥èªŒé©é…å™¨
        services = [
            ('discord-bot', LogSource.DISCORD_BOT),
            ('redis', LogSource.REDIS),
            ('prometheus', LogSource.PROMETHEUS),
            ('grafana', LogSource.GRAFANA),
            ('deployment', LogSource.DEPLOYMENT),
            ('monitoring', LogSource.MONITORING),
            ('integration', LogSource.INTEGRATION)
        ]
        
        for service_name, source in services:
            logger = logging.getLogger(service_name)
            adapter = StructuredLogAdapter(self, source)
            logger.addHandler(adapter)
            logger.setLevel(logging.INFO)
    
    def get_logs(self, service: Optional[str] = None, level: Optional[LogLevel] = None,
                hours: int = 24, limit: int = 1000) -> List[StructuredLogEntry]:
        """ç²å–æ—¥èªŒ"""
        with self.buffer_lock:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            
            filtered_logs = []
            for entry in self.log_buffer:
                if entry.timestamp < cutoff_time:
                    continue
                if service and entry.source.value != service:
                    continue
                if level and entry.level != level:
                    continue
                
                filtered_logs.append(entry)
                
                if len(filtered_logs) >= limit:
                    break
            
            return filtered_logs
    
    def analyze_logs(self, hours: int = 24) -> LogAnalysisReport:
        """åˆ†ææ—¥èªŒ"""
        with self.buffer_lock:
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_logs = [
                entry for entry in self.log_buffer
                if entry.timestamp >= cutoff_time
            ]
        
        if not recent_logs:
            return LogAnalysisReport(
                timestamp=datetime.now(),
                time_range_hours=hours,
                total_logs=0,
                log_level_distribution={},
                error_aggregations=[],
                top_errors=[],
                service_error_rates={},
                performance_issues=[],
                recommendations=["ç„¡è¶³å¤ æ—¥èªŒæ•¸æ“šé€²è¡Œåˆ†æ"]
            )
        
        # çµ±è¨ˆæ—¥èªŒç­‰ç´šåˆ†å¸ƒ
        level_distribution = {}
        for entry in recent_logs:
            level = entry.level.value
            level_distribution[level] = level_distribution.get(level, 0) + 1
        
        # éŒ¯èª¤èšåˆ
        error_aggregations = self._aggregate_errors(recent_logs)
        
        # é ‚ç´šéŒ¯èª¤
        error_messages = [entry.message for entry in recent_logs 
                         if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]]
        top_errors = list(set(error_messages))[:10]
        
        # æœå‹™éŒ¯èª¤ç‡
        service_error_rates = self._calculate_service_error_rates(recent_logs)
        
        # æ€§èƒ½å•é¡Œ
        performance_issues = self._identify_performance_issues(recent_logs)
        
        # å»ºè­°
        recommendations = self._generate_log_recommendations(
            recent_logs, error_aggregations, service_error_rates
        )
        
        return LogAnalysisReport(
            timestamp=datetime.now(),
            time_range_hours=hours,
            total_logs=len(recent_logs),
            log_level_distribution=level_distribution,
            error_aggregations=error_aggregations,
            top_errors=top_errors,
            service_error_rates=service_error_rates,
            performance_issues=performance_issues,
            recommendations=recommendations
        )
    
    def _aggregate_errors(self, logs: List[StructuredLogEntry]) -> List[ErrorAggregation]:
        """èšåˆéŒ¯èª¤"""
        error_groups = {}
        
        for entry in logs:
            if entry.level not in [LogLevel.ERROR, LogLevel.CRITICAL]:
                continue
            
            # ç°¡å–®çš„éŒ¯èª¤åˆ†çµ„ï¼ˆåŸºæ–¼æ¶ˆæ¯çš„å‰100å€‹å­—ç¬¦ï¼‰
            error_key = entry.message[:100]
            
            if error_key not in error_groups:
                error_groups[error_key] = {
                    'count': 0,
                    'first_occurrence': entry.timestamp,
                    'last_occurrence': entry.timestamp,
                    'services': set(),
                    'messages': []
                }
            
            group = error_groups[error_key]
            group['count'] += 1
            group['last_occurrence'] = max(group['last_occurrence'], entry.timestamp)
            group['first_occurrence'] = min(group['first_occurrence'], entry.timestamp)
            group['services'].add(entry.source.value)
            
            if len(group['messages']) < 3:
                group['messages'].append(entry.message)
        
        # è½‰æ›ç‚ºErrorAggregationå°è±¡
        aggregations = []
        for error_key, group in error_groups.items():
            duration_hours = (group['last_occurrence'] - group['first_occurrence']).total_seconds() / 3600
            error_rate = group['count'] / max(duration_hours, 1)
            
            aggregations.append(ErrorAggregation(
                error_type=error_key,
                count=group['count'],
                first_occurrence=group['first_occurrence'],
                last_occurrence=group['last_occurrence'],
                affected_services=list(group['services']),
                error_rate=error_rate,
                sample_messages=group['messages']
            ))
        
        # æŒ‰éŒ¯èª¤æ•¸é‡æ’åº
        return sorted(aggregations, key=lambda x: x.count, reverse=True)[:20]
    
    def _calculate_service_error_rates(self, logs: List[StructuredLogEntry]) -> Dict[str, float]:
        """è¨ˆç®—æœå‹™éŒ¯èª¤ç‡"""
        service_stats = {}
        
        for entry in logs:
            service = entry.source.value
            if service not in service_stats:
                service_stats[service] = {'total': 0, 'errors': 0}
            
            service_stats[service]['total'] += 1
            if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL]:
                service_stats[service]['errors'] += 1
        
        error_rates = {}
        for service, stats in service_stats.items():
            error_rates[service] = (stats['errors'] / stats['total']) * 100
        
        return error_rates
    
    def _identify_performance_issues(self, logs: List[StructuredLogEntry]) -> List[str]:
        """è­˜åˆ¥æ€§èƒ½å•é¡Œ"""
        issues = []
        
        # æª¢æŸ¥æ…¢æ“ä½œ
        slow_operations = []
        for entry in logs:
            if entry.duration_ms and entry.duration_ms > 5000:  # 5ç§’ä»¥ä¸Š
                slow_operations.append(f"{entry.source.value}/{entry.context.operation}: {entry.duration_ms:.0f}ms")
        
        if slow_operations:
            issues.append(f"ç™¼ç¾ {len(slow_operations)} å€‹æ…¢æ“ä½œ")
            issues.extend(slow_operations[:5])  # åªé¡¯ç¤ºå‰5å€‹
        
        # æª¢æŸ¥é«˜é »éŒ¯èª¤
        error_count = sum(1 for entry in logs if entry.level in [LogLevel.ERROR, LogLevel.CRITICAL])
        if error_count > len(logs) * 0.1:  # éŒ¯èª¤ç‡è¶…é10%
            issues.append(f"éŒ¯èª¤ç‡éé«˜: {(error_count/len(logs)*100):.1f}%")
        
        return issues
    
    def _generate_log_recommendations(self, logs: List[StructuredLogEntry],
                                    error_aggregations: List[ErrorAggregation],
                                    service_error_rates: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆæ—¥èªŒå»ºè­°"""
        recommendations = []
        
        # åŸºæ–¼éŒ¯èª¤ç‡çš„å»ºè­°
        high_error_services = [
            service for service, rate in service_error_rates.items()
            if rate > 5  # éŒ¯èª¤ç‡è¶…é5%
        ]
        
        if high_error_services:
            recommendations.append(f"æª¢æŸ¥é«˜éŒ¯èª¤ç‡æœå‹™: {', '.join(high_error_services)}")
        
        # åŸºæ–¼éŒ¯èª¤èšåˆçš„å»ºè­°
        if error_aggregations:
            top_error = error_aggregations[0]
            if top_error.count > 10:
                recommendations.append(f"å„ªå…ˆè™•ç†é »ç¹éŒ¯èª¤: {top_error.error_type[:50]}...")
        
        # åŸºæ–¼æ—¥èªŒé‡çš„å»ºè­°
        if len(logs) < 100:
            recommendations.append("æ—¥èªŒé‡è¼ƒå°‘ï¼Œè€ƒæ…®å¢åŠ æ—¥èªŒè©³ç´°ç¨‹åº¦")
        elif len(logs) > 10000:
            recommendations.append("æ—¥èªŒé‡éå¤§ï¼Œè€ƒæ…®æé«˜æ—¥èªŒç­‰ç´šæˆ–å¢åŠ éæ¿¾")
        
        if not recommendations:
            recommendations.append("æ—¥èªŒç‹€æ³æ­£å¸¸")
        
        return recommendations
    
    def get_stats(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        with self.buffer_lock:
            return {
                'total_logs': self.stats['total_logs'],
                'error_count': self.stats['error_count'],
                'buffer_size': len(self.log_buffer),
                'queue_size': self.log_queue.qsize(),
                'last_flush': self.stats['last_flush'].isoformat(),
                'thread_alive': self.processing_thread.is_alive()
            }


# å·¥å» æ–¹æ³•å’Œä¾¿åˆ©å‡½æ•¸
def create_logger(service_name: str, component: str) -> StructuredLogger:
    """å‰µå»ºçµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„å™¨"""
    return StructuredLogger(service_name, component)


def get_log_handler() -> UnifiedLogHandler:
    """ç²å–çµ±ä¸€æ—¥èªŒè™•ç†å™¨"""
    return UnifiedLogHandler.get_instance()


# è£é£¾å™¨
def log_operation(logger: StructuredLogger, operation_name: str):
    """æ“ä½œæ—¥èªŒè¨˜éŒ„è£é£¾å™¨"""
    def decorator(func: Callable) -> Callable:
        if asyncio.iscoroutinefunction(func):
            async def async_wrapper(*args, **kwargs):
                with logger.context(operation_name) as ctx:
                    start_time = time.time()
                    try:
                        logger.info(f"é–‹å§‹æ“ä½œ: {operation_name}")
                        result = await func(*args, **kwargs)
                        duration_ms = (time.time() - start_time) * 1000
                        logger.info(f"æ“ä½œå®Œæˆ: {operation_name}", 
                                  duration_ms=duration_ms)
                        return result
                    except Exception as e:
                        duration_ms = (time.time() - start_time) * 1000
                        logger.error(f"æ“ä½œå¤±æ•—: {operation_name}", 
                                   exception=e, duration_ms=duration_ms)
                        raise
            return async_wrapper
        else:
            def sync_wrapper(*args, **kwargs):
                with logger.context(operation_name) as ctx:
                    start_time = time.time()
                    try:
                        logger.info(f"é–‹å§‹æ“ä½œ: {operation_name}")
                        result = func(*args, **kwargs)
                        duration_ms = (time.time() - start_time) * 1000
                        logger.info(f"æ“ä½œå®Œæˆ: {operation_name}", 
                                  duration_ms=duration_ms)
                        return result
                    except Exception as e:
                        duration_ms = (time.time() - start_time) * 1000
                        logger.error(f"æ“ä½œå¤±æ•—: {operation_name}", 
                                   exception=e, duration_ms=duration_ms)
                        raise
            return sync_wrapper
    return decorator


# å‘½ä»¤è¡Œä»‹é¢
async def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ROAS Bot çµ±ä¸€æ—¥èªŒåˆ†æå·¥å…·')
    parser.add_argument('command', choices=['analyze', 'logs', 'stats', 'test'],
                       help='åŸ·è¡Œçš„å‘½ä»¤')
    parser.add_argument('--service', '-s', help='æŒ‡å®šæœå‹™åç¨±')
    parser.add_argument('--level', choices=['trace', 'debug', 'info', 'warning', 'error', 'critical'],
                       help='æ—¥èªŒç­‰ç´šéæ¿¾')
    parser.add_argument('--hours', type=int, default=24, help='æ™‚é–“ç¯„åœï¼ˆå°æ™‚ï¼‰')
    parser.add_argument('--limit', type=int, default=1000, help='æ—¥èªŒæ•¸é‡é™åˆ¶')
    parser.add_argument('--output', '-o', help='è¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    try:
        handler = get_log_handler()
        
        if args.command == 'analyze':
            report = handler.analyze_logs(args.hours)
            
            print(f"\n{'='*60}")
            print("ğŸ“Š ROAS Bot v2.4.3 æ—¥èªŒåˆ†æå ±å‘Š")
            print(f"{'='*60}")
            print(f"åˆ†ææ™‚é–“ç¯„åœ: {report.time_range_hours} å°æ™‚")
            print(f"ç¸½æ—¥èªŒæ•¸é‡: {report.total_logs}")
            
            if report.log_level_distribution:
                print(f"\næ—¥èªŒç­‰ç´šåˆ†å¸ƒ:")
                for level, count in report.log_level_distribution.items():
                    print(f"  {level}: {count}")
            
            if report.error_aggregations:
                print(f"\néŒ¯èª¤èšåˆ (å‰5å):")
                for i, agg in enumerate(report.error_aggregations[:5], 1):
                    print(f"  {i}. {agg.error_type[:60]}...")
                    print(f"     å‡ºç¾æ¬¡æ•¸: {agg.count}, éŒ¯èª¤ç‡: {agg.error_rate:.2f}/å°æ™‚")
                    print(f"     å½±éŸ¿æœå‹™: {', '.join(agg.affected_services)}")
            
            if report.service_error_rates:
                print(f"\næœå‹™éŒ¯èª¤ç‡:")
                for service, rate in report.service_error_rates.items():
                    print(f"  {service}: {rate:.2f}%")
            
            if report.performance_issues:
                print(f"\næ€§èƒ½å•é¡Œ:")
                for issue in report.performance_issues:
                    print(f"  â€¢ {issue}")
            
            if report.recommendations:
                print(f"\nå»ºè­°:")
                for rec in report.recommendations:
                    print(f"  â€¢ {rec}")
            
            if args.output:
                with open(args.output, 'w', encoding='utf-8') as f:
                    json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
                print(f"\nå ±å‘Šå·²ä¿å­˜åˆ°: {args.output}")
            
            return 0
            
        elif args.command == 'logs':
            level_filter = LogLevel(args.level) if args.level else None
            logs = handler.get_logs(
                service=args.service,
                level=level_filter,
                hours=args.hours,
                limit=args.limit
            )
            
            print(f"æ‰¾åˆ° {len(logs)} æ¢æ—¥èªŒ")
            
            for entry in logs[-20:]:  # é¡¯ç¤ºæœ€å¾Œ20æ¢
                timestamp = entry.timestamp.strftime('%Y-%m-%d %H:%M:%S')
                level = entry.level.value.upper()
                service = entry.source.value
                print(f"[{timestamp}] [{level}] {service}: {entry.message}")
            
            return 0
            
        elif args.command == 'stats':
            stats = handler.get_stats()
            
            print("ğŸ“ˆ æ—¥èªŒè™•ç†çµ±è¨ˆ:")
            print(f"  ç¸½æ—¥èªŒæ•¸: {stats['total_logs']}")
            print(f"  éŒ¯èª¤æ•¸é‡: {stats['error_count']}")
            print(f"  ç·©è¡å€å¤§å°: {stats['buffer_size']}")
            print(f"  éšŠåˆ—å¤§å°: {stats['queue_size']}")
            print(f"  æœ€å¾Œåˆ·æ–°: {stats['last_flush']}")
            print(f"  è™•ç†ç·šç¨‹ç‹€æ…‹: {'æ´»èº' if stats['thread_alive'] else 'åœæ­¢'}")
            
            return 0
            
        elif args.command == 'test':
            # æ¸¬è©¦æ—¥èªŒè¨˜éŒ„
            test_logger = create_logger("test-service", "test-component")
            
            with test_logger.context("test-operation") as ctx:
                test_logger.info("æ¸¬è©¦è³‡è¨Šæ—¥èªŒ")
                test_logger.warning("æ¸¬è©¦è­¦å‘Šæ—¥èªŒ")
                test_logger.error("æ¸¬è©¦éŒ¯èª¤æ—¥èªŒ", error_code="TEST-001")
            
            # ç­‰å¾…æ—¥èªŒè™•ç†
            await asyncio.sleep(2)
            
            print("æ¸¬è©¦æ—¥èªŒå·²è¨˜éŒ„")
            return 0
            
    except KeyboardInterrupt:
        print("\næ“ä½œå·²å–æ¶ˆ")
        return 130
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == '__main__':
    import sys
    sys.exit(asyncio.run(main()))